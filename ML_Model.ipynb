{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load X data\n",
    "file_path = 'processed_books.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    (processed_books, returns_per_id_per_stock) = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processed_books dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of time ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([5663, 29267, 27957, 18498, 30824, 2239, 23488, 25971, 2534, 2510, 23619, 3457, 25331, 7491, 30416, 27534, 5904, 22026, 12225, 24033, 29761, 14714, 17328, 6074, 22678, 4603, 21020, 29362, 24253, 9846, 31497, 21267, 16560, 21398, 18629, 10230, 152, 28969, 28323, 14065, 29252, 5907, 6693, 25179, 28868, 24149, 4746, 23598, 28210, 28838, 146, 2254, 20734, 23351, 12073, 17876, 1468, 12588, 12987, 10793, 18507, 1075, 11686, 27019, 2644, 10781, 28841, 21148, 9748, 11844, 8825, 21657, 22440, 29005, 24816, 2507, 21145, 22973, 5124, 15625, 11579, 20326, 1998, 18224, 14842, 30720, 6166, 20585, 5532, 1477, 536, 13791, 17179, 14449, 28880, 16402, 4329, 16429, 31113, 30443, 12585, 3686, 31351, 16679, 18129, 5666, 4460, 4493, 20469, 19034, 31595, 11433, 24402, 1063, 19281, 23765, 1980, 23994, 9060, 947, 26763, 25730, 7122, 25599, 18656, 18400, 1879, 13529, 31482, 24131, 17727, 8822, 18007, 24259, 28192, 12234, 18748, 25301, 1876, 23342, 28993, 1215, 27403, 2102, 18653, 31348, 15491, 21404, 8524, 32646, 8661, 4201, 31214, 19579, 16149, 22038, 23464, 24795, 13410, 27174, 23366, 10105, 27013, 24384, 10364, 8676, 27162, 28225, 17474, 13663, 2772, 4630, 21121, 20064, 23217, 27406, 28484, 8652, 25575, 8554, 14205, 1057, 27939, 3427, 31601, 16816, 13279, 31750, 22193, 18495, 9215, 24661, 29106, 12347, 4103, 12061, 10111, 18483, 6961, 27037, 2010, 29791, 411, 28475, 8950, 13377, 22178, 13678, 3957, 554, 15122, 1209, 1712, 7518, 21243, 14464, 31068, 32131, 23628, 15759, 3445, 28070, 2659, 21645, 18111, 21368, 25569, 25864, 13124, 30050, 25712, 28737, 16694, 1459, 18662, 9456, 30940, 26254, 8938, 25176, 29517, 18358, 27427, 5118, 31345, 4743, 6324, 17870, 23226, 31083, 26522, 17569, 31071, 3564, 22964, 23503, 12981, 23202, 27135, 11296, 5773, 7884, 30547, 15872, 6065, 11689, 4615, 22169, 6068, 4850, 2656, 20234, 20323, 31616, 1581, 5678, 5803, 7217, 21240, 19799, 1227, 22300, 2525, 9343, 7107, 12996, 23232, 4079, 7759, 32649, 4618, 9438, 11043, 6979, 15092, 10796, 12859, 10135, 22166, 2385, 25584, 16289, 5916, 3046, 1462, 5809, 8435, 12728, 19180, 8009, 14422, 4984, 1870, 17078, 30312, 16649, 7512, 13276, 27168, 6729, 10403, 10751, 13240, 12481, 1587, 22455, 16941, 27561, 14970, 24396, 8664, 8298, 6172, 16676, 23348, 13148, 17584, 6696, 20597, 30303, 20862, 32280, 20603, 24161, 2903, 19147, 17837, 23750, 21139, 18242, 20213, 23637, 23768, 23500, 18766, 32676, 5648, 4987, 14976, 16271, 289, 1191, 4371, 17989, 21934, 27153, 32134, 12103, 26099, 2891, 5517, 10123, 4627, 23863, 545, 6592, 17852, 4737, 8426, 26903, 1471, 17578, 4487, 4588, 11415, 697, 27278, 23220, 539, 6723, 5285, 8271, 17971, 5910, 32155, 18912, 4219, 5934, 27543, 9352, 17191, 9700, 31854, 8408, 6836, 15765, 5249, 5401, 12222, 12758, 16792, 17337, 32277, 25340, 13514, 20329, 21770, 29139, 17983, 4067, 7369, 29246, 28877, 31202, 14836, 24021, 5020, 14857, 21246, 9465, 15908, 6854, 25334, 2117, 14979, 15637, 4457, 5913, 13398, 20460, 29672, 20457, 30169, 5392, 9072, 19531, 1322, 22946, 6428, 6976, 8667, 6434, 4091, 5139, 25474, 7747, 13523, 13928, 16170, 5407, 12079, 21931, 22824, 22023, 24143, 24414, 22827, 31077, 20225, 32012, 24929, 3025, 6827, 3323, 4106, 11052, 32119, 3567, 17194, 14297, 14452, 27799, 6568, 310, 420, 438, 29374, 30023, 4889, 9194, 14982, 3552, 22047, 25197, 26388, 29380, 29889, 20341, 28186, 14312, 24393, 32009, 6562, 26891, 30327, 32247, 20996, 18236, 29240, 16280, 19817, 26873, 14169, 15750, 21136, 29145, 16611, 15548, 20008, 8611, 15131, 31914, 32173, 28761, 25894, 22622, 13598, 25772, 27981, 16328, 15938, 18141, 3886, 4654, 28267, 9659, 30729, 13821, 17888, 21988, 13857, 26168, 4142, 13949, 227, 2683, 9501, 28779, 20776, 32322, 8191, 5193, 26308, 13187, 30604, 19606, 4910, 16745, 2439, 5958, 5976, 19204, 16438, 8718, 32033, 12523, 18701, 25653, 9665, 27886, 30366, 18808, 19597, 19999, 25355, 13699, 14247, 20630, 1513, 16614, 12011, 31018, 20127, 6619, 26147, 12904, 17528, 8459, 15685, 745, 12395, 8346, 22732, 10546, 17644, 20669, 11475, 13044, 14908, 31944, 19222, 30494, 6235, 12115, 17090, 21431, 25483, 8081, 1239, 9138, 21038, 337, 31119, 32724, 25754, 14080, 18147, 7277, 22759, 20785, 11070, 1787, 9885, 468, 25525, 28380, 12151, 18692, 2183, 21848, 20782, 2567, 19725, 22893, 20499, 31819, 32474, 16706, 2424, 19234, 611, 19466, 19883, 23274, 11499, 13720, 20651, 25641, 4797, 15271, 25489, 16575, 16081, 27978, 6256, 22899, 13196, 12669, 30598, 7545, 2844, 7679, 28401, 1269, 4377, 10561, 1135, 8441, 8587, 13294, 22598, 27487, 28898, 9123, 12535, 8974, 3484, 2573, 14077, 32209, 2293, 17355, 20136, 13586, 709, 25087, 18174, 16992, 23265, 15798, 23399, 2415, 29431, 18567, 748, 21434, 12279, 30339, 8331, 23423, 23417, 5300, 14476, 19603, 18534, 24590, 19365, 26707, 31402, 32709, 21300, 21854, 3987, 27317, 26561, 6390, 27868, 13163, 26314, 29943, 4913, 16304, 15155, 25876, 25918, 32182, 25617, 7009, 21428, 9787, 27585, 6482, 17522, 5032, 1930, 10689, 12419, 14619, 10171, 19472, 30208, 31554, 72, 8706, 25879, 17617, 2811, 3999, 23405, 30232, 31155, 18180, 8721, 4547, 12633, 32596, 13452, 3877, 13318, 14646, 15280, 26400, 17120, 17626, 25501, 32200, 27469, 11722, 28395, 31789, 5297, 6613, 16352, 26430, 23524, 24989, 23536, 25367, 11764, 11749, 11987, 5729, 30896, 2582, 10186, 22887, 28499, 11490, 9367, 27722, 30357, 2433, 29026, 20928, 465, 19871, 23652, 5056, 29583, 358, 2037, 15807, 2436, 16721, 22750, 19493, 27094, 31527, 16831, 8840, 11481, 12666, 12139, 9918, 23539, 3758, 11874, 8447, 17486, 10421, 21693, 1242, 10448, 14601, 15393, 28770, 8224, 15521, 5580, 840, 11886, 6771, 5163, 14866, 13071, 16864, 11868, 23658, 28121, 5458, 18430, 17093, 2174, 10040, 13851, 27493, 5425, 24078, 28541, 32328, 17239, 218, 32590, 5842, 26555, 24334, 5598, 20410, 30866, 9001, 12797, 27752, 6643, 5303, 2046, 9114, 16733, 23792, 24572, 18445, 8590, 9248, 13324, 17227, 23426, 16200, 14869, 18960, 4264, 20770, 18177, 13321, 27481, 6476, 30750, 6342, 13729, 23030, 25504, 25745, 10150, 6607, 25108, 7566, 325, 1909, 13160, 20255, 32712, 12532, 2058, 14994, 25766, 9096, 15920, 13595, 21437, 2826, 12794, 7548, 22616, 10969, 21053, 10701, 29577, 6217, 14494, 5470, 21446, 10433, 7783, 3469, 6777, 26159, 32081, 14738, 25751, 17903, 17885, 852, 29824, 26052, 11487, 12782, 5601, 11597, 21169, 23819, 18427, 28002, 22884, 27457, 32736, 13425, 6631, 6089, 10311, 6914, 319, 20017, 19612, 14235, 26835, 11981, 9266, 22217, 29568, 24730, 24048, 3737, 17117, 25638, 6494, 30244, 32048, 18972, 24179, 2969, 31024, 23780, 28371, 29842, 31941, 1501, 21890, 4038, 25680, 13473, 26862, 9674, 26868, 10460, 30527, 17570, 17421, 14813, 11151, 30908, 4449, 3160, 4071, 9960, 26606, 26883, 2883, 30816, 15048, 19148, 1841, 7212, 18231, 9564, 16367, 30521, 31054, 15706, 24754, 11511, 19904, 23337, 2329, 10070, 5113, 6917, 22262, 24879, 13646, 13491, 28020, 27398, 14807, 18234, 28839, 7593, 21074, 6545, 23462, 266, 6941, 3797, 11639, 31864, 15846, 12163, 30512, 29330, 31733, 4690, 4601, 27240, 20434, 15173, 29592, 31736, 19154, 22289, 11809, 10734, 7447, 23045, 26186, 32739, 3017, 13235, 27014, 26091, 4318, 8662, 25799, 14021, 31337, 30798, 32653, 16504, 20678, 29336, 30125, 17832, 11508, 5488, 16507, 31170, 12964, 17966, 254, 3404, 27627, 1948, 22649, 5878, 14435, 24224, 25439, 9186, 22941, 4997, 31861, 20851, 21634, 25290, 15861, 9153, 25924, 19136, 20571, 10892, 21753, 15319, 9311, 20199, 29089, 22914, 26317, 26752, 32763, 9972, 21884, 6134, 9302, 29616, 6822, 25403, 27270, 23834, 10856, 15852, 31322, 1674, 2076, 21083, 28422, 6944, 15352, 5872, 1981, 32245, 4294, 20568, 123, 27514, 15462, 20577, 29494, 8918, 2085, 4991, 13735, 5497, 16519, 32361, 23736, 3291, 23858, 19240, 19636, 2213, 21601, 22798, 32611, 9028, 2109, 7200, 27910, 27127, 6566, 1823, 15194, 29875, 15697, 19389, 7310, 31587, 1820, 30789, 17013, 1719, 11940, 29375, 19532, 19660, 17695, 12991, 11904, 2892, 9275, 2999, 14926, 31456, 4714, 2758, 31331, 12437, 12717, 26204, 12291, 30774, 2374, 5777, 4851, 12726, 2502, 10987, 5235, 7322, 15176, 18865, 19255, 17034, 26082, 14420, 13241, 12309, 19386, 29872, 1430, 2088, 31060, 11389, 23566, 15185, 17698, 7444, 11419, 31572, 5759, 3002, 32254, 1040, 11898, 15209, 15599, 24388, 24894, 19678, 13643, 22405, 23724, 23828, 24120, 17305, 14280, 13762, 11374, 15042, 18600, 12428, 21762, 15989, 26999, 25025, 3008, 26362, 26844, 32635, 32352, 31209, 16629, 4723, 8763, 147, 15965, 22542, 26886, 11642, 12324, 18844, 30405, 16793, 31471, 22277, 24766, 11264, 18728, 32751, 7837, 26603, 5232, 1028, 3660, 1022, 7465, 27496, 15858, 18216, 4470, 26195, 27669, 1302, 4571, 18222, 25010, 16138, 23054, 28190, 373, 2466, 1427, 7343, 3130, 8534, 7194, 11121, 18082, 24123, 27511, 25683, 25668, 4193, 25427, 25031, 4431, 2353, 19499, 7316, 26841, 1707, 12839, 11541, 18996, 24775, 17707, 31173, 17820, 28958, 2740, 21116, 16900, 15715, 31462, 22384, 14697, 8665, 20827, 4958, 12166, 17448, 22548, 24742, 13488, 6274, 29997, 31298, 21744, 10100, 19651, 24349, 26484, 8117, 7843, 18088, 16489, 22003, 3389, 9936, 13872, 27788, 17841, 22947, 11243, 11812, 1826, 6932, 7051, 23179, 20196, 29634, 32614, 13771, 18582, 2219, 9448, 27919, 28449, 15727, 23212, 3029, 10633, 14795, 15063, 12559, 14423, 30256, 7572, 6679, 8891, 17957, 9281, 12577, 14125, 23962, 21208, 5756, 4032, 8647, 10725, 13217, 1433, 3931, 15840, 16093, 25570, 18999, 24385, 17951, 17424, 29226, 27246, 8623, 12583, 26222, 26475, 19639, 26460, 650, 32748, 23069, 12065, 20559, 22780, 19267, 10088, 21640, 21732, 25019, 6286, 30134, 9445, 17010, 17031, 14810, 30658, 9037, 2463, 30250, 24921, 1037, 7992, 7745, 1326, 5101, 8659, 15730, 25796, 7709, 20327, 32376, 3922, 11279, 13500, 20973, 3154, 27526, 12949, 13354, 25177, 18993, 26874, 30384, 2600, 3270, 17838, 13503, 17293, 20306, 10326, 23185, 9427, 5512, 20550, 15584, 15742, 23170, 32108, 6819, 7054, 32126, 16, 1016, 15703, 25150, 12178, 2591, 20172, 22822, 30128, 17823, 659, 7864, 18597, 6316, 10243, 13432, 5676, 7373, 8442, 29941, 11056, 8454, 7775, 6730, 18502, 14176, 15525, 20086, 28354, 13792, 20881, 19845, 4762, 22697, 19815, 13920, 2782, 6742, 159, 3836, 1350, 16707, 1600, 17490, 14730, 9076, 22188, 31522, 27851, 10669, 951, 9067, 4613, 6846, 19065, 27815, 1359, 3961, 9606, 6325, 28881, 11178, 1219, 14733, 30700, 4649, 19434, 10898, 13393, 3735, 10779, 15906, 7897, 7885, 29554, 16704, 454, 18020, 17975, 18285, 20631, 21533, 10389, 23132, 17454, 9064, 32680, 16811, 29515, 13417, 21164, 30212, 18764, 16823, 21911, 30852, 11952, 11440, 9350, 15772, 30733, 7245, 3211, 3601, 7653, 11985, 424, 17091, 25713, 17058, 12348, 15486, 5173, 20872, 10017, 9091, 17603, 27940, 29786, 7632, 11726, 28896, 30334, 10005, 24817, 427, 1761, 30456, 2946, 15269, 25085, 9198, 19309, 11589, 15894, 10264, 27181, 11303, 18371, 9889, 11059, 22825, 4616, 11431, 32662, 4622, 20491, 7367, 19955, 16016, 14977, 23751, 14483, 28634, 13673, 3318, 20732, 29149, 23254, 29819, 10547, 8546, 19699, 23656, 5667, 11577, 32174, 19157, 14721, 5164, 8448, 28509, 4905, 15644, 9207, 5825, 5795, 11062, 14465, 26410, 27964, 32686, 32692, 4869, 3955, 3333, 27711, 27833, 15516, 25844, 26005, 12780, 11449, 17210, 26538, 3044, 22331, 25755, 19949, 11330, 25347, 19068, 30569, 22590, 5152, 5816, 15903, 26889, 1728, 27985, 14995, 24171, 31, 11729, 17999, 3720, 18940, 20336, 3580, 13039, 22828, 29012, 30078, 24406, 11318, 12494, 29280, 32299, 18916, 7909, 9633, 11860, 22447, 27416, 26273, 28485, 326, 25475, 2136, 10767, 22962, 22307, 9752, 23504, 10776, 10523, 4902, 31251, 2410, 4226, 4089, 24680, 19053, 5932, 14218, 32433, 9993, 16570, 21777, 7799, 8016, 8978, 5831, 6965, 15400, 1070, 19839, 27288, 32704, 22304, 22441, 21655, 7668, 2139, 12250, 30730, 24707, 25731, 17460, 26523, 7629, 20083, 14215, 5929, 2118, 969, 8192, 19690, 317, 17606, 11559, 24010, 11184, 19294, 13560, 25463, 30986, 7921, 27154, 15132, 25716, 25993, 19050, 20762, 17070, 6483, 30474, 2553, 436, 9862, 28625, 5682, 2657, 11041, 25219, 28342, 30974, 31236, 9222, 21024, 13798, 8186, 24034, 13655, 17198, 3431, 14715, 29271, 5944, 4479, 19830, 3711, 9493, 22042, 29932, 5417, 6197, 24183, 17076, 15647, 12229, 14447, 21920, 16040, 21685, 11035, 16031, 9100, 13941, 18794, 25335, 7260, 10806, 31653, 28244, 26788, 5539, 25338, 18919, 32046, 7882, 9737, 29947, 27178, 13822, 4774, 18142, 19333, 25359, 2392, 27020, 30313, 8585, 28619, 11327, 4518, 14572, 2803, 15239, 23001, 30471, 18928, 13533, 3327, 31254, 817, 9496, 957, 14319, 18514, 23903, 1862, 9201, 12247, 15757, 1100, 19428, 21658, 30304, 29676, 9219, 19580, 10672, 29944, 18407, 20351, 25344, 13018, 23873, 5149, 30328, 14316, 16147, 20214, 6617, 24022, 29679, 12878, 24933, 2014, 25999, 579, 1365, 16802, 23233, 29003, 32534, 16948, 3187, 13140, 6480, 14054, 3732, 15388, 1463, 1213, 841, 1481, 29548, 17987, 28217, 6599, 8573, 12905, 19300, 5658, 24573, 11199, 7251, 18392, 10642, 1198, 9329, 25094, 24701, 26672, 29664, 19062, 2267, 18023, 31242, 30965, 1722, 26407, 27970, 29649, 22081, 23486, 30325, 10934, 27854, 30063, 18109, 24662, 26106, 8168, 13328, 24085, 11497, 23823, 23153, 3378, 19729, 6647, 31570, 5846, 8109, 17639, 20408, 6403, 31034, 19488, 21834, 2458, 17395, 21843, 15010, 16978, 1416, 19765, 8749, 14126, 20914, 9279, 14623, 25761, 18565, 6108, 12408, 20268, 25270, 21825, 2193, 23299, 20810, 11899, 16320, 19735, 23034, 6632, 6906, 15433, 8085, 9779, 21307, 15960, 4131, 32463, 10827, 22766, 6367, 109, 31302, 31156, 25639, 19104, 13602, 16999, 30379, 26955, 3098, 23019, 27875, 20551, 32082, 22635, 19747, 28122, 103, 11375, 17398, 618, 27247, 15186, 2172, 25118, 31656, 29590, 9770, 7716, 18178, 7567, 7150, 25148, 15713, 12012, 18955, 12935, 20935, 31439, 14918, 26678, 1142, 4557, 15540, 20265, 20691, 22117, 23144, 1020, 12420, 26062, 7162, 26830, 27890, 28676, 8365, 29316, 24588, 9940, 29337, 20033, 3762, 13316, 7564, 1380, 29441, 4173, 30370, 26428, 2077, 28426, 1273, 17109, 25389, 4265, 15823, 19625, 11122, 1264, 27857, 20021, 11363, 28512, 31406, 868, 12399, 15418, 10985, 23147, 28256, 28801, 12021, 26020, 8219, 4414, 25002, 6114, 3607, 15966, 725, 10949, 14751, 22355, 2169, 18425, 31701, 14111, 15671, 16499, 31433, 8618, 5218, 16987, 27363, 8070, 8481, 23028, 21328, 22120, 14501, 19509, 6272, 20405, 11994, 25773, 18586, 31412, 32186, 1011, 24198, 24868, 11872, 30492, 25654, 11113, 19512, 895, 12423, 20271, 2312, 30376, 14632, 1651, 8615, 19226, 27092, 7674, 24341, 8353, 17809, 3381, 20164, 13980, 14361, 22903, 26586, 28539, 5480, 21179, 335, 5340, 6909, 21218, 18595, 25803, 4027, 20944, 20953, 31138, 14504, 4667, 12920, 22263, 26973, 493, 11750, 3223, 13968, 886, 29054, 30748, 23049, 31394, 28262, 24600, 31671, 31689, 3908, 7043, 6528, 14001, 18848, 25234, 627, 27467, 27339, 487, 20563, 13861, 12676, 13599, 20128, 9797, 5206, 5611, 11869, 17791, 22513, 13989, 20384, 2568, 2184, 26190, 5706, 9243, 16594, 12542, 17112, 23156, 24457, 6144, 32195, 5444, 28521, 2711, 10726, 10994, 24713, 14093, 1663, 3092, 2833, 633, 11735, 13971, 14924, 19994, 1404, 1529, 11607, 14912, 6489, 2199, 27595, 29965, 27619, 14909, 28932, 23290, 13986, 28670, 13614, 1532, 24725, 7055, 19476, 32466, 12691, 10035, 10702, 18059, 27071, 29828, 10455, 1377, 12131, 18184, 30620, 3622, 22126, 21602, 2178, 16192, 4962, 6102, 14263, 10461, 21718, 1946, 16624, 27622, 2982, 13745, 23531, 22909, 1544, 2196, 11351, 7180, 30215, 3384, 3113, 7811, 27208, 10964, 29048, 207, 11887, 18029, 4140, 25916, 1249, 22912, 10071, 16454, 26708, 5063, 31269, 16731, 4560, 10336, 14754, 30498, 10559, 25627, 25282, 26720, 24728, 2440, 6930, 10291, 6668, 10708, 12694, 14635, 26440, 30224, 7686, 31004, 16460, 13983, 19491, 22498, 3884, 6799, 18967, 9282, 9285, 12533, 12822, 4661, 32219, 6382, 4155, 10321, 10946, 32746, 26455, 17785, 3640, 23796, 213, 728, 15448, 16103, 1392, 22501, 14897, 32597, 7013, 22385, 1797, 24874, 21301, 15290, 20950, 11893, 22519, 17008, 28292, 1904, 15689, 17672, 2979, 9377, 16630, 21203, 21051, 17377, 12411, 4432, 18949, 31415, 21995, 2717, 26806, 7677, 27771, 26562, 24871, 18836, 31266, 8222, 1291, 29974, 12521, 4158, 12030, 13480, 17505, 20557, 28527, 27095, 14906, 24880, 24073, 26568, 31528, 16746, 1255, 12661, 14105, 16880, 371, 25636, 16466, 7028, 591, 21849, 2559, 23427, 27994, 14004, 14769, 15147, 23683, 7418, 14629, 17264, 22879, 6626, 16877, 30638, 746, 4935, 30641, 26693, 2303, 8859, 5620, 97, 2181, 10163, 20673, 26937, 29429, 31144, 1541, 12923, 31046, 22495, 22638, 14510, 31814, 11804, 18628, 9166, 9967, 17050, 14564, 26208, 1991, 5513, 10899, 27012, 9077, 28429, 29480, 29885, 3703, 3399, 20078, 18220, 12703, 675, 15344, 7758, 23195, 4867, 4310, 1205, 11920, 3694, 21486, 26613, 4724, 28572, 30040, 17973, 17324, 12852, 15728, 21364, 5787, 18509, 4072, 3804, 11786, 19551, 30805, 27289, 2631, 10339, 1866, 25059, 29239, 12840, 4739, 8776, 17172, 23579, 25687, 25166, 13394, 27795, 17565, 26607, 15365, 2390, 26759, 23201, 17955, 31463, 20858, 12965, 13102, 8913, 4587, 32767, 21888, 23317, 31097, 10220, 24127, 27149, 27015, 9160, 19164, 11405, 9583, 22290, 13662, 2881, 16118, 5793, 22159, 24523, 22671, 17708, 12322, 14177, 30013, 32255, 22695, 5629, 3012, 15335, 9312, 535, 26107, 16282, 21659, 26375, 2643, 17053, 18491, 8934, 14183, 12176, 24398, 10369, 26488, 12730, 31359, 19527, 4849, 28200, 6177, 16374, 28983, 20435, 24535, 24770, 4060, 7195, 7752, 7764, 2780, 15627, 20453, 6585, 20843, 28474, 14951, 25023, 31055, 10494, 21897, 11959, 26595, 9208, 23353, 14311, 29906, 23356, 8797, 3685, 27664, 8011, 31883, 17315, 10628, 12182, 28319, 28078, 9720, 23737, 28837, 6939, 20715, 21647, 785, 17169, 15883, 15746, 4995, 1178, 16017, 17193, 25318, 25583, 380, 27152, 27527, 32758, 18351, 32639, 19935, 18500, 3962, 2086, 8937, 27524, 9431, 3152, 13626, 31338, 1565, 2366, 12352, 31609, 7460, 29120, 20057, 26366, 14582, 10619, 16660, 11792, 2893, 13096, 31984, 11393, 4477, 30430, 273, 23624, 31320, 8636, 17181, 28697, 3396, 10610, 25452, 6445, 7874, 29507, 2479, 9988, 28596, 7219, 18634, 19810, 2491, 14442, 26503, 16511, 19033, 21239, 29775, 5915, 5400, 6016, 23972, 22427, 5141, 32109, 3009, 14579, 32654, 18908, 23341, 5510, 5, 17428, 13763, 10616, 12444, 8014, 20063, 17056, 12206, 13269, 17949, 15499, 13650, 12998, 3033, 24913, 28992, 28328, 19417, 11655, 19155, 11152, 23228, 5763, 18640, 29656, 21099, 19923, 10491, 2104, 10604, 25580, 11914, 14016, 28066, 11941, 904, 9190, 16127, 21248, 29614, 26336, 14445, 7237, 18077, 10890, 14278, 13891, 25309, 10250, 6823, 22829, 8779, 31719, 18205, 24401, 13495, 20837, 29644, 30272, 9973, 11953, 12465, 17946, 25854, 26997, 30790, 14305, 12834, 22674, 27158, 3661, 23603, 7621, 19554, 3280, 21507, 5239, 16526, 2125, 19274, 28563, 11143, 23490, 9205, 3655, 14171, 15353, 19271, 22013, 13513, 32240, 7213, 14573, 4069, 7478, 31347, 20084, 27810, 2908, 25312, 1464, 13775, 28959, 15061, 22543, 15466, 11682, 6820, 3536, 21367, 30793, 32374, 23600, 16666, 4209, 11, 15341, 11137, 157, 21391, 21373, 26771, 32249, 19262, 10488, 10113, 19390, 16240, 13900, 4075, 12584, 30692, 17967, 6287, 3146, 2259, 6174, 9687, 12718, 20581, 12611, 16910, 8675, 15216, 29608, 14549, 21614, 25437, 31874, 21891, 4084, 13382, 17157, 9976, 2902, 5391, 18247, 24002, 8773, 20474, 27003, 18878, 2119, 4489, 7743, 4858, 23088, 7624, 25163, 6296, 18494, 7883, 20995, 9038, 30412, 17029, 13912, 13531, 12581, 19265, 21906, 29504, 13501, 18479, 10765, 1827, 3414, 5245, 24014, 5790, 27932, 9970, 25190, 9300, 30177, 1321, 5266, 16404, 14838, 14817, 25815, 16919, 15853, 26494, 14561, 19542, 10816, 26804, 18911, 3486, 4361, 6746, 4367, 1521, 18170, 32452, 32053, 26938, 5829, 30183, 23246, 18012, 13028, 13451, 16696, 12114, 6493, 22627, 26250, 8044, 25863, 4138, 11048, 16699, 7270, 15770, 25089, 18560, 4385, 12751, 27304, 14886, 211, 6850, 25640, 16556, 30597, 28656, 13150, 30585, 6070, 21805, 29930, 735, 5171, 7374, 1747, 3846, 3465, 9890, 8205, 22752, 7249, 23642, 22585, 27313, 12498, 15934, 31404, 30987, 3069, 17363, 23017, 2000, 30454, 14326, 14749, 15008, 26292, 27215, 14740, 11090, 2929, 17491, 23130, 8068, 18956, 26944, 11453, 14088, 23136, 4364, 22079, 29793, 24717, 22067, 4927, 11852, 21555, 23145, 14356, 29046, 29171, 30225, 28775, 14061, 6213, 1640, 22076, 10554, 27977, 32693, 10262, 18792, 13302, 5025, 62, 24178, 24050, 24589, 297, 24437, 32175, 13552, 8696, 20644, 29296, 18822, 25107, 854, 16330, 6895, 7014, 27989, 29150, 26155, 10929, 18545, 2018, 13421, 6859, 19697, 16431, 23639, 25866, 10688, 3322, 22064, 27828, 4388, 9887, 26905, 9381, 309, 20132, 3328, 14350, 7675, 22496, 6085, 16815, 9640, 7133, 11054, 24705, 8032, 11718, 23630, 5832, 15139, 25357, 24318, 31133, 27956, 14487, 26024, 27822, 28126, 28635, 20382, 7151, 31267, 11700, 20227, 23520, 13567, 12266, 27188, 29302, 2956, 17598, 6463, 6481, 11962, 27837, 303, 1107, 2170, 11730, 12760, 7258, 6999, 4787, 12358, 568, 1119, 13180, 11060, 16312, 14341, 7666, 10789, 17777, 27060, 10810, 22487, 31934, 991, 29570, 18134, 25619, 9866, 4754, 8190, 22877, 11081, 17217, 19459, 3179, 9610, 14079, 29802, 30350, 15529, 17762, 25488, 15535, 23386, 13960, 27042, 29823, 22466, 10042, 10941, 28379, 14070, 20135, 2146, 10661, 8583, 21695, 22996, 5424, 31380, 5302, 12254, 1744, 7005, 13707, 13936, 22621, 18682, 8702, 27846, 23112, 21439, 25616, 18402, 24443, 23243, 11864, 26387, 32038, 30609, 32690, 23794, 2545, 8708, 15904, 7124, 10956, 16288, 5046, 30210, 19072, 985, 10938, 20498, 4257, 9518, 14234, 4403, 24949, 11325, 13019, 3721, 16172, 10530, 31258, 589, 2566, 9869, 3974, 6356, 12102, 13838, 26125, 27974, 17872, 26021, 10932, 18262, 27182, 17875, 4266, 5978, 25220, 23892, 29037, 19444, 31389, 17247, 6076, 27569, 17643, 1125, 4400, 7008, 18393, 20099, 21272, 15547, 169, 14752, 16071, 6359, 18000, 1247, 13159, 7261, 4004, 5704, 10030, 19453, 16839, 17634, 25351, 30341, 5686, 11715, 27962, 13948, 18932, 20900, 24157, 19977, 17604, 25622, 1890, 4275, 19176, 31627, 13436, 16428, 1128, 10000, 28111, 18944, 21040, 26152, 26798, 2027, 17339, 1348, 5817, 15276, 17378, 28745, 8196, 15633, 22749, 13710, 12531, 13010, 13305, 17506, 19873, 1363, 4132, 20790, 16943, 13579, 12486, 15907, 5153, 9735, 26277, 12111, 5305, 27581, 12135, 27307, 9759, 1235, 3596, 12141, 6740, 26813, 16851, 17342, 2917, 5314, 11995, 25467, 10682, 29692, 4013, 3587, 30999, 1875, 11876, 2932, 27194, 7517, 17244, 6082, 595, 15246, 22725, 25369, 29138, 27593, 5975, 15517, 28757, 5421, 32017, 32547, 21445, 26786, 2006, 1816, 19629, 22011, 10590, 22377, 3138, 22922, 28832, 14928, 21466, 16774, 4570, 4817, 7818, 9146, 16771, 22794, 28930, 3516, 10450, 9935, 26471, 23062, 32631, 31279, 3668, 12713, 14648, 1813, 24497, 23821, 28820, 1563, 10840, 8744, 18042, 25250, 22014, 6520, 14279, 13737, 1158, 18316, 13627, 14809, 1822, 24339, 26837, 20927, 9947, 640, 2718, 12147, 4293, 23312, 15568, 22898, 23547, 15845, 30383, 27784, 32083, 1810, 25131, 29880, 9164, 4186, 24220, 28043, 29853, 26197, 31303, 14389, 9914, 3001, 13484, 16086, 3626, 21716, 15291, 12579, 22133, 1688, 31714, 21886, 15824, 25429, 27361, 5987, 24351, 5496, 18587, 18742, 1956, 1923, 19528, 2709, 4156, 5213, 15687, 1551, 27120, 10745, 1658, 26170, 18072, 1173, 9694, 31324, 12930, 3406, 15288, 16488, 16637, 3108, 7467, 13350, 1676, 30103, 17804, 20945, 21734, 9030, 9000, 24357, 5749, 8753, 10572, 21329, 18447, 14276, 2846, 21344, 19358, 13868, 20055, 19495, 908, 29746, 1289, 21326, 3233, 4844, 11501, 3650, 8747, 15035, 17265, 23940, 29445, 2748, 4302, 6940, 24470, 4159, 18831, 8613, 14273, 6410, 19385, 20826, 23958, 14895, 2468, 29716, 6931, 1417, 1938, 5368, 11248, 18471, 3135, 24878, 26447, 32235, 15693, 12287, 646, 11641, 5347, 10480, 1274, 7041, 1149, 4555, 13097, 19876, 26578, 2489, 31949, 652, 6139, 31434, 14547, 4052, 10081, 6657, 9396, 24881, 31059, 21094, 31443, 256, 2331, 12945, 32753, 1176, 4031, 8518, 10349, 23967, 5356, 5490, 19915, 9822, 3367, 12436, 24741, 26599, 1962, 24997, 7845, 17295, 4037, 8750, 1920, 4695, 6925, 16500, 2867, 6904, 17530, 15595, 8357, 31327, 10608, 20439, 7333, 8607, 26864, 23708, 13362, 7309, 19373, 14800, 9557, 6928, 30276, 4707, 11227, 27876, 28963, 12028, 26337, 27516, 8256, 6815, 19489, 27891, 17914, 1536, 12537, 13880, 20317, 22249, 26307, 18959, 31306, 5344, 14416, 8104, 20972, 31193, 16479, 5609, 18611, 20939, 20415, 25399, 29228, 17670, 26081, 16869, 4043, 26956, 29219, 31836, 14243, 32473, 753, 29871, 6029, 2593, 18203, 2992, 9021, 20406, 17825, 26849, 15982, 22928, 19090, 26069, 32470, 1161, 12704, 21883, 21103, 19626, 19260, 30803, 8101, 17813, 16601, 3373, 30657, 1408, 25015, 32461, 10206, 4951, 17259, 19230, 13594, 18212, 11120, 25018, 22008, 15586, 28409, 22115, 1000, 25030, 8211, 7854, 17390, 16643, 1310, 31196, 4034, 15836, 17167, 4957, 2447, 8476, 10709, 12957, 22532, 10045, 10191, 22907, 29990, 20445, 10849, 30791, 17676, 2075, 1524, 20796, 30794, 31157, 250, 6648, 22106, 23836, 12552, 26572, 17429, 11096, 5728, 10703, 15818, 6523, 4543, 9664, 11111, 229, 21493, 7821, 12963, 30106, 18703, 2197, 16235, 1292, 2444, 5853, 29448, 16113, 31047, 31970, 14913, 16089, 11504, 19647, 16357, 6398, 32750, 27757, 3531, 6121, 6020, 6809, 5505, 10072, 15163, 1280, 3513, 6145, 25277, 9289, 21079, 11263, 11367, 14285, 6115, 19614, 20418, 25551, 5743, 2185, 28534, 29064, 30624, 16753, 20430, 11775, 19483, 22517, 23955, 25560, 32330, 506, 7714, 29740, 22666, 232, 2876, 15991, 32205, 24101, 15038, 18218, 9932, 14916, 18200, 30279, 3921, 13207, 19623, 17911, 4978, 7458, 10209, 26191, 381, 14797, 21621, 9819, 12546, 21990, 1789, 32342, 25265, 3123, 14246, 26042, 22392, 128, 13368, 29972, 29850, 20936, 17387, 12299, 6782, 26450, 24473, 6270, 4713, 17161, 18739, 19772, 25274, 28022, 8360, 25813, 11007])\n"
     ]
    }
   ],
   "source": [
    "# Display the keys of the processed_books dictionary\n",
    "print(processed_books.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3830 time buckets\n"
     ]
    }
   ],
   "source": [
    "keys_list = list(processed_books.keys())\n",
    "\n",
    "print(f\"We have {len(keys_list)} time buckets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([43, 69, 124, 29, 31, 50, 111, 32, 41, 47])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the keys of the stocks\n",
    "processed_books[5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10 stocks in each time bucket\n"
     ]
    }
   ],
   "source": [
    "keys_list_stocks = list(processed_books[5].keys())\n",
    "\n",
    "print(f\"We have {len(keys_list_stocks)} stocks in each time bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y data\n",
    "file_path = 'optimal_weights_hrp.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    optimal_weights = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise one stock for one time id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>time_id</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>book_train.parquet/stock_id</th>\n",
       "      <th>wap</th>\n",
       "      <th>log_return</th>\n",
       "      <th>spread</th>\n",
       "      <th>mid_price</th>\n",
       "      <th>order_imbalance</th>\n",
       "      <th>depth</th>\n",
       "      <th>bid_ask_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000013</td>\n",
       "      <td>1.000039</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000065</td>\n",
       "      <td>355</td>\n",
       "      <td>200</td>\n",
       "      <td>1555</td>\n",
       "      <td>1100</td>\n",
       "      <td>43</td>\n",
       "      <td>1.000030</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>0.279279</td>\n",
       "      <td>555</td>\n",
       "      <td>5.665799e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>1.000039</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>1.000065</td>\n",
       "      <td>855</td>\n",
       "      <td>355</td>\n",
       "      <td>355</td>\n",
       "      <td>655</td>\n",
       "      <td>43</td>\n",
       "      <td>1.000024</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>1.000013</td>\n",
       "      <td>0.413223</td>\n",
       "      <td>1210</td>\n",
       "      <td>4.293636e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000118</td>\n",
       "      <td>1.000144</td>\n",
       "      <td>1.000092</td>\n",
       "      <td>1.000170</td>\n",
       "      <td>355</td>\n",
       "      <td>400</td>\n",
       "      <td>855</td>\n",
       "      <td>55</td>\n",
       "      <td>43</td>\n",
       "      <td>1.000130</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.000131</td>\n",
       "      <td>-0.059603</td>\n",
       "      <td>755</td>\n",
       "      <td>-8.311070e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000196</td>\n",
       "      <td>1.000223</td>\n",
       "      <td>1.000170</td>\n",
       "      <td>1.000249</td>\n",
       "      <td>555</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>655</td>\n",
       "      <td>43</td>\n",
       "      <td>1.000213</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.000210</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>855</td>\n",
       "      <td>4.016601e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000432</td>\n",
       "      <td>1.000458</td>\n",
       "      <td>1.000406</td>\n",
       "      <td>1.000485</td>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>880</td>\n",
       "      <td>1300</td>\n",
       "      <td>43</td>\n",
       "      <td>1.000445</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.000445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>800</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>596</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001375</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>1.001349</td>\n",
       "      <td>1.001427</td>\n",
       "      <td>300</td>\n",
       "      <td>242</td>\n",
       "      <td>500</td>\n",
       "      <td>342</td>\n",
       "      <td>43</td>\n",
       "      <td>1.001389</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.001388</td>\n",
       "      <td>0.107011</td>\n",
       "      <td>542</td>\n",
       "      <td>2.134930e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>597</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001375</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>1.001349</td>\n",
       "      <td>1.001427</td>\n",
       "      <td>542</td>\n",
       "      <td>142</td>\n",
       "      <td>1100</td>\n",
       "      <td>542</td>\n",
       "      <td>43</td>\n",
       "      <td>1.001396</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.001388</td>\n",
       "      <td>0.584795</td>\n",
       "      <td>684</td>\n",
       "      <td>1.365229e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>598</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001480</td>\n",
       "      <td>1.001506</td>\n",
       "      <td>1.001454</td>\n",
       "      <td>1.001532</td>\n",
       "      <td>100</td>\n",
       "      <td>142</td>\n",
       "      <td>842</td>\n",
       "      <td>442</td>\n",
       "      <td>43</td>\n",
       "      <td>1.001491</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.001493</td>\n",
       "      <td>-0.173554</td>\n",
       "      <td>242</td>\n",
       "      <td>-7.637790e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>599</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>1.001427</td>\n",
       "      <td>1.001375</td>\n",
       "      <td>1.001454</td>\n",
       "      <td>500</td>\n",
       "      <td>142</td>\n",
       "      <td>500</td>\n",
       "      <td>942</td>\n",
       "      <td>43</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.001414</td>\n",
       "      <td>0.557632</td>\n",
       "      <td>642</td>\n",
       "      <td>1.322384e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>600</td>\n",
       "      <td>5</td>\n",
       "      <td>1.001401</td>\n",
       "      <td>1.001427</td>\n",
       "      <td>1.001375</td>\n",
       "      <td>1.001454</td>\n",
       "      <td>500</td>\n",
       "      <td>142</td>\n",
       "      <td>500</td>\n",
       "      <td>942</td>\n",
       "      <td>43</td>\n",
       "      <td>1.001422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1.001414</td>\n",
       "      <td>0.557632</td>\n",
       "      <td>642</td>\n",
       "      <td>1.322384e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     seconds_in_bucket  time_id  bid_price1  ask_price1  bid_price2  \\\n",
       "0                    1        5    1.000013    1.000039    0.999987   \n",
       "1                    2        5    0.999987    1.000039    0.999961   \n",
       "2                    3        5    1.000118    1.000144    1.000092   \n",
       "3                    4        5    1.000196    1.000223    1.000170   \n",
       "4                    5        5    1.000432    1.000458    1.000406   \n",
       "..                 ...      ...         ...         ...         ...   \n",
       "595                596        5    1.001375    1.001401    1.001349   \n",
       "596                597        5    1.001375    1.001401    1.001349   \n",
       "597                598        5    1.001480    1.001506    1.001454   \n",
       "598                599        5    1.001401    1.001427    1.001375   \n",
       "599                600        5    1.001401    1.001427    1.001375   \n",
       "\n",
       "     ask_price2  bid_size1  ask_size1  bid_size2  ask_size2  \\\n",
       "0      1.000065        355        200       1555       1100   \n",
       "1      1.000065        855        355        355        655   \n",
       "2      1.000170        355        400        855         55   \n",
       "3      1.000249        555        300        300        655   \n",
       "4      1.000485        400        400        880       1300   \n",
       "..          ...        ...        ...        ...        ...   \n",
       "595    1.001427        300        242        500        342   \n",
       "596    1.001427        542        142       1100        542   \n",
       "597    1.001532        100        142        842        442   \n",
       "598    1.001454        500        142        500        942   \n",
       "599    1.001454        500        142        500        942   \n",
       "\n",
       "     book_train.parquet/stock_id       wap  log_return    spread  mid_price  \\\n",
       "0                             43  1.000030    0.000110  0.000026   1.000026   \n",
       "1                             43  1.000024   -0.000006  0.000052   1.000013   \n",
       "2                             43  1.000130    0.000106  0.000026   1.000131   \n",
       "3                             43  1.000213    0.000083  0.000026   1.000210   \n",
       "4                             43  1.000445    0.000232  0.000026   1.000445   \n",
       "..                           ...       ...         ...       ...        ...   \n",
       "595                           43  1.001389    0.000123  0.000026   1.001388   \n",
       "596                           43  1.001396    0.000006  0.000026   1.001388   \n",
       "597                           43  1.001491    0.000095  0.000026   1.001493   \n",
       "598                           43  1.001422   -0.000069  0.000026   1.001414   \n",
       "599                           43  1.001422    0.000000  0.000026   1.001414   \n",
       "\n",
       "     order_imbalance  depth  bid_ask_slope  \n",
       "0           0.279279    555   5.665799e-08  \n",
       "1           0.413223   1210   4.293636e-08  \n",
       "2          -0.059603    755  -8.311070e-09  \n",
       "3           0.298246    855   4.016601e-08  \n",
       "4           0.000000    800   0.000000e+00  \n",
       "..               ...    ...            ...  \n",
       "595         0.107011    542   2.134930e-08  \n",
       "596         0.584795    684   1.365229e-07  \n",
       "597        -0.173554    242  -7.637790e-08  \n",
       "598         0.557632    642   1.322384e-07  \n",
       "599         0.557632    642   1.322384e-07  \n",
       "\n",
       "[600 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time bucket id = 5, stock id = 43\n",
    "processed_books[5][43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n",
    ".                                                                                        .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise optimal weights for one time id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    0.168368\n",
       "7    0.114538\n",
       "6    0.093119\n",
       "1    0.109424\n",
       "0    0.172541\n",
       "2    0.040840\n",
       "3    0.078670\n",
       "9    0.143965\n",
       "4    0.037650\n",
       "5    0.040885\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_weights[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that we need to sort our optimal weights\n",
    "sorted_optimal_weights = {}\n",
    "\n",
    "# Sort the elements for each time_id in optimal_weights\n",
    "for time_id in optimal_weights:\n",
    "    # Sort the series by index and add to the new dictionary\n",
    "    sorted_optimal_weights[time_id] = optimal_weights[time_id].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.172541\n",
       "1    0.109424\n",
       "2    0.040840\n",
       "3    0.078670\n",
       "4    0.037650\n",
       "5    0.040885\n",
       "6    0.093119\n",
       "7    0.114538\n",
       "8    0.168368\n",
       "9    0.143965\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking\n",
    "sorted_optimal_weights[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping only some columns\n",
    "\n",
    "Here we keep only the 4 last columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep_from_end = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary for the new structure\n",
    "processed_books_bis = {}\n",
    "\n",
    "# Iterate over the time buckets\n",
    "for time_id in processed_books:\n",
    "    # Initialize an empty dictionary for each time bucket in the new structure\n",
    "    processed_books_bis[time_id] = {}\n",
    "    \n",
    "    # Iterate over the stock ids within each time bucket\n",
    "    for stock_id in processed_books[time_id]:\n",
    "        # Select the DataFrame corresponding to the current stock id and time id\n",
    "        df = processed_books[time_id][stock_id]\n",
    "        \n",
    "        # Select the last columns_to_keep_from_end columns of the DataFrame\n",
    "        df_bis = df.iloc[:, -columns_to_keep_from_end:]\n",
    "        \n",
    "        # Store the reduced DataFrame in the new structure\n",
    "        processed_books_bis[time_id][stock_id] = df_bis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid_price</th>\n",
       "      <th>order_imbalance</th>\n",
       "      <th>depth</th>\n",
       "      <th>bid_ask_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000941</td>\n",
       "      <td>0.445152</td>\n",
       "      <td>1413</td>\n",
       "      <td>2.961265e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000941</td>\n",
       "      <td>0.614012</td>\n",
       "      <td>1513</td>\n",
       "      <td>4.908748e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000941</td>\n",
       "      <td>0.614012</td>\n",
       "      <td>1513</td>\n",
       "      <td>4.908748e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000753</td>\n",
       "      <td>0.902748</td>\n",
       "      <td>1892</td>\n",
       "      <td>1.941355e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000753</td>\n",
       "      <td>0.832898</td>\n",
       "      <td>2298</td>\n",
       "      <td>8.909370e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>1.002448</td>\n",
       "      <td>0.702851</td>\n",
       "      <td>1999</td>\n",
       "      <td>5.231117e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1.002448</td>\n",
       "      <td>0.235747</td>\n",
       "      <td>1298</td>\n",
       "      <td>1.446477e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1.002448</td>\n",
       "      <td>0.235747</td>\n",
       "      <td>1298</td>\n",
       "      <td>1.446477e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1.002448</td>\n",
       "      <td>0.147353</td>\n",
       "      <td>1398</td>\n",
       "      <td>8.097339e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1.002448</td>\n",
       "      <td>0.147353</td>\n",
       "      <td>1398</td>\n",
       "      <td>8.097339e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mid_price  order_imbalance  depth  bid_ask_slope\n",
       "0     1.000941         0.445152   1413   2.961265e-07\n",
       "1     1.000941         0.614012   1513   4.908748e-07\n",
       "2     1.000941         0.614012   1513   4.908748e-07\n",
       "3     1.000753         0.902748   1892   1.941355e-06\n",
       "4     1.000753         0.832898   2298   8.909370e-07\n",
       "..         ...              ...    ...            ...\n",
       "595   1.002448         0.702851   1999   5.231117e-07\n",
       "596   1.002448         0.235747   1298   1.446477e-07\n",
       "597   1.002448         0.235747   1298   1.446477e-07\n",
       "598   1.002448         0.147353   1398   8.097339e-08\n",
       "599   1.002448         0.147353   1398   8.097339e-08\n",
       "\n",
       "[600 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test\n",
    "processed_books_bis[5][47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape the data for NN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([3830, 10, 4, 600])\n",
      "Y shape: torch.Size([3830, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create lists for inputs (X) and outputs (Y)\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for time_id in processed_books_bis:\n",
    "    # Combine the dataframes for all stocks into a single 3D array for the time_id\n",
    "    stock_data = []\n",
    "    for stock_id in sorted(processed_books_bis[time_id].keys()):  # Ensure consistent order\n",
    "        stock_df = processed_books_bis[time_id][stock_id].values.T  # Transpose \n",
    "        stock_data.append(stock_df)\n",
    "    \n",
    "    # Stack to get a 3D array of shape (5, 2, 600)\n",
    "    combined_df = np.stack(stock_data)\n",
    "    \n",
    "    # Add to the list of inputs (X)\n",
    "    X.append(combined_df)\n",
    "    \n",
    "    # Convert the optimal_weights for this time_id to a numpy array and add to the list of outputs (Y)\n",
    "    weights = np.array(sorted_optimal_weights[time_id])\n",
    "    Y.append(weights)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# Verify shapes\n",
    "print(f'X shape: {X.shape}') \n",
    "print(f'Y shape: {Y.shape}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (simple implementation of a fixed model without parameter tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "dataset = TensorDataset(X, Y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CNN model with two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=columns_to_keep_from_end, out_channels=16, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "\n",
    "        # Calculate the flattened size after convolution and pooling\n",
    "        self.flattened_size = 32 * 10 * 150  # Placeholder size, will be adjusted dynamically\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)  # Apply softmax activation\n",
    "\n",
    "        return x\n",
    "    \n",
    "model = SimpleCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 0.0658, Train RMSE: 0.2559, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/40, Train Loss: 0.0648, Train RMSE: 0.2545, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/40, Train Loss: 0.0554, Train RMSE: 0.2305, Val Loss: 0.0057, Val RMSE: 0.0752\n",
      "Epoch 4/40, Train Loss: 0.0053, Train RMSE: 0.0723, Val Loss: 0.0043, Val RMSE: 0.0655\n",
      "Epoch 5/40, Train Loss: 0.0042, Train RMSE: 0.0642, Val Loss: 0.0036, Val RMSE: 0.0595\n",
      "Epoch 6/40, Train Loss: 0.0035, Train RMSE: 0.0589, Val Loss: 0.0031, Val RMSE: 0.0556\n",
      "Epoch 7/40, Train Loss: 0.0031, Train RMSE: 0.0555, Val Loss: 0.0029, Val RMSE: 0.0532\n",
      "Epoch 8/40, Train Loss: 0.0029, Train RMSE: 0.0535, Val Loss: 0.0028, Val RMSE: 0.0519\n",
      "Epoch 9/40, Train Loss: 0.0028, Train RMSE: 0.0523, Val Loss: 0.0027, Val RMSE: 0.0513\n",
      "Epoch 10/40, Train Loss: 0.0027, Train RMSE: 0.0516, Val Loss: 0.0027, Val RMSE: 0.0510\n",
      "Epoch 11/40, Train Loss: 0.0027, Train RMSE: 0.0511, Val Loss: 0.0026, Val RMSE: 0.0508\n",
      "Epoch 12/40, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 13/40, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 14/40, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0026, Val RMSE: 0.0505\n",
      "Epoch 15/40, Train Loss: 0.0026, Train RMSE: 0.0507, Val Loss: 0.0026, Val RMSE: 0.0505\n",
      "Epoch 16/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 17/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 18/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 19/40, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 20/40, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 21/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 22/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 23/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 24/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 25/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 26/40, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 27/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 28/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 29/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 30/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 31/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 32/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 33/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 34/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 35/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 36/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 37/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 38/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 39/40, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 40/40, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n"
     ]
    }
   ],
   "source": [
    "# Define Mean Squared Error (MSE) Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer with Adam and a learning rate of 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define Root Mean Squared Error (RMSE) function\n",
    "def rmse(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error (RMSE) between predictions and targets.\n",
    "    \n",
    "    Args:\n",
    "    - predictions (torch.Tensor): Predicted values.\n",
    "    - targets (torch.Tensor): True target values.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: RMSE value.\n",
    "    \"\"\"\n",
    "    return torch.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "# Define the training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model using the training data.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): Neural network model.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "    - criterion (torch.nn.Module): Loss function.\n",
    "    - optimizer (torch.optim.Optimizer): Optimization algorithm.\n",
    "    - device (torch.device): Device (CPU or GPU) to perform operations.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Mean training loss.\n",
    "    - float: Mean training RMSE.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_rmse = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Calculate the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "        running_rmse += rmse(outputs, targets).item()  # Accumulate RMSE\n",
    "    return running_loss / len(train_loader), running_rmse / len(train_loader)\n",
    "\n",
    "# Define the validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model using the validation data.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): Neural network model.\n",
    "    - val_loader (torch.utils.data.DataLoader): DataLoader for the validation data.\n",
    "    - criterion (torch.nn.Module): Loss function.\n",
    "    - device (torch.device): Device (CPU or GPU) to perform operations.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Mean validation loss.\n",
    "    - float: Mean validation RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_rmse = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Calculate the loss\n",
    "            val_loss += loss.item()  # Accumulate loss\n",
    "            val_rmse += rmse(outputs, targets).item()  # Accumulate RMSE\n",
    "    return val_loss / len(val_loader), val_rmse / len(val_loader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_rmse = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_rmse = validate(model, val_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a few predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted optimal weights for the first input:\n",
      "tensor([[0.2601, 0.0875, 0.0360, 0.1458, 0.0559, 0.0504, 0.0793, 0.0693, 0.1131,\n",
      "         0.1026]], device='cuda:0')\n",
      "True weights for the first input:\n",
      "tensor([0.3829, 0.0561, 0.0214, 0.1632, 0.0188, 0.0228, 0.0874, 0.0372, 0.1462,\n",
      "        0.0639], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Take the first input from the training data\n",
    "first_input, true_weights = train_dataset[0]  # Retrieve both input and true weights\n",
    "first_input = first_input.unsqueeze(0).to(device)  # Add a batch dimension\n",
    "true_weights = true_weights.to(device)  # Send true weights to the same device (CPU/GPU)\n",
    "\n",
    "# Perform prediction with the model\n",
    "with torch.no_grad():\n",
    "    predicted_weights = model(first_input)\n",
    "\n",
    "# Display predicted optimal weights and true weights\n",
    "print(\"Predicted optimal weights for the first input:\")\n",
    "print(predicted_weights)\n",
    "\n",
    "print(\"True weights for the first input:\")\n",
    "print(true_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with num_layers=1, dropout=0.5, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0640, Train RMSE: 0.2523, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 5/60, Train Loss: 0.0646, Train RMSE: 0.2542, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 6/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 7/60, Train Loss: 0.0646, Train RMSE: 0.2540, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 8/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 9/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 10/60, Train Loss: 0.0663, Train RMSE: 0.2574, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 11/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 12/60, Train Loss: 0.0647, Train RMSE: 0.2542, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 13/60, Train Loss: 0.0659, Train RMSE: 0.2566, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 14/60, Train Loss: 0.0646, Train RMSE: 0.2541, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 15/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 16/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 17/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 18/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 19/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 20/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 21/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 22/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 23/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 24/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 25/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 26/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 27/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 28/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 29/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 30/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 31/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 32/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 33/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 34/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 35/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 36/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 37/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 38/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 39/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 40/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 41/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 42/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 43/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 44/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 45/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 46/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 47/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 48/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 49/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 50/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 51/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 52/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 53/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 54/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 55/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 56/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 57/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 58/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 59/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 60/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Training model with num_layers=1, dropout=0.5, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0041, Train RMSE: 0.0632, Val Loss: 0.0027, Val RMSE: 0.0519\n",
      "Epoch 2/60, Train Loss: 0.0035, Train RMSE: 0.0583, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 3/60, Train Loss: 0.0033, Train RMSE: 0.0572, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 4/60, Train Loss: 0.0033, Train RMSE: 0.0566, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 5/60, Train Loss: 0.0031, Train RMSE: 0.0554, Val Loss: 0.0028, Val RMSE: 0.0521\n",
      "Epoch 6/60, Train Loss: 0.0030, Train RMSE: 0.0542, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 7/60, Train Loss: 0.0029, Train RMSE: 0.0534, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 8/60, Train Loss: 0.0029, Train RMSE: 0.0533, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 9/60, Train Loss: 0.0028, Train RMSE: 0.0523, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 10/60, Train Loss: 0.0028, Train RMSE: 0.0521, Val Loss: 0.0028, Val RMSE: 0.0520\n",
      "Epoch 11/60, Train Loss: 0.0027, Train RMSE: 0.0512, Val Loss: 0.0024, Val RMSE: 0.0489\n",
      "Epoch 12/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 13/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 14/60, Train Loss: 0.0026, Train RMSE: 0.0499, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 15/60, Train Loss: 0.0026, Train RMSE: 0.0500, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 16/60, Train Loss: 0.0025, Train RMSE: 0.0491, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 17/60, Train Loss: 0.0024, Train RMSE: 0.0484, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 18/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 19/60, Train Loss: 0.0024, Train RMSE: 0.0487, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 20/60, Train Loss: 0.0023, Train RMSE: 0.0476, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 21/60, Train Loss: 0.0023, Train RMSE: 0.0476, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 22/60, Train Loss: 0.0024, Train RMSE: 0.0482, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 23/60, Train Loss: 0.0024, Train RMSE: 0.0479, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 24/60, Train Loss: 0.0023, Train RMSE: 0.0472, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 25/60, Train Loss: 0.0023, Train RMSE: 0.0475, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 26/60, Train Loss: 0.0023, Train RMSE: 0.0471, Val Loss: 0.0025, Val RMSE: 0.0488\n",
      "Epoch 27/60, Train Loss: 0.0022, Train RMSE: 0.0461, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 28/60, Train Loss: 0.0022, Train RMSE: 0.0461, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 29/60, Train Loss: 0.0021, Train RMSE: 0.0458, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 30/60, Train Loss: 0.0020, Train RMSE: 0.0446, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 31/60, Train Loss: 0.0021, Train RMSE: 0.0452, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 32/60, Train Loss: 0.0020, Train RMSE: 0.0447, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 33/60, Train Loss: 0.0020, Train RMSE: 0.0443, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 34/60, Train Loss: 0.0019, Train RMSE: 0.0436, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 35/60, Train Loss: 0.0020, Train RMSE: 0.0440, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 36/60, Train Loss: 0.0019, Train RMSE: 0.0434, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 37/60, Train Loss: 0.0019, Train RMSE: 0.0433, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 38/60, Train Loss: 0.0019, Train RMSE: 0.0428, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 39/60, Train Loss: 0.0019, Train RMSE: 0.0427, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 40/60, Train Loss: 0.0019, Train RMSE: 0.0430, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 41/60, Train Loss: 0.0018, Train RMSE: 0.0423, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 42/60, Train Loss: 0.0019, Train RMSE: 0.0432, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 43/60, Train Loss: 0.0018, Train RMSE: 0.0417, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 44/60, Train Loss: 0.0017, Train RMSE: 0.0410, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 45/60, Train Loss: 0.0018, Train RMSE: 0.0415, Val Loss: 0.0026, Val RMSE: 0.0498\n",
      "Epoch 46/60, Train Loss: 0.0017, Train RMSE: 0.0409, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 47/60, Train Loss: 0.0018, Train RMSE: 0.0418, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 48/60, Train Loss: 0.0018, Train RMSE: 0.0418, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 49/60, Train Loss: 0.0018, Train RMSE: 0.0415, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 50/60, Train Loss: 0.0017, Train RMSE: 0.0408, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 51/60, Train Loss: 0.0017, Train RMSE: 0.0407, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 52/60, Train Loss: 0.0016, Train RMSE: 0.0401, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 53/60, Train Loss: 0.0016, Train RMSE: 0.0399, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 54/60, Train Loss: 0.0017, Train RMSE: 0.0403, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 55/60, Train Loss: 0.0016, Train RMSE: 0.0399, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 56/60, Train Loss: 0.0016, Train RMSE: 0.0399, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 57/60, Train Loss: 0.0017, Train RMSE: 0.0401, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 58/60, Train Loss: 0.0016, Train RMSE: 0.0398, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 59/60, Train Loss: 0.0016, Train RMSE: 0.0397, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 60/60, Train Loss: 0.0017, Train RMSE: 0.0402, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Training model with num_layers=1, dropout=0.6, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0655, Train RMSE: 0.2554, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0646, Train RMSE: 0.2540, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 5/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 6/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 7/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 8/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 9/60, Train Loss: 0.0670, Train RMSE: 0.2586, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 10/60, Train Loss: 0.0646, Train RMSE: 0.2541, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 11/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 12/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 13/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 14/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 15/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 16/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 17/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 18/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 19/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 20/60, Train Loss: 0.0644, Train RMSE: 0.2536, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 21/60, Train Loss: 0.0643, Train RMSE: 0.2536, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 22/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 23/60, Train Loss: 0.0643, Train RMSE: 0.2536, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 24/60, Train Loss: 0.0643, Train RMSE: 0.2536, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 25/60, Train Loss: 0.0643, Train RMSE: 0.2535, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 26/60, Train Loss: 0.0643, Train RMSE: 0.2535, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 27/60, Train Loss: 0.0637, Train RMSE: 0.2523, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 28/60, Train Loss: 0.0632, Train RMSE: 0.2513, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 29/60, Train Loss: 0.0631, Train RMSE: 0.2511, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 30/60, Train Loss: 0.0618, Train RMSE: 0.2485, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 31/60, Train Loss: 0.0613, Train RMSE: 0.2474, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 32/60, Train Loss: 0.0612, Train RMSE: 0.2474, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 33/60, Train Loss: 0.0614, Train RMSE: 0.2477, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 34/60, Train Loss: 0.0615, Train RMSE: 0.2478, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 35/60, Train Loss: 0.0614, Train RMSE: 0.2476, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 36/60, Train Loss: 0.0617, Train RMSE: 0.2482, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 37/60, Train Loss: 0.0616, Train RMSE: 0.2481, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 38/60, Train Loss: 0.0601, Train RMSE: 0.2450, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 39/60, Train Loss: 0.0594, Train RMSE: 0.2435, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 40/60, Train Loss: 0.0583, Train RMSE: 0.2412, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 41/60, Train Loss: 0.0521, Train RMSE: 0.2280, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 42/60, Train Loss: 0.0523, Train RMSE: 0.2284, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 43/60, Train Loss: 0.0517, Train RMSE: 0.2271, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 44/60, Train Loss: 0.0519, Train RMSE: 0.2275, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 45/60, Train Loss: 0.0502, Train RMSE: 0.2238, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 46/60, Train Loss: 0.0512, Train RMSE: 0.2259, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 47/60, Train Loss: 0.0498, Train RMSE: 0.2228, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 48/60, Train Loss: 0.0501, Train RMSE: 0.2235, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 49/60, Train Loss: 0.0513, Train RMSE: 0.2262, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 50/60, Train Loss: 0.0513, Train RMSE: 0.2261, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 51/60, Train Loss: 0.0511, Train RMSE: 0.2257, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 52/60, Train Loss: 0.0512, Train RMSE: 0.2260, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 53/60, Train Loss: 0.0514, Train RMSE: 0.2264, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 54/60, Train Loss: 0.0506, Train RMSE: 0.2247, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 55/60, Train Loss: 0.0431, Train RMSE: 0.2072, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 56/60, Train Loss: 0.0418, Train RMSE: 0.2040, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 57/60, Train Loss: 0.0411, Train RMSE: 0.2021, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 58/60, Train Loss: 0.0421, Train RMSE: 0.2047, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 59/60, Train Loss: 0.0416, Train RMSE: 0.2033, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 60/60, Train Loss: 0.0425, Train RMSE: 0.2056, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Training model with num_layers=1, dropout=0.6, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0045, Train RMSE: 0.0660, Val Loss: 0.0030, Val RMSE: 0.0546\n",
      "Epoch 2/60, Train Loss: 0.0039, Train RMSE: 0.0622, Val Loss: 0.0030, Val RMSE: 0.0540\n",
      "Epoch 3/60, Train Loss: 0.0038, Train RMSE: 0.0609, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 4/60, Train Loss: 0.0034, Train RMSE: 0.0580, Val Loss: 0.0027, Val RMSE: 0.0518\n",
      "Epoch 5/60, Train Loss: 0.0034, Train RMSE: 0.0574, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 6/60, Train Loss: 0.0032, Train RMSE: 0.0563, Val Loss: 0.0025, Val RMSE: 0.0499\n",
      "Epoch 7/60, Train Loss: 0.0031, Train RMSE: 0.0551, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 8/60, Train Loss: 0.0030, Train RMSE: 0.0547, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 9/60, Train Loss: 0.0030, Train RMSE: 0.0540, Val Loss: 0.0026, Val RMSE: 0.0508\n",
      "Epoch 10/60, Train Loss: 0.0029, Train RMSE: 0.0532, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 11/60, Train Loss: 0.0028, Train RMSE: 0.0528, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 12/60, Train Loss: 0.0028, Train RMSE: 0.0527, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 13/60, Train Loss: 0.0028, Train RMSE: 0.0520, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 14/60, Train Loss: 0.0027, Train RMSE: 0.0518, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 15/60, Train Loss: 0.0027, Train RMSE: 0.0515, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 16/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 17/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 18/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 19/60, Train Loss: 0.0027, Train RMSE: 0.0511, Val Loss: 0.0027, Val RMSE: 0.0510\n",
      "Epoch 20/60, Train Loss: 0.0025, Train RMSE: 0.0498, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 21/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 22/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0027, Val RMSE: 0.0509\n",
      "Epoch 23/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 24/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 25/60, Train Loss: 0.0024, Train RMSE: 0.0485, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 26/60, Train Loss: 0.0024, Train RMSE: 0.0480, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 27/60, Train Loss: 0.0024, Train RMSE: 0.0485, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 28/60, Train Loss: 0.0024, Train RMSE: 0.0482, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 29/60, Train Loss: 0.0023, Train RMSE: 0.0477, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 30/60, Train Loss: 0.0023, Train RMSE: 0.0478, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 31/60, Train Loss: 0.0023, Train RMSE: 0.0476, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 32/60, Train Loss: 0.0023, Train RMSE: 0.0470, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 33/60, Train Loss: 0.0023, Train RMSE: 0.0471, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 34/60, Train Loss: 0.0023, Train RMSE: 0.0469, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 35/60, Train Loss: 0.0023, Train RMSE: 0.0479, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 36/60, Train Loss: 0.0023, Train RMSE: 0.0472, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 37/60, Train Loss: 0.0023, Train RMSE: 0.0470, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 38/60, Train Loss: 0.0022, Train RMSE: 0.0462, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 39/60, Train Loss: 0.0021, Train RMSE: 0.0456, Val Loss: 0.0026, Val RMSE: 0.0507\n",
      "Epoch 40/60, Train Loss: 0.0021, Train RMSE: 0.0456, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 41/60, Train Loss: 0.0021, Train RMSE: 0.0454, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 42/60, Train Loss: 0.0021, Train RMSE: 0.0448, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 43/60, Train Loss: 0.0021, Train RMSE: 0.0450, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 44/60, Train Loss: 0.0021, Train RMSE: 0.0452, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 45/60, Train Loss: 0.0021, Train RMSE: 0.0449, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 46/60, Train Loss: 0.0021, Train RMSE: 0.0450, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 47/60, Train Loss: 0.0020, Train RMSE: 0.0442, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 48/60, Train Loss: 0.0021, Train RMSE: 0.0447, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 49/60, Train Loss: 0.0020, Train RMSE: 0.0439, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 50/60, Train Loss: 0.0019, Train RMSE: 0.0429, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 51/60, Train Loss: 0.0021, Train RMSE: 0.0449, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 52/60, Train Loss: 0.0020, Train RMSE: 0.0436, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 53/60, Train Loss: 0.0018, Train RMSE: 0.0421, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 54/60, Train Loss: 0.0020, Train RMSE: 0.0438, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 55/60, Train Loss: 0.0019, Train RMSE: 0.0427, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 56/60, Train Loss: 0.0019, Train RMSE: 0.0428, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 57/60, Train Loss: 0.0019, Train RMSE: 0.0426, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 58/60, Train Loss: 0.0018, Train RMSE: 0.0421, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 59/60, Train Loss: 0.0019, Train RMSE: 0.0424, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 60/60, Train Loss: 0.0019, Train RMSE: 0.0424, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Training model with num_layers=1, dropout=0.7, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0649, Train RMSE: 0.2540, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0650, Train RMSE: 0.2548, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0569, Train RMSE: 0.2383, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0501, Train RMSE: 0.2235, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 5/60, Train Loss: 0.0406, Train RMSE: 0.2006, Val Loss: 0.0657, Val RMSE: 0.2564\n",
      "Epoch 6/60, Train Loss: 0.0139, Train RMSE: 0.1152, Val Loss: 0.0044, Val RMSE: 0.0658\n",
      "Epoch 7/60, Train Loss: 0.0038, Train RMSE: 0.0611, Val Loss: 0.0032, Val RMSE: 0.0564\n",
      "Epoch 8/60, Train Loss: 0.0032, Train RMSE: 0.0559, Val Loss: 0.0029, Val RMSE: 0.0531\n",
      "Epoch 9/60, Train Loss: 0.0029, Train RMSE: 0.0533, Val Loss: 0.0027, Val RMSE: 0.0516\n",
      "Epoch 10/60, Train Loss: 0.0028, Train RMSE: 0.0518, Val Loss: 0.0027, Val RMSE: 0.0509\n",
      "Epoch 11/60, Train Loss: 0.0027, Train RMSE: 0.0512, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 12/60, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 13/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 14/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 15/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 16/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 17/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 18/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 19/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 20/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 21/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 22/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 23/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 24/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 25/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 26/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 27/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 28/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 29/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 30/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 31/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 32/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 33/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 34/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 35/60, Train Loss: 0.0026, Train RMSE: 0.0502, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 36/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 37/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 38/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 39/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 40/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 41/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 42/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 43/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 44/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 45/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 46/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 47/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 48/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0498\n",
      "Epoch 49/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 50/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 51/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 52/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 53/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 54/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 55/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 56/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 57/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 58/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 59/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 60/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Training model with num_layers=1, dropout=0.7, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0046, Train RMSE: 0.0673, Val Loss: 0.0030, Val RMSE: 0.0545\n",
      "Epoch 2/60, Train Loss: 0.0042, Train RMSE: 0.0644, Val Loss: 0.0031, Val RMSE: 0.0551\n",
      "Epoch 3/60, Train Loss: 0.0042, Train RMSE: 0.0643, Val Loss: 0.0037, Val RMSE: 0.0605\n",
      "Epoch 4/60, Train Loss: 0.0041, Train RMSE: 0.0636, Val Loss: 0.0027, Val RMSE: 0.0512\n",
      "Epoch 5/60, Train Loss: 0.0038, Train RMSE: 0.0610, Val Loss: 0.0032, Val RMSE: 0.0562\n",
      "Epoch 6/60, Train Loss: 0.0037, Train RMSE: 0.0605, Val Loss: 0.0028, Val RMSE: 0.0526\n",
      "Epoch 7/60, Train Loss: 0.0038, Train RMSE: 0.0615, Val Loss: 0.0030, Val RMSE: 0.0543\n",
      "Epoch 8/60, Train Loss: 0.0037, Train RMSE: 0.0602, Val Loss: 0.0030, Val RMSE: 0.0544\n",
      "Epoch 9/60, Train Loss: 0.0037, Train RMSE: 0.0603, Val Loss: 0.0026, Val RMSE: 0.0508\n",
      "Epoch 10/60, Train Loss: 0.0035, Train RMSE: 0.0585, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 11/60, Train Loss: 0.0035, Train RMSE: 0.0582, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 12/60, Train Loss: 0.0035, Train RMSE: 0.0583, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 13/60, Train Loss: 0.0035, Train RMSE: 0.0584, Val Loss: 0.0030, Val RMSE: 0.0540\n",
      "Epoch 14/60, Train Loss: 0.0034, Train RMSE: 0.0580, Val Loss: 0.0027, Val RMSE: 0.0515\n",
      "Epoch 15/60, Train Loss: 0.0034, Train RMSE: 0.0579, Val Loss: 0.0028, Val RMSE: 0.0526\n",
      "Epoch 16/60, Train Loss: 0.0033, Train RMSE: 0.0572, Val Loss: 0.0026, Val RMSE: 0.0498\n",
      "Epoch 17/60, Train Loss: 0.0033, Train RMSE: 0.0571, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 18/60, Train Loss: 0.0033, Train RMSE: 0.0567, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 19/60, Train Loss: 0.0032, Train RMSE: 0.0560, Val Loss: 0.0026, Val RMSE: 0.0507\n",
      "Epoch 20/60, Train Loss: 0.0032, Train RMSE: 0.0564, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 21/60, Train Loss: 0.0032, Train RMSE: 0.0557, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 22/60, Train Loss: 0.0032, Train RMSE: 0.0557, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 23/60, Train Loss: 0.0032, Train RMSE: 0.0563, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 24/60, Train Loss: 0.0030, Train RMSE: 0.0546, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 25/60, Train Loss: 0.0030, Train RMSE: 0.0545, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 26/60, Train Loss: 0.0031, Train RMSE: 0.0549, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 27/60, Train Loss: 0.0031, Train RMSE: 0.0548, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 28/60, Train Loss: 0.0031, Train RMSE: 0.0552, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 29/60, Train Loss: 0.0031, Train RMSE: 0.0547, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 30/60, Train Loss: 0.0030, Train RMSE: 0.0540, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 31/60, Train Loss: 0.0029, Train RMSE: 0.0536, Val Loss: 0.0026, Val RMSE: 0.0505\n",
      "Epoch 32/60, Train Loss: 0.0029, Train RMSE: 0.0531, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 33/60, Train Loss: 0.0029, Train RMSE: 0.0529, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 34/60, Train Loss: 0.0029, Train RMSE: 0.0528, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 35/60, Train Loss: 0.0029, Train RMSE: 0.0529, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 36/60, Train Loss: 0.0027, Train RMSE: 0.0517, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 37/60, Train Loss: 0.0028, Train RMSE: 0.0519, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 38/60, Train Loss: 0.0028, Train RMSE: 0.0520, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 39/60, Train Loss: 0.0028, Train RMSE: 0.0524, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 40/60, Train Loss: 0.0027, Train RMSE: 0.0519, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 41/60, Train Loss: 0.0027, Train RMSE: 0.0514, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 42/60, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0028, Val RMSE: 0.0527\n",
      "Epoch 43/60, Train Loss: 0.0027, Train RMSE: 0.0512, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 44/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 45/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0025, Val RMSE: 0.0499\n",
      "Epoch 46/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 47/60, Train Loss: 0.0026, Train RMSE: 0.0499, Val Loss: 0.0025, Val RMSE: 0.0488\n",
      "Epoch 48/60, Train Loss: 0.0025, Train RMSE: 0.0497, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 49/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 50/60, Train Loss: 0.0026, Train RMSE: 0.0499, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 51/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 52/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0025, Val RMSE: 0.0488\n",
      "Epoch 53/60, Train Loss: 0.0025, Train RMSE: 0.0492, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 54/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 55/60, Train Loss: 0.0025, Train RMSE: 0.0491, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 56/60, Train Loss: 0.0025, Train RMSE: 0.0491, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 57/60, Train Loss: 0.0024, Train RMSE: 0.0487, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 58/60, Train Loss: 0.0024, Train RMSE: 0.0487, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 59/60, Train Loss: 0.0024, Train RMSE: 0.0483, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 60/60, Train Loss: 0.0024, Train RMSE: 0.0480, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Training model with num_layers=2, dropout=0.5, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0648, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 5/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 6/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 7/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 8/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 9/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 10/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 11/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 12/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 13/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 14/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 15/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 16/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 17/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 18/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 19/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 20/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 21/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 22/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 23/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 24/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 25/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 26/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 27/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 28/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 29/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 30/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 31/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 32/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 33/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 34/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 35/60, Train Loss: 0.0653, Train RMSE: 0.2555, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 36/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 37/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 38/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 39/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 40/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 41/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 42/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 43/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 44/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 45/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 46/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 47/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 48/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 49/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 50/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 51/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 52/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 53/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 54/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 55/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 56/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 57/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 58/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 59/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 60/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Training model with num_layers=2, dropout=0.5, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0042, Train RMSE: 0.0638, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 2/60, Train Loss: 0.0035, Train RMSE: 0.0583, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 3/60, Train Loss: 0.0032, Train RMSE: 0.0559, Val Loss: 0.0022, Val RMSE: 0.0466\n",
      "Epoch 4/60, Train Loss: 0.0031, Train RMSE: 0.0556, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 5/60, Train Loss: 0.0030, Train RMSE: 0.0541, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 6/60, Train Loss: 0.0030, Train RMSE: 0.0540, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 7/60, Train Loss: 0.0028, Train RMSE: 0.0521, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 8/60, Train Loss: 0.0027, Train RMSE: 0.0515, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 9/60, Train Loss: 0.0028, Train RMSE: 0.0520, Val Loss: 0.0023, Val RMSE: 0.0467\n",
      "Epoch 10/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0022, Val RMSE: 0.0462\n",
      "Epoch 11/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 12/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 13/60, Train Loss: 0.0026, Train RMSE: 0.0500, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 14/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 15/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 16/60, Train Loss: 0.0024, Train RMSE: 0.0488, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 17/60, Train Loss: 0.0024, Train RMSE: 0.0484, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 18/60, Train Loss: 0.0024, Train RMSE: 0.0480, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 19/60, Train Loss: 0.0023, Train RMSE: 0.0477, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 20/60, Train Loss: 0.0024, Train RMSE: 0.0484, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 21/60, Train Loss: 0.0023, Train RMSE: 0.0477, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 22/60, Train Loss: 0.0023, Train RMSE: 0.0467, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 23/60, Train Loss: 0.0022, Train RMSE: 0.0464, Val Loss: 0.0022, Val RMSE: 0.0463\n",
      "Epoch 24/60, Train Loss: 0.0022, Train RMSE: 0.0465, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 25/60, Train Loss: 0.0022, Train RMSE: 0.0468, Val Loss: 0.0022, Val RMSE: 0.0466\n",
      "Epoch 26/60, Train Loss: 0.0022, Train RMSE: 0.0461, Val Loss: 0.0023, Val RMSE: 0.0467\n",
      "Epoch 27/60, Train Loss: 0.0021, Train RMSE: 0.0455, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 28/60, Train Loss: 0.0022, Train RMSE: 0.0459, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 29/60, Train Loss: 0.0021, Train RMSE: 0.0453, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 30/60, Train Loss: 0.0021, Train RMSE: 0.0452, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 31/60, Train Loss: 0.0021, Train RMSE: 0.0450, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 32/60, Train Loss: 0.0021, Train RMSE: 0.0449, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 33/60, Train Loss: 0.0021, Train RMSE: 0.0455, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 34/60, Train Loss: 0.0020, Train RMSE: 0.0445, Val Loss: 0.0022, Val RMSE: 0.0468\n",
      "Epoch 35/60, Train Loss: 0.0020, Train RMSE: 0.0442, Val Loss: 0.0022, Val RMSE: 0.0466\n",
      "Epoch 36/60, Train Loss: 0.0020, Train RMSE: 0.0439, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 37/60, Train Loss: 0.0020, Train RMSE: 0.0444, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 38/60, Train Loss: 0.0020, Train RMSE: 0.0438, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 39/60, Train Loss: 0.0020, Train RMSE: 0.0440, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 40/60, Train Loss: 0.0020, Train RMSE: 0.0436, Val Loss: 0.0023, Val RMSE: 0.0478\n",
      "Epoch 41/60, Train Loss: 0.0019, Train RMSE: 0.0430, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 42/60, Train Loss: 0.0018, Train RMSE: 0.0424, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 43/60, Train Loss: 0.0019, Train RMSE: 0.0425, Val Loss: 0.0022, Val RMSE: 0.0466\n",
      "Epoch 44/60, Train Loss: 0.0019, Train RMSE: 0.0426, Val Loss: 0.0022, Val RMSE: 0.0462\n",
      "Epoch 45/60, Train Loss: 0.0019, Train RMSE: 0.0431, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 46/60, Train Loss: 0.0019, Train RMSE: 0.0427, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 47/60, Train Loss: 0.0018, Train RMSE: 0.0423, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 48/60, Train Loss: 0.0018, Train RMSE: 0.0425, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 49/60, Train Loss: 0.0018, Train RMSE: 0.0420, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 50/60, Train Loss: 0.0017, Train RMSE: 0.0415, Val Loss: 0.0022, Val RMSE: 0.0469\n",
      "Epoch 51/60, Train Loss: 0.0018, Train RMSE: 0.0425, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 52/60, Train Loss: 0.0018, Train RMSE: 0.0416, Val Loss: 0.0023, Val RMSE: 0.0479\n",
      "Epoch 53/60, Train Loss: 0.0018, Train RMSE: 0.0414, Val Loss: 0.0022, Val RMSE: 0.0466\n",
      "Epoch 54/60, Train Loss: 0.0017, Train RMSE: 0.0413, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 55/60, Train Loss: 0.0017, Train RMSE: 0.0405, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 56/60, Train Loss: 0.0017, Train RMSE: 0.0409, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 57/60, Train Loss: 0.0017, Train RMSE: 0.0407, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 58/60, Train Loss: 0.0017, Train RMSE: 0.0406, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 59/60, Train Loss: 0.0017, Train RMSE: 0.0412, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 60/60, Train Loss: 0.0016, Train RMSE: 0.0401, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Training model with num_layers=2, dropout=0.6, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0639, Train RMSE: 0.2520, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0648, Train RMSE: 0.2545, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0646, Train RMSE: 0.2541, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 5/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 6/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 7/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 8/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 9/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 10/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 11/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 12/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 13/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 14/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 15/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 16/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 17/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 18/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 19/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 20/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 21/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 22/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 23/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 24/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 25/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 26/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 27/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 28/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 29/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 30/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 31/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 32/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 33/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 34/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 35/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 36/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 37/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 38/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 39/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 40/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 41/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 42/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 43/60, Train Loss: 0.0644, Train RMSE: 0.2536, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 44/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 45/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 46/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 47/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 48/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 49/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 50/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 51/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 52/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 53/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 54/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 55/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 56/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 57/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 58/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 59/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 60/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Training model with num_layers=2, dropout=0.6, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0043, Train RMSE: 0.0646, Val Loss: 0.0026, Val RMSE: 0.0508\n",
      "Epoch 2/60, Train Loss: 0.0037, Train RMSE: 0.0601, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 3/60, Train Loss: 0.0035, Train RMSE: 0.0585, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 4/60, Train Loss: 0.0034, Train RMSE: 0.0577, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 5/60, Train Loss: 0.0034, Train RMSE: 0.0575, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 6/60, Train Loss: 0.0033, Train RMSE: 0.0568, Val Loss: 0.0027, Val RMSE: 0.0512\n",
      "Epoch 7/60, Train Loss: 0.0032, Train RMSE: 0.0556, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 8/60, Train Loss: 0.0031, Train RMSE: 0.0551, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 9/60, Train Loss: 0.0031, Train RMSE: 0.0554, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 10/60, Train Loss: 0.0031, Train RMSE: 0.0551, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 11/60, Train Loss: 0.0030, Train RMSE: 0.0541, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 12/60, Train Loss: 0.0030, Train RMSE: 0.0542, Val Loss: 0.0026, Val RMSE: 0.0505\n",
      "Epoch 13/60, Train Loss: 0.0029, Train RMSE: 0.0532, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 14/60, Train Loss: 0.0028, Train RMSE: 0.0524, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 15/60, Train Loss: 0.0028, Train RMSE: 0.0527, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 16/60, Train Loss: 0.0028, Train RMSE: 0.0519, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 17/60, Train Loss: 0.0027, Train RMSE: 0.0519, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 18/60, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 19/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 20/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 21/60, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 22/60, Train Loss: 0.0027, Train RMSE: 0.0508, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 23/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 24/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 25/60, Train Loss: 0.0025, Train RMSE: 0.0497, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 26/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 27/60, Train Loss: 0.0025, Train RMSE: 0.0491, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 28/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 29/60, Train Loss: 0.0024, Train RMSE: 0.0485, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 30/60, Train Loss: 0.0025, Train RMSE: 0.0491, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 31/60, Train Loss: 0.0024, Train RMSE: 0.0484, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 32/60, Train Loss: 0.0024, Train RMSE: 0.0483, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 33/60, Train Loss: 0.0023, Train RMSE: 0.0477, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 34/60, Train Loss: 0.0023, Train RMSE: 0.0477, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 35/60, Train Loss: 0.0023, Train RMSE: 0.0478, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 36/60, Train Loss: 0.0022, Train RMSE: 0.0469, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 37/60, Train Loss: 0.0022, Train RMSE: 0.0466, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 38/60, Train Loss: 0.0022, Train RMSE: 0.0466, Val Loss: 0.0024, Val RMSE: 0.0489\n",
      "Epoch 39/60, Train Loss: 0.0022, Train RMSE: 0.0465, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 40/60, Train Loss: 0.0022, Train RMSE: 0.0465, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 41/60, Train Loss: 0.0022, Train RMSE: 0.0462, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 42/60, Train Loss: 0.0021, Train RMSE: 0.0458, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 43/60, Train Loss: 0.0021, Train RMSE: 0.0456, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 44/60, Train Loss: 0.0022, Train RMSE: 0.0458, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 45/60, Train Loss: 0.0020, Train RMSE: 0.0442, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 46/60, Train Loss: 0.0020, Train RMSE: 0.0444, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 47/60, Train Loss: 0.0021, Train RMSE: 0.0453, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 48/60, Train Loss: 0.0021, Train RMSE: 0.0451, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 49/60, Train Loss: 0.0020, Train RMSE: 0.0440, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 50/60, Train Loss: 0.0020, Train RMSE: 0.0445, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 51/60, Train Loss: 0.0020, Train RMSE: 0.0440, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 52/60, Train Loss: 0.0020, Train RMSE: 0.0439, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 53/60, Train Loss: 0.0020, Train RMSE: 0.0445, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 54/60, Train Loss: 0.0019, Train RMSE: 0.0435, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 55/60, Train Loss: 0.0019, Train RMSE: 0.0432, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 56/60, Train Loss: 0.0019, Train RMSE: 0.0432, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 57/60, Train Loss: 0.0019, Train RMSE: 0.0431, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 58/60, Train Loss: 0.0018, Train RMSE: 0.0424, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 59/60, Train Loss: 0.0019, Train RMSE: 0.0427, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 60/60, Train Loss: 0.0018, Train RMSE: 0.0423, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Training model with num_layers=2, dropout=0.7, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0644, Train RMSE: 0.2532, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0631, Train RMSE: 0.2509, Val Loss: 0.0406, Val RMSE: 0.2008\n",
      "Epoch 4/60, Train Loss: 0.0069, Train RMSE: 0.0796, Val Loss: 0.0044, Val RMSE: 0.0660\n",
      "Epoch 5/60, Train Loss: 0.0043, Train RMSE: 0.0650, Val Loss: 0.0036, Val RMSE: 0.0599\n",
      "Epoch 6/60, Train Loss: 0.0036, Train RMSE: 0.0596, Val Loss: 0.0032, Val RMSE: 0.0557\n",
      "Epoch 7/60, Train Loss: 0.0032, Train RMSE: 0.0558, Val Loss: 0.0029, Val RMSE: 0.0530\n",
      "Epoch 8/60, Train Loss: 0.0029, Train RMSE: 0.0535, Val Loss: 0.0027, Val RMSE: 0.0516\n",
      "Epoch 9/60, Train Loss: 0.0028, Train RMSE: 0.0521, Val Loss: 0.0027, Val RMSE: 0.0509\n",
      "Epoch 10/60, Train Loss: 0.0027, Train RMSE: 0.0514, Val Loss: 0.0026, Val RMSE: 0.0505\n",
      "Epoch 11/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 12/60, Train Loss: 0.0027, Train RMSE: 0.0507, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 13/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 14/60, Train Loss: 0.0026, Train RMSE: 0.0507, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 15/60, Train Loss: 0.0026, Train RMSE: 0.0507, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 16/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 17/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 18/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 19/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 20/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 21/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 22/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 23/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 24/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 25/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 26/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 27/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 28/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 29/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 30/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 31/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 32/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 33/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 34/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 35/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 36/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 37/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 38/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 39/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 40/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 41/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 42/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 43/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 44/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 45/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 46/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 47/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 48/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 49/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 50/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 51/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 52/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 53/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 54/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 55/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 56/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 57/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 58/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 59/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 60/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Training model with num_layers=2, dropout=0.7, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0043, Train RMSE: 0.0655, Val Loss: 0.0027, Val RMSE: 0.0516\n",
      "Epoch 2/60, Train Loss: 0.0041, Train RMSE: 0.0635, Val Loss: 0.0030, Val RMSE: 0.0540\n",
      "Epoch 3/60, Train Loss: 0.0041, Train RMSE: 0.0634, Val Loss: 0.0027, Val RMSE: 0.0509\n",
      "Epoch 4/60, Train Loss: 0.0038, Train RMSE: 0.0610, Val Loss: 0.0026, Val RMSE: 0.0507\n",
      "Epoch 5/60, Train Loss: 0.0037, Train RMSE: 0.0600, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 6/60, Train Loss: 0.0036, Train RMSE: 0.0594, Val Loss: 0.0029, Val RMSE: 0.0538\n",
      "Epoch 7/60, Train Loss: 0.0035, Train RMSE: 0.0588, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 8/60, Train Loss: 0.0035, Train RMSE: 0.0582, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 9/60, Train Loss: 0.0034, Train RMSE: 0.0577, Val Loss: 0.0029, Val RMSE: 0.0532\n",
      "Epoch 10/60, Train Loss: 0.0033, Train RMSE: 0.0566, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 11/60, Train Loss: 0.0033, Train RMSE: 0.0571, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 12/60, Train Loss: 0.0033, Train RMSE: 0.0568, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 13/60, Train Loss: 0.0032, Train RMSE: 0.0562, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 14/60, Train Loss: 0.0032, Train RMSE: 0.0560, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 15/60, Train Loss: 0.0032, Train RMSE: 0.0561, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 16/60, Train Loss: 0.0032, Train RMSE: 0.0559, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 17/60, Train Loss: 0.0031, Train RMSE: 0.0555, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 18/60, Train Loss: 0.0031, Train RMSE: 0.0550, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 19/60, Train Loss: 0.0031, Train RMSE: 0.0551, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 20/60, Train Loss: 0.0031, Train RMSE: 0.0551, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 21/60, Train Loss: 0.0030, Train RMSE: 0.0544, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 22/60, Train Loss: 0.0030, Train RMSE: 0.0538, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 23/60, Train Loss: 0.0029, Train RMSE: 0.0531, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 24/60, Train Loss: 0.0030, Train RMSE: 0.0539, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 25/60, Train Loss: 0.0029, Train RMSE: 0.0531, Val Loss: 0.0025, Val RMSE: 0.0498\n",
      "Epoch 26/60, Train Loss: 0.0029, Train RMSE: 0.0534, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 27/60, Train Loss: 0.0028, Train RMSE: 0.0528, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 28/60, Train Loss: 0.0029, Train RMSE: 0.0533, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 29/60, Train Loss: 0.0028, Train RMSE: 0.0523, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 30/60, Train Loss: 0.0028, Train RMSE: 0.0520, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 31/60, Train Loss: 0.0028, Train RMSE: 0.0524, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 32/60, Train Loss: 0.0028, Train RMSE: 0.0520, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 33/60, Train Loss: 0.0027, Train RMSE: 0.0515, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 34/60, Train Loss: 0.0027, Train RMSE: 0.0517, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 35/60, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 36/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 37/60, Train Loss: 0.0027, Train RMSE: 0.0512, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 38/60, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 39/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 40/60, Train Loss: 0.0026, Train RMSE: 0.0502, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 41/60, Train Loss: 0.0026, Train RMSE: 0.0499, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 42/60, Train Loss: 0.0026, Train RMSE: 0.0500, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 43/60, Train Loss: 0.0025, Train RMSE: 0.0499, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 44/60, Train Loss: 0.0026, Train RMSE: 0.0499, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 45/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 46/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0024, Val RMSE: 0.0489\n",
      "Epoch 47/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 48/60, Train Loss: 0.0025, Train RMSE: 0.0490, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 49/60, Train Loss: 0.0025, Train RMSE: 0.0489, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 50/60, Train Loss: 0.0024, Train RMSE: 0.0483, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 51/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 52/60, Train Loss: 0.0024, Train RMSE: 0.0488, Val Loss: 0.0024, Val RMSE: 0.0489\n",
      "Epoch 53/60, Train Loss: 0.0024, Train RMSE: 0.0483, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 54/60, Train Loss: 0.0023, Train RMSE: 0.0479, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 55/60, Train Loss: 0.0024, Train RMSE: 0.0485, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 56/60, Train Loss: 0.0024, Train RMSE: 0.0479, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 57/60, Train Loss: 0.0024, Train RMSE: 0.0479, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 58/60, Train Loss: 0.0024, Train RMSE: 0.0479, Val Loss: 0.0024, Val RMSE: 0.0489\n",
      "Epoch 59/60, Train Loss: 0.0023, Train RMSE: 0.0478, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 60/60, Train Loss: 0.0024, Train RMSE: 0.0478, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Training model with num_layers=3, dropout=0.5, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0639, Train RMSE: 0.2520, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0646, Train RMSE: 0.2540, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0922, Train RMSE: 0.3034, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 4/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 5/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 6/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 7/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 8/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 9/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 10/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 11/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 12/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 13/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 14/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 15/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 16/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 17/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 18/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 19/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 20/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 21/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 22/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 23/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 24/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 25/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 26/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 27/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 28/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 29/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 30/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 31/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 32/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 33/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 34/60, Train Loss: 0.0938, Train RMSE: 0.3063, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 35/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 36/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 37/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 38/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 39/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 40/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 41/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 42/60, Train Loss: 0.0938, Train RMSE: 0.3063, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 43/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 44/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 45/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 46/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 47/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 48/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 49/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 50/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 51/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 52/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 53/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 54/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 55/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 56/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 57/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 58/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 59/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Epoch 60/60, Train Loss: 0.0938, Train RMSE: 0.3062, Val Loss: 0.0929, Val RMSE: 0.3047\n",
      "Training model with num_layers=3, dropout=0.5, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0044, Train RMSE: 0.0652, Val Loss: 0.0029, Val RMSE: 0.0536\n",
      "Epoch 2/60, Train Loss: 0.0035, Train RMSE: 0.0585, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 3/60, Train Loss: 0.0032, Train RMSE: 0.0558, Val Loss: 0.0027, Val RMSE: 0.0513\n",
      "Epoch 4/60, Train Loss: 0.0031, Train RMSE: 0.0550, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 5/60, Train Loss: 0.0029, Train RMSE: 0.0530, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 6/60, Train Loss: 0.0028, Train RMSE: 0.0525, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 7/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 8/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 9/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 10/60, Train Loss: 0.0024, Train RMSE: 0.0488, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 11/60, Train Loss: 0.0024, Train RMSE: 0.0481, Val Loss: 0.0024, Val RMSE: 0.0489\n",
      "Epoch 12/60, Train Loss: 0.0023, Train RMSE: 0.0476, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 13/60, Train Loss: 0.0022, Train RMSE: 0.0469, Val Loss: 0.0024, Val RMSE: 0.0490\n",
      "Epoch 14/60, Train Loss: 0.0022, Train RMSE: 0.0464, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 15/60, Train Loss: 0.0021, Train RMSE: 0.0454, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 16/60, Train Loss: 0.0020, Train RMSE: 0.0447, Val Loss: 0.0026, Val RMSE: 0.0508\n",
      "Epoch 17/60, Train Loss: 0.0021, Train RMSE: 0.0452, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 18/60, Train Loss: 0.0020, Train RMSE: 0.0445, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 19/60, Train Loss: 0.0019, Train RMSE: 0.0434, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 20/60, Train Loss: 0.0019, Train RMSE: 0.0437, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 21/60, Train Loss: 0.0019, Train RMSE: 0.0431, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 22/60, Train Loss: 0.0018, Train RMSE: 0.0421, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 23/60, Train Loss: 0.0019, Train RMSE: 0.0430, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 24/60, Train Loss: 0.0018, Train RMSE: 0.0419, Val Loss: 0.0023, Val RMSE: 0.0479\n",
      "Epoch 25/60, Train Loss: 0.0018, Train RMSE: 0.0418, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 26/60, Train Loss: 0.0018, Train RMSE: 0.0421, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 27/60, Train Loss: 0.0018, Train RMSE: 0.0419, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 28/60, Train Loss: 0.0017, Train RMSE: 0.0407, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 29/60, Train Loss: 0.0018, Train RMSE: 0.0419, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 30/60, Train Loss: 0.0017, Train RMSE: 0.0407, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 31/60, Train Loss: 0.0017, Train RMSE: 0.0405, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 32/60, Train Loss: 0.0017, Train RMSE: 0.0403, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 33/60, Train Loss: 0.0017, Train RMSE: 0.0408, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 34/60, Train Loss: 0.0017, Train RMSE: 0.0406, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 35/60, Train Loss: 0.0016, Train RMSE: 0.0402, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 36/60, Train Loss: 0.0017, Train RMSE: 0.0405, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 37/60, Train Loss: 0.0016, Train RMSE: 0.0402, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 38/60, Train Loss: 0.0017, Train RMSE: 0.0405, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 39/60, Train Loss: 0.0016, Train RMSE: 0.0399, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 40/60, Train Loss: 0.0017, Train RMSE: 0.0410, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 41/60, Train Loss: 0.0016, Train RMSE: 0.0401, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 42/60, Train Loss: 0.0015, Train RMSE: 0.0388, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 43/60, Train Loss: 0.0015, Train RMSE: 0.0383, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 44/60, Train Loss: 0.0016, Train RMSE: 0.0393, Val Loss: 0.0022, Val RMSE: 0.0468\n",
      "Epoch 45/60, Train Loss: 0.0016, Train RMSE: 0.0394, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 46/60, Train Loss: 0.0015, Train RMSE: 0.0386, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 47/60, Train Loss: 0.0017, Train RMSE: 0.0404, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 48/60, Train Loss: 0.0015, Train RMSE: 0.0384, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 49/60, Train Loss: 0.0015, Train RMSE: 0.0385, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 50/60, Train Loss: 0.0015, Train RMSE: 0.0380, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 51/60, Train Loss: 0.0014, Train RMSE: 0.0370, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 52/60, Train Loss: 0.0015, Train RMSE: 0.0381, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 53/60, Train Loss: 0.0015, Train RMSE: 0.0388, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 54/60, Train Loss: 0.0015, Train RMSE: 0.0386, Val Loss: 0.0023, Val RMSE: 0.0478\n",
      "Epoch 55/60, Train Loss: 0.0014, Train RMSE: 0.0377, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 56/60, Train Loss: 0.0015, Train RMSE: 0.0388, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 57/60, Train Loss: 0.0015, Train RMSE: 0.0378, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 58/60, Train Loss: 0.0015, Train RMSE: 0.0384, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 59/60, Train Loss: 0.0014, Train RMSE: 0.0373, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 60/60, Train Loss: 0.0015, Train RMSE: 0.0378, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Training model with num_layers=3, dropout=0.6, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0643, Train RMSE: 0.2529, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0644, Train RMSE: 0.2536, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0641, Train RMSE: 0.2530, Val Loss: 0.0657, Val RMSE: 0.2562\n",
      "Epoch 5/60, Train Loss: 0.0099, Train RMSE: 0.0892, Val Loss: 0.0043, Val RMSE: 0.0652\n",
      "Epoch 6/60, Train Loss: 0.0041, Train RMSE: 0.0636, Val Loss: 0.0035, Val RMSE: 0.0587\n",
      "Epoch 7/60, Train Loss: 0.0034, Train RMSE: 0.0580, Val Loss: 0.0030, Val RMSE: 0.0545\n",
      "Epoch 8/60, Train Loss: 0.0030, Train RMSE: 0.0547, Val Loss: 0.0028, Val RMSE: 0.0523\n",
      "Epoch 9/60, Train Loss: 0.0028, Train RMSE: 0.0528, Val Loss: 0.0027, Val RMSE: 0.0512\n",
      "Epoch 10/60, Train Loss: 0.0027, Train RMSE: 0.0517, Val Loss: 0.0026, Val RMSE: 0.0508\n",
      "Epoch 11/60, Train Loss: 0.0027, Train RMSE: 0.0511, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 12/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0026, Val RMSE: 0.0504\n",
      "Epoch 13/60, Train Loss: 0.0027, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 14/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 15/60, Train Loss: 0.0026, Train RMSE: 0.0507, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 16/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 17/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 18/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 19/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 20/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 21/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 22/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 23/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 24/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 25/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 26/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 27/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 28/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 29/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 30/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 31/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 32/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 33/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 34/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 35/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 36/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 37/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 38/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 39/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 40/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 41/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 42/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 43/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 44/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 45/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 46/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 47/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 48/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 49/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 50/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 51/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 52/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 53/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 54/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 55/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 56/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 57/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 58/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 59/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 60/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Training model with num_layers=3, dropout=0.6, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0047, Train RMSE: 0.0680, Val Loss: 0.0030, Val RMSE: 0.0547\n",
      "Epoch 2/60, Train Loss: 0.0039, Train RMSE: 0.0621, Val Loss: 0.0028, Val RMSE: 0.0522\n",
      "Epoch 3/60, Train Loss: 0.0036, Train RMSE: 0.0598, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 4/60, Train Loss: 0.0035, Train RMSE: 0.0587, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 5/60, Train Loss: 0.0033, Train RMSE: 0.0567, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 6/60, Train Loss: 0.0031, Train RMSE: 0.0552, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 7/60, Train Loss: 0.0030, Train RMSE: 0.0540, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 8/60, Train Loss: 0.0029, Train RMSE: 0.0537, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 9/60, Train Loss: 0.0028, Train RMSE: 0.0527, Val Loss: 0.0024, Val RMSE: 0.0489\n",
      "Epoch 10/60, Train Loss: 0.0028, Train RMSE: 0.0526, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 11/60, Train Loss: 0.0029, Train RMSE: 0.0530, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 12/60, Train Loss: 0.0027, Train RMSE: 0.0517, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 13/60, Train Loss: 0.0027, Train RMSE: 0.0512, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 14/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 15/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 16/60, Train Loss: 0.0025, Train RMSE: 0.0499, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 17/60, Train Loss: 0.0025, Train RMSE: 0.0497, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 18/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 19/60, Train Loss: 0.0024, Train RMSE: 0.0489, Val Loss: 0.0022, Val RMSE: 0.0468\n",
      "Epoch 20/60, Train Loss: 0.0024, Train RMSE: 0.0484, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 21/60, Train Loss: 0.0023, Train RMSE: 0.0480, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 22/60, Train Loss: 0.0023, Train RMSE: 0.0473, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 23/60, Train Loss: 0.0023, Train RMSE: 0.0478, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 24/60, Train Loss: 0.0022, Train RMSE: 0.0469, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 25/60, Train Loss: 0.0022, Train RMSE: 0.0466, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 26/60, Train Loss: 0.0023, Train RMSE: 0.0472, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 27/60, Train Loss: 0.0022, Train RMSE: 0.0466, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 28/60, Train Loss: 0.0022, Train RMSE: 0.0465, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 29/60, Train Loss: 0.0021, Train RMSE: 0.0456, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 30/60, Train Loss: 0.0021, Train RMSE: 0.0451, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 31/60, Train Loss: 0.0021, Train RMSE: 0.0455, Val Loss: 0.0024, Val RMSE: 0.0488\n",
      "Epoch 32/60, Train Loss: 0.0021, Train RMSE: 0.0451, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 33/60, Train Loss: 0.0021, Train RMSE: 0.0455, Val Loss: 0.0023, Val RMSE: 0.0478\n",
      "Epoch 34/60, Train Loss: 0.0020, Train RMSE: 0.0443, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 35/60, Train Loss: 0.0020, Train RMSE: 0.0441, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 36/60, Train Loss: 0.0020, Train RMSE: 0.0439, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 37/60, Train Loss: 0.0020, Train RMSE: 0.0444, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 38/60, Train Loss: 0.0020, Train RMSE: 0.0442, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 39/60, Train Loss: 0.0020, Train RMSE: 0.0438, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 40/60, Train Loss: 0.0019, Train RMSE: 0.0436, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 41/60, Train Loss: 0.0018, Train RMSE: 0.0425, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 42/60, Train Loss: 0.0019, Train RMSE: 0.0433, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 43/60, Train Loss: 0.0018, Train RMSE: 0.0426, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 44/60, Train Loss: 0.0018, Train RMSE: 0.0424, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 45/60, Train Loss: 0.0019, Train RMSE: 0.0426, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 46/60, Train Loss: 0.0019, Train RMSE: 0.0427, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 47/60, Train Loss: 0.0018, Train RMSE: 0.0422, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 48/60, Train Loss: 0.0018, Train RMSE: 0.0413, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 49/60, Train Loss: 0.0017, Train RMSE: 0.0408, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 50/60, Train Loss: 0.0017, Train RMSE: 0.0412, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 51/60, Train Loss: 0.0018, Train RMSE: 0.0418, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 52/60, Train Loss: 0.0018, Train RMSE: 0.0423, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 53/60, Train Loss: 0.0018, Train RMSE: 0.0418, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 54/60, Train Loss: 0.0017, Train RMSE: 0.0407, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 55/60, Train Loss: 0.0018, Train RMSE: 0.0418, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 56/60, Train Loss: 0.0017, Train RMSE: 0.0408, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 57/60, Train Loss: 0.0017, Train RMSE: 0.0406, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 58/60, Train Loss: 0.0016, Train RMSE: 0.0402, Val Loss: 0.0023, Val RMSE: 0.0478\n",
      "Epoch 59/60, Train Loss: 0.0017, Train RMSE: 0.0404, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 60/60, Train Loss: 0.0017, Train RMSE: 0.0411, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Training model with num_layers=3, dropout=0.7, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0641, Train RMSE: 0.2525, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0647, Train RMSE: 0.2544, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0647, Train RMSE: 0.2542, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0663, Train RMSE: 0.2574, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 5/60, Train Loss: 0.0622, Train RMSE: 0.2492, Val Loss: 0.0655, Val RMSE: 0.2558\n",
      "Epoch 6/60, Train Loss: 0.0298, Train RMSE: 0.1703, Val Loss: 0.0656, Val RMSE: 0.2562\n",
      "Epoch 7/60, Train Loss: 0.0226, Train RMSE: 0.1491, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 8/60, Train Loss: 0.0213, Train RMSE: 0.1450, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 9/60, Train Loss: 0.0214, Train RMSE: 0.1452, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 10/60, Train Loss: 0.0215, Train RMSE: 0.1455, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 11/60, Train Loss: 0.0200, Train RMSE: 0.1403, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 12/60, Train Loss: 0.0207, Train RMSE: 0.1426, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 13/60, Train Loss: 0.0212, Train RMSE: 0.1444, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 14/60, Train Loss: 0.0211, Train RMSE: 0.1443, Val Loss: 0.0657, Val RMSE: 0.2562\n",
      "Epoch 15/60, Train Loss: 0.0200, Train RMSE: 0.1403, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 16/60, Train Loss: 0.0211, Train RMSE: 0.1443, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 17/60, Train Loss: 0.0209, Train RMSE: 0.1433, Val Loss: 0.0655, Val RMSE: 0.2559\n",
      "Epoch 18/60, Train Loss: 0.0215, Train RMSE: 0.1454, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 19/60, Train Loss: 0.0211, Train RMSE: 0.1440, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 20/60, Train Loss: 0.0212, Train RMSE: 0.1442, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 21/60, Train Loss: 0.0209, Train RMSE: 0.1433, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 22/60, Train Loss: 0.0208, Train RMSE: 0.1430, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 23/60, Train Loss: 0.0209, Train RMSE: 0.1435, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 24/60, Train Loss: 0.0207, Train RMSE: 0.1431, Val Loss: 0.0656, Val RMSE: 0.2561\n",
      "Epoch 25/60, Train Loss: 0.0213, Train RMSE: 0.1450, Val Loss: 0.0657, Val RMSE: 0.2562\n",
      "Epoch 26/60, Train Loss: 0.0205, Train RMSE: 0.1419, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 27/60, Train Loss: 0.0222, Train RMSE: 0.1479, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 28/60, Train Loss: 0.0216, Train RMSE: 0.1460, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 29/60, Train Loss: 0.0204, Train RMSE: 0.1414, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 30/60, Train Loss: 0.0214, Train RMSE: 0.1449, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 31/60, Train Loss: 0.0213, Train RMSE: 0.1448, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 32/60, Train Loss: 0.0208, Train RMSE: 0.1429, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 33/60, Train Loss: 0.0216, Train RMSE: 0.1456, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 34/60, Train Loss: 0.0222, Train RMSE: 0.1477, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 35/60, Train Loss: 0.0216, Train RMSE: 0.1458, Val Loss: 0.0656, Val RMSE: 0.2561\n",
      "Epoch 36/60, Train Loss: 0.0204, Train RMSE: 0.1416, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 37/60, Train Loss: 0.0214, Train RMSE: 0.1451, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 38/60, Train Loss: 0.0212, Train RMSE: 0.1445, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 39/60, Train Loss: 0.0208, Train RMSE: 0.1430, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 40/60, Train Loss: 0.0213, Train RMSE: 0.1451, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 41/60, Train Loss: 0.0216, Train RMSE: 0.1461, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 42/60, Train Loss: 0.0214, Train RMSE: 0.1447, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 43/60, Train Loss: 0.0210, Train RMSE: 0.1438, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 44/60, Train Loss: 0.0204, Train RMSE: 0.1417, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 45/60, Train Loss: 0.0211, Train RMSE: 0.1443, Val Loss: 0.0657, Val RMSE: 0.2562\n",
      "Epoch 46/60, Train Loss: 0.0221, Train RMSE: 0.1474, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 47/60, Train Loss: 0.0211, Train RMSE: 0.1442, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 48/60, Train Loss: 0.0210, Train RMSE: 0.1439, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 49/60, Train Loss: 0.0210, Train RMSE: 0.1440, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 50/60, Train Loss: 0.0214, Train RMSE: 0.1452, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 51/60, Train Loss: 0.0210, Train RMSE: 0.1437, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 52/60, Train Loss: 0.0219, Train RMSE: 0.1470, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 53/60, Train Loss: 0.0213, Train RMSE: 0.1448, Val Loss: 0.0656, Val RMSE: 0.2560\n",
      "Epoch 54/60, Train Loss: 0.0224, Train RMSE: 0.1488, Val Loss: 0.0657, Val RMSE: 0.2563\n",
      "Epoch 55/60, Train Loss: 0.0214, Train RMSE: 0.1453, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 56/60, Train Loss: 0.0209, Train RMSE: 0.1435, Val Loss: 0.0654, Val RMSE: 0.2557\n",
      "Epoch 57/60, Train Loss: 0.0207, Train RMSE: 0.1425, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 58/60, Train Loss: 0.0213, Train RMSE: 0.1447, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 59/60, Train Loss: 0.0215, Train RMSE: 0.1456, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 60/60, Train Loss: 0.0213, Train RMSE: 0.1450, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Training model with num_layers=3, dropout=0.7, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0046, Train RMSE: 0.0674, Val Loss: 0.0028, Val RMSE: 0.0521\n",
      "Epoch 2/60, Train Loss: 0.0042, Train RMSE: 0.0642, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 3/60, Train Loss: 0.0039, Train RMSE: 0.0622, Val Loss: 0.0031, Val RMSE: 0.0552\n",
      "Epoch 4/60, Train Loss: 0.0038, Train RMSE: 0.0614, Val Loss: 0.0028, Val RMSE: 0.0528\n",
      "Epoch 5/60, Train Loss: 0.0036, Train RMSE: 0.0594, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 6/60, Train Loss: 0.0035, Train RMSE: 0.0587, Val Loss: 0.0025, Val RMSE: 0.0499\n",
      "Epoch 7/60, Train Loss: 0.0035, Train RMSE: 0.0586, Val Loss: 0.0026, Val RMSE: 0.0507\n",
      "Epoch 8/60, Train Loss: 0.0033, Train RMSE: 0.0572, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 9/60, Train Loss: 0.0033, Train RMSE: 0.0566, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 10/60, Train Loss: 0.0032, Train RMSE: 0.0563, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 11/60, Train Loss: 0.0032, Train RMSE: 0.0561, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 12/60, Train Loss: 0.0031, Train RMSE: 0.0551, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 13/60, Train Loss: 0.0030, Train RMSE: 0.0542, Val Loss: 0.0027, Val RMSE: 0.0515\n",
      "Epoch 14/60, Train Loss: 0.0030, Train RMSE: 0.0544, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 15/60, Train Loss: 0.0030, Train RMSE: 0.0538, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 16/60, Train Loss: 0.0029, Train RMSE: 0.0535, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 17/60, Train Loss: 0.0030, Train RMSE: 0.0537, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 18/60, Train Loss: 0.0029, Train RMSE: 0.0531, Val Loss: 0.0023, Val RMSE: 0.0478\n",
      "Epoch 19/60, Train Loss: 0.0028, Train RMSE: 0.0522, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 20/60, Train Loss: 0.0028, Train RMSE: 0.0522, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 21/60, Train Loss: 0.0028, Train RMSE: 0.0526, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 22/60, Train Loss: 0.0027, Train RMSE: 0.0517, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 23/60, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 24/60, Train Loss: 0.0027, Train RMSE: 0.0517, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 25/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 26/60, Train Loss: 0.0027, Train RMSE: 0.0513, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 27/60, Train Loss: 0.0027, Train RMSE: 0.0514, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 28/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 29/60, Train Loss: 0.0026, Train RMSE: 0.0502, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 30/60, Train Loss: 0.0025, Train RMSE: 0.0498, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 31/60, Train Loss: 0.0026, Train RMSE: 0.0499, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 32/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 33/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 34/60, Train Loss: 0.0025, Train RMSE: 0.0497, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 35/60, Train Loss: 0.0025, Train RMSE: 0.0492, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 36/60, Train Loss: 0.0025, Train RMSE: 0.0492, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 37/60, Train Loss: 0.0024, Train RMSE: 0.0487, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 38/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 39/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 40/60, Train Loss: 0.0024, Train RMSE: 0.0483, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 41/60, Train Loss: 0.0024, Train RMSE: 0.0480, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 42/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 43/60, Train Loss: 0.0023, Train RMSE: 0.0476, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 44/60, Train Loss: 0.0024, Train RMSE: 0.0480, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 45/60, Train Loss: 0.0023, Train RMSE: 0.0478, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 46/60, Train Loss: 0.0023, Train RMSE: 0.0471, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 47/60, Train Loss: 0.0022, Train RMSE: 0.0466, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 48/60, Train Loss: 0.0023, Train RMSE: 0.0471, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 49/60, Train Loss: 0.0023, Train RMSE: 0.0470, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 50/60, Train Loss: 0.0022, Train RMSE: 0.0466, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 51/60, Train Loss: 0.0022, Train RMSE: 0.0466, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 52/60, Train Loss: 0.0022, Train RMSE: 0.0465, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 53/60, Train Loss: 0.0022, Train RMSE: 0.0462, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 54/60, Train Loss: 0.0022, Train RMSE: 0.0457, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 55/60, Train Loss: 0.0022, Train RMSE: 0.0465, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 56/60, Train Loss: 0.0022, Train RMSE: 0.0458, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 57/60, Train Loss: 0.0022, Train RMSE: 0.0459, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 58/60, Train Loss: 0.0022, Train RMSE: 0.0462, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 59/60, Train Loss: 0.0022, Train RMSE: 0.0458, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 60/60, Train Loss: 0.0021, Train RMSE: 0.0453, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Training model with num_layers=4, dropout=0.5, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0646, Train RMSE: 0.2534, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 5/60, Train Loss: 0.0645, Train RMSE: 0.2539, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 6/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 7/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 8/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 9/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 10/60, Train Loss: 0.0651, Train RMSE: 0.2551, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 11/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 12/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 13/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 14/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 15/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 16/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 17/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 18/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 19/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 20/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 21/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 22/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 23/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 24/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 25/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 26/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 27/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 28/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 29/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 30/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 31/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 32/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 33/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 34/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 35/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 36/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 37/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 38/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 39/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 40/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 41/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 42/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 43/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 44/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 45/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 46/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 47/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 48/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 49/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 50/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 51/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 52/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 53/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 54/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 55/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 56/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 57/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 58/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 59/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 60/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Training model with num_layers=4, dropout=0.5, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0045, Train RMSE: 0.0662, Val Loss: 0.0025, Val RMSE: 0.0496\n",
      "Epoch 2/60, Train Loss: 0.0038, Train RMSE: 0.0615, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 3/60, Train Loss: 0.0036, Train RMSE: 0.0594, Val Loss: 0.0029, Val RMSE: 0.0535\n",
      "Epoch 4/60, Train Loss: 0.0034, Train RMSE: 0.0578, Val Loss: 0.0025, Val RMSE: 0.0492\n",
      "Epoch 5/60, Train Loss: 0.0032, Train RMSE: 0.0559, Val Loss: 0.0024, Val RMSE: 0.0489\n",
      "Epoch 6/60, Train Loss: 0.0031, Train RMSE: 0.0549, Val Loss: 0.0021, Val RMSE: 0.0453\n",
      "Epoch 7/60, Train Loss: 0.0030, Train RMSE: 0.0539, Val Loss: 0.0022, Val RMSE: 0.0458\n",
      "Epoch 8/60, Train Loss: 0.0030, Train RMSE: 0.0540, Val Loss: 0.0022, Val RMSE: 0.0462\n",
      "Epoch 9/60, Train Loss: 0.0029, Train RMSE: 0.0530, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 10/60, Train Loss: 0.0029, Train RMSE: 0.0532, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 11/60, Train Loss: 0.0028, Train RMSE: 0.0525, Val Loss: 0.0025, Val RMSE: 0.0495\n",
      "Epoch 12/60, Train Loss: 0.0028, Train RMSE: 0.0521, Val Loss: 0.0022, Val RMSE: 0.0456\n",
      "Epoch 13/60, Train Loss: 0.0027, Train RMSE: 0.0510, Val Loss: 0.0022, Val RMSE: 0.0456\n",
      "Epoch 14/60, Train Loss: 0.0027, Train RMSE: 0.0512, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 15/60, Train Loss: 0.0026, Train RMSE: 0.0510, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 16/60, Train Loss: 0.0026, Train RMSE: 0.0507, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 17/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0022, Val RMSE: 0.0462\n",
      "Epoch 18/60, Train Loss: 0.0026, Train RMSE: 0.0502, Val Loss: 0.0023, Val RMSE: 0.0467\n",
      "Epoch 19/60, Train Loss: 0.0025, Train RMSE: 0.0499, Val Loss: 0.0021, Val RMSE: 0.0453\n",
      "Epoch 20/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0021, Val RMSE: 0.0455\n",
      "Epoch 21/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 22/60, Train Loss: 0.0025, Train RMSE: 0.0491, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 23/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0022, Val RMSE: 0.0466\n",
      "Epoch 24/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 25/60, Train Loss: 0.0024, Train RMSE: 0.0487, Val Loss: 0.0021, Val RMSE: 0.0454\n",
      "Epoch 26/60, Train Loss: 0.0024, Train RMSE: 0.0479, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 27/60, Train Loss: 0.0023, Train RMSE: 0.0473, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 28/60, Train Loss: 0.0023, Train RMSE: 0.0476, Val Loss: 0.0021, Val RMSE: 0.0455\n",
      "Epoch 29/60, Train Loss: 0.0023, Train RMSE: 0.0474, Val Loss: 0.0022, Val RMSE: 0.0459\n",
      "Epoch 30/60, Train Loss: 0.0023, Train RMSE: 0.0470, Val Loss: 0.0022, Val RMSE: 0.0463\n",
      "Epoch 31/60, Train Loss: 0.0023, Train RMSE: 0.0470, Val Loss: 0.0021, Val RMSE: 0.0447\n",
      "Epoch 32/60, Train Loss: 0.0023, Train RMSE: 0.0469, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 33/60, Train Loss: 0.0022, Train RMSE: 0.0467, Val Loss: 0.0021, Val RMSE: 0.0455\n",
      "Epoch 34/60, Train Loss: 0.0022, Train RMSE: 0.0466, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 35/60, Train Loss: 0.0022, Train RMSE: 0.0460, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 36/60, Train Loss: 0.0022, Train RMSE: 0.0462, Val Loss: 0.0022, Val RMSE: 0.0463\n",
      "Epoch 37/60, Train Loss: 0.0022, Train RMSE: 0.0459, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 38/60, Train Loss: 0.0021, Train RMSE: 0.0458, Val Loss: 0.0021, Val RMSE: 0.0454\n",
      "Epoch 39/60, Train Loss: 0.0021, Train RMSE: 0.0454, Val Loss: 0.0021, Val RMSE: 0.0454\n",
      "Epoch 40/60, Train Loss: 0.0021, Train RMSE: 0.0452, Val Loss: 0.0022, Val RMSE: 0.0463\n",
      "Epoch 41/60, Train Loss: 0.0020, Train RMSE: 0.0442, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 42/60, Train Loss: 0.0020, Train RMSE: 0.0445, Val Loss: 0.0021, Val RMSE: 0.0455\n",
      "Epoch 43/60, Train Loss: 0.0020, Train RMSE: 0.0441, Val Loss: 0.0021, Val RMSE: 0.0452\n",
      "Epoch 44/60, Train Loss: 0.0020, Train RMSE: 0.0440, Val Loss: 0.0021, Val RMSE: 0.0455\n",
      "Epoch 45/60, Train Loss: 0.0020, Train RMSE: 0.0436, Val Loss: 0.0022, Val RMSE: 0.0468\n",
      "Epoch 46/60, Train Loss: 0.0019, Train RMSE: 0.0434, Val Loss: 0.0021, Val RMSE: 0.0455\n",
      "Epoch 47/60, Train Loss: 0.0019, Train RMSE: 0.0431, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 48/60, Train Loss: 0.0019, Train RMSE: 0.0431, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 49/60, Train Loss: 0.0018, Train RMSE: 0.0425, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 50/60, Train Loss: 0.0018, Train RMSE: 0.0424, Val Loss: 0.0022, Val RMSE: 0.0459\n",
      "Epoch 51/60, Train Loss: 0.0018, Train RMSE: 0.0418, Val Loss: 0.0022, Val RMSE: 0.0457\n",
      "Epoch 52/60, Train Loss: 0.0018, Train RMSE: 0.0414, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 53/60, Train Loss: 0.0017, Train RMSE: 0.0408, Val Loss: 0.0022, Val RMSE: 0.0466\n",
      "Epoch 54/60, Train Loss: 0.0018, Train RMSE: 0.0420, Val Loss: 0.0021, Val RMSE: 0.0455\n",
      "Epoch 55/60, Train Loss: 0.0018, Train RMSE: 0.0422, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 56/60, Train Loss: 0.0019, Train RMSE: 0.0426, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 57/60, Train Loss: 0.0017, Train RMSE: 0.0411, Val Loss: 0.0022, Val RMSE: 0.0458\n",
      "Epoch 58/60, Train Loss: 0.0018, Train RMSE: 0.0422, Val Loss: 0.0022, Val RMSE: 0.0459\n",
      "Epoch 59/60, Train Loss: 0.0018, Train RMSE: 0.0414, Val Loss: 0.0022, Val RMSE: 0.0460\n",
      "Epoch 60/60, Train Loss: 0.0017, Train RMSE: 0.0402, Val Loss: 0.0022, Val RMSE: 0.0460\n",
      "Training model with num_layers=4, dropout=0.6, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0708, Train RMSE: 0.2649, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0647, Train RMSE: 0.2543, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 3/60, Train Loss: 0.0656, Train RMSE: 0.2561, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 4/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 5/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 6/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 7/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 8/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 9/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 10/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 11/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 12/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 13/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 14/60, Train Loss: 0.0645, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 15/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 16/60, Train Loss: 0.0644, Train RMSE: 0.2537, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 17/60, Train Loss: 0.0644, Train RMSE: 0.2538, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 18/60, Train Loss: 0.0643, Train RMSE: 0.2534, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 19/60, Train Loss: 0.0514, Train RMSE: 0.2189, Val Loss: 0.0051, Val RMSE: 0.0715\n",
      "Epoch 20/60, Train Loss: 0.0039, Train RMSE: 0.0618, Val Loss: 0.0027, Val RMSE: 0.0516\n",
      "Epoch 21/60, Train Loss: 0.0030, Train RMSE: 0.0540, Val Loss: 0.0026, Val RMSE: 0.0500\n",
      "Epoch 22/60, Train Loss: 0.0027, Train RMSE: 0.0514, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 23/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 24/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 25/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0024, Val RMSE: 0.0484\n",
      "Epoch 26/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0024, Val RMSE: 0.0482\n",
      "Epoch 27/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 28/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0025, Val RMSE: 0.0487\n",
      "Epoch 29/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 30/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 31/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 32/60, Train Loss: 0.0025, Train RMSE: 0.0492, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 33/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 34/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0025, Val RMSE: 0.0494\n",
      "Epoch 35/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 36/60, Train Loss: 0.0025, Train RMSE: 0.0497, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 37/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 38/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 39/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 40/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 41/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0489\n",
      "Epoch 42/60, Train Loss: 0.0025, Train RMSE: 0.0492, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 43/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0488\n",
      "Epoch 44/60, Train Loss: 0.0025, Train RMSE: 0.0498, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 45/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 46/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0025, Val RMSE: 0.0493\n",
      "Epoch 47/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 48/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 49/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 50/60, Train Loss: 0.0025, Train RMSE: 0.0492, Val Loss: 0.0024, Val RMSE: 0.0486\n",
      "Epoch 51/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 52/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 53/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0024, Val RMSE: 0.0487\n",
      "Epoch 54/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0025, Val RMSE: 0.0487\n",
      "Epoch 55/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0025, Val RMSE: 0.0488\n",
      "Epoch 56/60, Train Loss: 0.0025, Train RMSE: 0.0492, Val Loss: 0.0024, Val RMSE: 0.0485\n",
      "Epoch 57/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 58/60, Train Loss: 0.0025, Train RMSE: 0.0494, Val Loss: 0.0026, Val RMSE: 0.0498\n",
      "Epoch 59/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0025, Val RMSE: 0.0488\n",
      "Epoch 60/60, Train Loss: 0.0025, Train RMSE: 0.0491, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Training model with num_layers=4, dropout=0.6, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0050, Train RMSE: 0.0695, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 2/60, Train Loss: 0.0039, Train RMSE: 0.0619, Val Loss: 0.0023, Val RMSE: 0.0478\n",
      "Epoch 3/60, Train Loss: 0.0037, Train RMSE: 0.0601, Val Loss: 0.0023, Val RMSE: 0.0478\n",
      "Epoch 4/60, Train Loss: 0.0035, Train RMSE: 0.0587, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 5/60, Train Loss: 0.0033, Train RMSE: 0.0569, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 6/60, Train Loss: 0.0031, Train RMSE: 0.0552, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 7/60, Train Loss: 0.0030, Train RMSE: 0.0545, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 8/60, Train Loss: 0.0030, Train RMSE: 0.0541, Val Loss: 0.0025, Val RMSE: 0.0497\n",
      "Epoch 9/60, Train Loss: 0.0030, Train RMSE: 0.0540, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 10/60, Train Loss: 0.0029, Train RMSE: 0.0533, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 11/60, Train Loss: 0.0029, Train RMSE: 0.0531, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 12/60, Train Loss: 0.0027, Train RMSE: 0.0515, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 13/60, Train Loss: 0.0028, Train RMSE: 0.0527, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 14/60, Train Loss: 0.0027, Train RMSE: 0.0517, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 15/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 16/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0022, Val RMSE: 0.0462\n",
      "Epoch 17/60, Train Loss: 0.0027, Train RMSE: 0.0513, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 18/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0022, Val RMSE: 0.0458\n",
      "Epoch 19/60, Train Loss: 0.0026, Train RMSE: 0.0500, Val Loss: 0.0022, Val RMSE: 0.0460\n",
      "Epoch 20/60, Train Loss: 0.0025, Train RMSE: 0.0492, Val Loss: 0.0022, Val RMSE: 0.0459\n",
      "Epoch 21/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 22/60, Train Loss: 0.0025, Train RMSE: 0.0495, Val Loss: 0.0022, Val RMSE: 0.0458\n",
      "Epoch 23/60, Train Loss: 0.0024, Train RMSE: 0.0489, Val Loss: 0.0022, Val RMSE: 0.0458\n",
      "Epoch 24/60, Train Loss: 0.0024, Train RMSE: 0.0487, Val Loss: 0.0022, Val RMSE: 0.0458\n",
      "Epoch 25/60, Train Loss: 0.0024, Train RMSE: 0.0487, Val Loss: 0.0021, Val RMSE: 0.0454\n",
      "Epoch 26/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 27/60, Train Loss: 0.0024, Train RMSE: 0.0487, Val Loss: 0.0022, Val RMSE: 0.0458\n",
      "Epoch 28/60, Train Loss: 0.0023, Train RMSE: 0.0475, Val Loss: 0.0022, Val RMSE: 0.0459\n",
      "Epoch 29/60, Train Loss: 0.0023, Train RMSE: 0.0477, Val Loss: 0.0022, Val RMSE: 0.0458\n",
      "Epoch 30/60, Train Loss: 0.0022, Train RMSE: 0.0468, Val Loss: 0.0022, Val RMSE: 0.0459\n",
      "Epoch 31/60, Train Loss: 0.0023, Train RMSE: 0.0472, Val Loss: 0.0022, Val RMSE: 0.0462\n",
      "Epoch 32/60, Train Loss: 0.0023, Train RMSE: 0.0473, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 33/60, Train Loss: 0.0023, Train RMSE: 0.0472, Val Loss: 0.0022, Val RMSE: 0.0463\n",
      "Epoch 34/60, Train Loss: 0.0022, Train RMSE: 0.0463, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 35/60, Train Loss: 0.0022, Train RMSE: 0.0465, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 36/60, Train Loss: 0.0022, Train RMSE: 0.0460, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 37/60, Train Loss: 0.0021, Train RMSE: 0.0455, Val Loss: 0.0023, Val RMSE: 0.0467\n",
      "Epoch 38/60, Train Loss: 0.0022, Train RMSE: 0.0460, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 39/60, Train Loss: 0.0021, Train RMSE: 0.0451, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 40/60, Train Loss: 0.0020, Train RMSE: 0.0447, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 41/60, Train Loss: 0.0021, Train RMSE: 0.0448, Val Loss: 0.0022, Val RMSE: 0.0460\n",
      "Epoch 42/60, Train Loss: 0.0020, Train RMSE: 0.0442, Val Loss: 0.0022, Val RMSE: 0.0459\n",
      "Epoch 43/60, Train Loss: 0.0020, Train RMSE: 0.0445, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 44/60, Train Loss: 0.0020, Train RMSE: 0.0440, Val Loss: 0.0022, Val RMSE: 0.0462\n",
      "Epoch 45/60, Train Loss: 0.0019, Train RMSE: 0.0429, Val Loss: 0.0022, Val RMSE: 0.0462\n",
      "Epoch 46/60, Train Loss: 0.0019, Train RMSE: 0.0432, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 47/60, Train Loss: 0.0019, Train RMSE: 0.0431, Val Loss: 0.0022, Val RMSE: 0.0459\n",
      "Epoch 48/60, Train Loss: 0.0019, Train RMSE: 0.0431, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 49/60, Train Loss: 0.0018, Train RMSE: 0.0426, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 50/60, Train Loss: 0.0019, Train RMSE: 0.0434, Val Loss: 0.0022, Val RMSE: 0.0461\n",
      "Epoch 51/60, Train Loss: 0.0019, Train RMSE: 0.0427, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 52/60, Train Loss: 0.0018, Train RMSE: 0.0423, Val Loss: 0.0023, Val RMSE: 0.0465\n",
      "Epoch 53/60, Train Loss: 0.0018, Train RMSE: 0.0420, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 54/60, Train Loss: 0.0019, Train RMSE: 0.0430, Val Loss: 0.0022, Val RMSE: 0.0464\n",
      "Epoch 55/60, Train Loss: 0.0019, Train RMSE: 0.0426, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 56/60, Train Loss: 0.0018, Train RMSE: 0.0420, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 57/60, Train Loss: 0.0018, Train RMSE: 0.0420, Val Loss: 0.0023, Val RMSE: 0.0467\n",
      "Epoch 58/60, Train Loss: 0.0018, Train RMSE: 0.0422, Val Loss: 0.0022, Val RMSE: 0.0465\n",
      "Epoch 59/60, Train Loss: 0.0018, Train RMSE: 0.0417, Val Loss: 0.0023, Val RMSE: 0.0467\n",
      "Epoch 60/60, Train Loss: 0.0018, Train RMSE: 0.0419, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Training model with num_layers=4, dropout=0.7, learning_rate=0.001\n",
      "Epoch 1/60, Train Loss: 0.0642, Train RMSE: 0.2527, Val Loss: 0.0658, Val RMSE: 0.2564\n",
      "Epoch 2/60, Train Loss: 0.0279, Train RMSE: 0.1500, Val Loss: 0.0052, Val RMSE: 0.0720\n",
      "Epoch 3/60, Train Loss: 0.0050, Train RMSE: 0.0704, Val Loss: 0.0043, Val RMSE: 0.0649\n",
      "Epoch 4/60, Train Loss: 0.0042, Train RMSE: 0.0643, Val Loss: 0.0036, Val RMSE: 0.0598\n",
      "Epoch 5/60, Train Loss: 0.0036, Train RMSE: 0.0597, Val Loss: 0.0032, Val RMSE: 0.0561\n",
      "Epoch 6/60, Train Loss: 0.0032, Train RMSE: 0.0563, Val Loss: 0.0029, Val RMSE: 0.0535\n",
      "Epoch 7/60, Train Loss: 0.0030, Train RMSE: 0.0538, Val Loss: 0.0028, Val RMSE: 0.0520\n",
      "Epoch 8/60, Train Loss: 0.0028, Train RMSE: 0.0525, Val Loss: 0.0027, Val RMSE: 0.0511\n",
      "Epoch 9/60, Train Loss: 0.0027, Train RMSE: 0.0518, Val Loss: 0.0026, Val RMSE: 0.0507\n",
      "Epoch 10/60, Train Loss: 0.0027, Train RMSE: 0.0512, Val Loss: 0.0026, Val RMSE: 0.0505\n",
      "Epoch 11/60, Train Loss: 0.0027, Train RMSE: 0.0509, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 12/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 13/60, Train Loss: 0.0026, Train RMSE: 0.0507, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 14/60, Train Loss: 0.0027, Train RMSE: 0.0507, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 15/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 16/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 17/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 18/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 19/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 20/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 21/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 22/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 23/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 24/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 25/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 26/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 27/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 28/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 29/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 30/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 31/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 32/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 33/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 34/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 35/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 36/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 37/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 38/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 39/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 40/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 41/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 42/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 43/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 44/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 45/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 46/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 47/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 48/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 49/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 50/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 51/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 52/60, Train Loss: 0.0026, Train RMSE: 0.0502, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 53/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 54/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 55/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 56/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 57/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 58/60, Train Loss: 0.0026, Train RMSE: 0.0504, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Epoch 59/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0503\n",
      "Epoch 60/60, Train Loss: 0.0026, Train RMSE: 0.0505, Val Loss: 0.0026, Val RMSE: 0.0502\n",
      "Training model with num_layers=4, dropout=0.7, learning_rate=0.0001\n",
      "Epoch 1/60, Train Loss: 0.0050, Train RMSE: 0.0702, Val Loss: 0.0034, Val RMSE: 0.0577\n",
      "Epoch 2/60, Train Loss: 0.0044, Train RMSE: 0.0661, Val Loss: 0.0027, Val RMSE: 0.0519\n",
      "Epoch 3/60, Train Loss: 0.0041, Train RMSE: 0.0638, Val Loss: 0.0026, Val RMSE: 0.0506\n",
      "Epoch 4/60, Train Loss: 0.0039, Train RMSE: 0.0624, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 5/60, Train Loss: 0.0039, Train RMSE: 0.0618, Val Loss: 0.0025, Val RMSE: 0.0491\n",
      "Epoch 6/60, Train Loss: 0.0036, Train RMSE: 0.0596, Val Loss: 0.0035, Val RMSE: 0.0591\n",
      "Epoch 7/60, Train Loss: 0.0036, Train RMSE: 0.0598, Val Loss: 0.0026, Val RMSE: 0.0499\n",
      "Epoch 8/60, Train Loss: 0.0034, Train RMSE: 0.0578, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 9/60, Train Loss: 0.0034, Train RMSE: 0.0580, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 10/60, Train Loss: 0.0033, Train RMSE: 0.0571, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 11/60, Train Loss: 0.0032, Train RMSE: 0.0559, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 12/60, Train Loss: 0.0032, Train RMSE: 0.0561, Val Loss: 0.0026, Val RMSE: 0.0501\n",
      "Epoch 13/60, Train Loss: 0.0031, Train RMSE: 0.0555, Val Loss: 0.0025, Val RMSE: 0.0490\n",
      "Epoch 14/60, Train Loss: 0.0032, Train RMSE: 0.0556, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 15/60, Train Loss: 0.0031, Train RMSE: 0.0549, Val Loss: 0.0024, Val RMSE: 0.0478\n",
      "Epoch 16/60, Train Loss: 0.0030, Train RMSE: 0.0546, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 17/60, Train Loss: 0.0030, Train RMSE: 0.0542, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 18/60, Train Loss: 0.0030, Train RMSE: 0.0539, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 19/60, Train Loss: 0.0029, Train RMSE: 0.0535, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 20/60, Train Loss: 0.0029, Train RMSE: 0.0535, Val Loss: 0.0023, Val RMSE: 0.0478\n",
      "Epoch 21/60, Train Loss: 0.0028, Train RMSE: 0.0524, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 22/60, Train Loss: 0.0029, Train RMSE: 0.0535, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 23/60, Train Loss: 0.0028, Train RMSE: 0.0528, Val Loss: 0.0024, Val RMSE: 0.0480\n",
      "Epoch 24/60, Train Loss: 0.0028, Train RMSE: 0.0521, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 25/60, Train Loss: 0.0028, Train RMSE: 0.0520, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 26/60, Train Loss: 0.0027, Train RMSE: 0.0517, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 27/60, Train Loss: 0.0027, Train RMSE: 0.0512, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 28/60, Train Loss: 0.0027, Train RMSE: 0.0514, Val Loss: 0.0023, Val RMSE: 0.0475\n",
      "Epoch 29/60, Train Loss: 0.0027, Train RMSE: 0.0514, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 30/60, Train Loss: 0.0026, Train RMSE: 0.0508, Val Loss: 0.0023, Val RMSE: 0.0469\n",
      "Epoch 31/60, Train Loss: 0.0026, Train RMSE: 0.0506, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 32/60, Train Loss: 0.0026, Train RMSE: 0.0501, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 33/60, Train Loss: 0.0025, Train RMSE: 0.0498, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 34/60, Train Loss: 0.0026, Train RMSE: 0.0503, Val Loss: 0.0024, Val RMSE: 0.0481\n",
      "Epoch 35/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 36/60, Train Loss: 0.0024, Train RMSE: 0.0489, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 37/60, Train Loss: 0.0025, Train RMSE: 0.0493, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 38/60, Train Loss: 0.0025, Train RMSE: 0.0496, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 39/60, Train Loss: 0.0024, Train RMSE: 0.0488, Val Loss: 0.0023, Val RMSE: 0.0474\n",
      "Epoch 40/60, Train Loss: 0.0024, Train RMSE: 0.0488, Val Loss: 0.0024, Val RMSE: 0.0478\n",
      "Epoch 41/60, Train Loss: 0.0024, Train RMSE: 0.0485, Val Loss: 0.0023, Val RMSE: 0.0467\n",
      "Epoch 42/60, Train Loss: 0.0024, Train RMSE: 0.0484, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 43/60, Train Loss: 0.0024, Train RMSE: 0.0486, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 44/60, Train Loss: 0.0024, Train RMSE: 0.0485, Val Loss: 0.0023, Val RMSE: 0.0477\n",
      "Epoch 45/60, Train Loss: 0.0023, Train RMSE: 0.0477, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 46/60, Train Loss: 0.0023, Train RMSE: 0.0477, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 47/60, Train Loss: 0.0023, Train RMSE: 0.0476, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 48/60, Train Loss: 0.0023, Train RMSE: 0.0475, Val Loss: 0.0022, Val RMSE: 0.0467\n",
      "Epoch 49/60, Train Loss: 0.0023, Train RMSE: 0.0471, Val Loss: 0.0023, Val RMSE: 0.0472\n",
      "Epoch 50/60, Train Loss: 0.0023, Train RMSE: 0.0470, Val Loss: 0.0024, Val RMSE: 0.0483\n",
      "Epoch 51/60, Train Loss: 0.0023, Train RMSE: 0.0474, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 52/60, Train Loss: 0.0023, Train RMSE: 0.0473, Val Loss: 0.0023, Val RMSE: 0.0476\n",
      "Epoch 53/60, Train Loss: 0.0023, Train RMSE: 0.0470, Val Loss: 0.0023, Val RMSE: 0.0473\n",
      "Epoch 54/60, Train Loss: 0.0023, Train RMSE: 0.0469, Val Loss: 0.0024, Val RMSE: 0.0479\n",
      "Epoch 55/60, Train Loss: 0.0022, Train RMSE: 0.0467, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 56/60, Train Loss: 0.0022, Train RMSE: 0.0465, Val Loss: 0.0023, Val RMSE: 0.0470\n",
      "Epoch 57/60, Train Loss: 0.0022, Train RMSE: 0.0467, Val Loss: 0.0022, Val RMSE: 0.0466\n",
      "Epoch 58/60, Train Loss: 0.0022, Train RMSE: 0.0461, Val Loss: 0.0023, Val RMSE: 0.0471\n",
      "Epoch 59/60, Train Loss: 0.0022, Train RMSE: 0.0462, Val Loss: 0.0023, Val RMSE: 0.0468\n",
      "Epoch 60/60, Train Loss: 0.0022, Train RMSE: 0.0460, Val Loss: 0.0023, Val RMSE: 0.0469\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "# Define the SimpleCNN class\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_layers=1, dropout=0.7):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout\n",
    "        \n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=columns_to_keep_from_end, out_channels=16, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.bn_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(1, num_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels=16*(2**(i-1)), out_channels=16*(2**i), kernel_size=(1, 3), padding=(0, 1)))\n",
    "            self.bn_layers.append(nn.BatchNorm2d(16*(2**i)))\n",
    "        \n",
    "        # Define pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "\n",
    "        # Calculate the flattened size after convolution and pooling\n",
    "        self.flattened_size = 16 * (2**(num_layers-1)) * 10 * (600 // (2**num_layers))\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Define activation and dropout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.pool(x)\n",
    "            x = self.relu(self.bn_layers[i](self.conv_layers[i](x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters grid\n",
    "hyperparams_grid = {\n",
    "    'num_layers': [1, 2, 3, 4],\n",
    "    'dropout': [0.5, 0.6, 0.7],\n",
    "    'learning_rate': [0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Training loop with hyperparameter tuning\n",
    "best_model_info = {}  # Dictionary to store the best model information\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for num_layers, dropout, learning_rate in itertools.product(*hyperparams_grid.values()):\n",
    "    print(f\"Training model with num_layers={num_layers}, dropout={dropout}, learning_rate={learning_rate}\")\n",
    "    \n",
    "    # Create model instance\n",
    "    model = SimpleCNN(num_layers=num_layers, dropout=dropout).to(device)\n",
    "    \n",
    "    # Define optimizer and criterion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_rmse = train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_rmse = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Val Loss: {val_loss:.4f}, Val RMSE: {val_rmse:.4f}')\n",
    "        \n",
    "        # Check if this model has the best validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Store the best model information\n",
    "            best_model_info = {\n",
    "                'num_layers': num_layers,\n",
    "                'dropout': dropout,\n",
    "                'learning_rate': learning_rate,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss\n",
    "            }\n",
    "\n",
    "# Save the best model information\n",
    "torch.save(best_model_info, 'best_model_info.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has the following parameters : num_layers = 4, dropout = 0.5, learning_rate = 0.0001\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best model has the following parameters : num_layers = {best_model_info['num_layers']}, dropout = {best_model_info['dropout']}, learning_rate = {best_model_info['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model analysis (you can also test here a model with your own parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Train RMSE Loss: 0.0625, Val RMSE Loss: 0.0482\n",
      "Epoch 2/60, Train RMSE Loss: 0.0564, Val RMSE Loss: 0.0489\n",
      "Epoch 3/60, Train RMSE Loss: 0.0543, Val RMSE Loss: 0.0457\n",
      "Epoch 4/60, Train RMSE Loss: 0.0530, Val RMSE Loss: 0.0467\n",
      "Epoch 5/60, Train RMSE Loss: 0.0510, Val RMSE Loss: 0.0459\n",
      "Epoch 6/60, Train RMSE Loss: 0.0502, Val RMSE Loss: 0.0459\n",
      "Epoch 7/60, Train RMSE Loss: 0.0505, Val RMSE Loss: 0.0483\n",
      "Epoch 8/60, Train RMSE Loss: 0.0489, Val RMSE Loss: 0.0460\n",
      "Epoch 9/60, Train RMSE Loss: 0.0491, Val RMSE Loss: 0.0456\n",
      "Epoch 10/60, Train RMSE Loss: 0.0484, Val RMSE Loss: 0.0459\n",
      "Epoch 11/60, Train RMSE Loss: 0.0470, Val RMSE Loss: 0.0466\n",
      "Epoch 12/60, Train RMSE Loss: 0.0463, Val RMSE Loss: 0.0450\n",
      "Epoch 13/60, Train RMSE Loss: 0.0453, Val RMSE Loss: 0.0452\n",
      "Epoch 14/60, Train RMSE Loss: 0.0451, Val RMSE Loss: 0.0458\n",
      "Epoch 15/60, Train RMSE Loss: 0.0449, Val RMSE Loss: 0.0462\n",
      "Epoch 16/60, Train RMSE Loss: 0.0443, Val RMSE Loss: 0.0471\n",
      "Epoch 17/60, Train RMSE Loss: 0.0438, Val RMSE Loss: 0.0453\n",
      "Epoch 18/60, Train RMSE Loss: 0.0433, Val RMSE Loss: 0.0461\n",
      "Epoch 19/60, Train RMSE Loss: 0.0421, Val RMSE Loss: 0.0461\n",
      "Epoch 20/60, Train RMSE Loss: 0.0427, Val RMSE Loss: 0.0455\n",
      "Epoch 21/60, Train RMSE Loss: 0.0420, Val RMSE Loss: 0.0470\n",
      "Epoch 22/60, Train RMSE Loss: 0.0421, Val RMSE Loss: 0.0463\n",
      "Epoch 23/60, Train RMSE Loss: 0.0416, Val RMSE Loss: 0.0463\n",
      "Epoch 24/60, Train RMSE Loss: 0.0406, Val RMSE Loss: 0.0459\n",
      "Epoch 25/60, Train RMSE Loss: 0.0397, Val RMSE Loss: 0.0463\n",
      "Epoch 26/60, Train RMSE Loss: 0.0399, Val RMSE Loss: 0.0461\n",
      "Epoch 27/60, Train RMSE Loss: 0.0407, Val RMSE Loss: 0.0465\n",
      "Epoch 28/60, Train RMSE Loss: 0.0408, Val RMSE Loss: 0.0477\n",
      "Epoch 29/60, Train RMSE Loss: 0.0399, Val RMSE Loss: 0.0470\n",
      "Epoch 30/60, Train RMSE Loss: 0.0399, Val RMSE Loss: 0.0470\n",
      "Epoch 31/60, Train RMSE Loss: 0.0398, Val RMSE Loss: 0.0463\n",
      "Epoch 32/60, Train RMSE Loss: 0.0387, Val RMSE Loss: 0.0462\n",
      "Epoch 33/60, Train RMSE Loss: 0.0388, Val RMSE Loss: 0.0467\n",
      "Epoch 34/60, Train RMSE Loss: 0.0383, Val RMSE Loss: 0.0468\n",
      "Epoch 35/60, Train RMSE Loss: 0.0388, Val RMSE Loss: 0.0460\n",
      "Epoch 36/60, Train RMSE Loss: 0.0383, Val RMSE Loss: 0.0458\n",
      "Epoch 37/60, Train RMSE Loss: 0.0383, Val RMSE Loss: 0.0464\n",
      "Epoch 38/60, Train RMSE Loss: 0.0383, Val RMSE Loss: 0.0465\n",
      "Epoch 39/60, Train RMSE Loss: 0.0390, Val RMSE Loss: 0.0472\n",
      "Epoch 40/60, Train RMSE Loss: 0.0386, Val RMSE Loss: 0.0469\n",
      "Epoch 41/60, Train RMSE Loss: 0.0377, Val RMSE Loss: 0.0499\n",
      "Epoch 42/60, Train RMSE Loss: 0.0381, Val RMSE Loss: 0.0471\n",
      "Epoch 43/60, Train RMSE Loss: 0.0388, Val RMSE Loss: 0.0467\n",
      "Epoch 44/60, Train RMSE Loss: 0.0383, Val RMSE Loss: 0.0467\n",
      "Epoch 45/60, Train RMSE Loss: 0.0377, Val RMSE Loss: 0.0460\n",
      "Epoch 46/60, Train RMSE Loss: 0.0368, Val RMSE Loss: 0.0463\n",
      "Epoch 47/60, Train RMSE Loss: 0.0366, Val RMSE Loss: 0.0466\n",
      "Epoch 48/60, Train RMSE Loss: 0.0368, Val RMSE Loss: 0.0464\n",
      "Epoch 49/60, Train RMSE Loss: 0.0369, Val RMSE Loss: 0.0469\n",
      "Epoch 50/60, Train RMSE Loss: 0.0371, Val RMSE Loss: 0.0474\n",
      "Epoch 51/60, Train RMSE Loss: 0.0365, Val RMSE Loss: 0.0461\n",
      "Epoch 52/60, Train RMSE Loss: 0.0363, Val RMSE Loss: 0.0466\n",
      "Epoch 53/60, Train RMSE Loss: 0.0372, Val RMSE Loss: 0.0472\n",
      "Epoch 54/60, Train RMSE Loss: 0.0364, Val RMSE Loss: 0.0465\n",
      "Epoch 55/60, Train RMSE Loss: 0.0358, Val RMSE Loss: 0.0465\n",
      "Epoch 56/60, Train RMSE Loss: 0.0367, Val RMSE Loss: 0.0465\n",
      "Epoch 57/60, Train RMSE Loss: 0.0369, Val RMSE Loss: 0.0469\n",
      "Epoch 58/60, Train RMSE Loss: 0.0361, Val RMSE Loss: 0.0465\n",
      "Epoch 59/60, Train RMSE Loss: 0.0356, Val RMSE Loss: 0.0466\n",
      "Epoch 60/60, Train RMSE Loss: 0.0357, Val RMSE Loss: 0.0463\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAHHCAYAAABEEKc/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAACeu0lEQVR4nOzdd3hT1RvA8W/SvUt3C2XPQtlQpoCCRVC27M0PARkKggouEBVUUFBQRGXKHiIiMmVT9t57t5S2tKW7Te7vj0sDoYNO0sL7eZ48TW/OvfckTZP3nvOeczSKoigIIYQQQggDrakrIIQQQghR0EiAJIQQQgjxBAmQhBBCCCGeIAGSEEIIIcQTJEASQgghhHiCBEhCCCGEEE+QAEkIIYQQ4gkSIAkhhBBCPEECJCGEEEKIJ0iAJEQhcu3aNTQaDfPmzcvR/hqNhvHjx+dpnQqavn37UrJkyWd+3vHjx6PRaIy2lSxZkr59+z5133nz5qHRaLh27Vqe1Se37xUhXnQSIAmTS/1y0Gg07N69O83jiqLg6+uLRqPh9ddfN0ENny71y/Fpt6ZNm5q6qi+80NBQzM3N6dmzZ4ZlHjx4gI2NDR06dHiGNcuZxYsXM23aNFNXw0jfvn2N3vdWVlaUL1+eTz/9lISEhDTlU8v973//S/d4H330kaFMWFiY0WN///03TZo0wcPDA1tbW0qXLk3nzp3ZsGGDoUxqsJjRbfLkyZk+n9T/7yfPLZ5v5qaugBCprK2tWbx4MY0aNTLavmPHDm7duoWVlZWJavZ0HTp0oGzZsobfY2JiGDJkCO3btzf6kvX09MzVeUqUKEF8fDwWFhY52j8+Ph5z8xf7397Dw4MWLVrw119/ERcXh62tbZoyq1evJiEhIdMgKivOnz+PVpu/16GLFy/m1KlTvPvuu0bbc/teyS0rKyt+++03AKKiovjrr7+YOHEily9fZtGiRWnKW1tbs2rVKn766ScsLS2NHluyZAnW1tZpgqspU6YwZswYmjRpwtixY7G1teXSpUts2bKFpUuX0rJlS6Py3bp1o1WrVmnOXaNGjdw+XfEcerE/KUWB0qpVK1asWMEPP/xg9CW+ePFiatWqVaCv3qpWrUrVqlUNv4eFhTFkyBCqVq2a6ZdsQkIClpaWWf4S1Wg0WFtb57ieudn3edKjRw82bNjA2rVr6dq1a5rHFy9ejJOTE61bt87VeUwZ1Of2vZJbT7bSvf322zRo0IAlS5bw3XffpblYaNmyJWvXruXff/+lbdu2hu179+7l6tWrdOzYkVWrVhm2p6SkMHHiRFq0aMGmTZvSnD80NDTNtpo1a+Y66BUvDuliEwVGt27dCA8PZ/PmzYZtSUlJrFy5ku7du6e7j16vZ9q0aVSuXBlra2s8PT0ZNGgQ9+/fNyr3119/0bp1a3x8fLCysqJMmTJMnDgRnU5nVK5p06ZUqVKFM2fO0KxZM2xtbSlatCjffPNNrp/f9u3b0Wg0LF26lI8//piiRYtia2tLdHQ0ERERjB49Gn9/f+zt7XF0dOS1117j+PHjRsdIL6+kb9++2Nvbc/v2bdq1a4e9vT3u7u6MHj06zfN7Mgcptevg0qVL9O3bF2dnZ5ycnOjXrx9xcXFG+8bHxzNixAjc3NxwcHCgTZs23L59O0t5TUlJSXz66afUqlULJycn7OzsaNy4Mdu2bUv3+U2ZMoXZs2dTpkwZrKysqFOnDgcPHkxz3DVr1lClShWsra2pUqUKf/75Z6b1SNW+fXvs7OxYvHhxmsdCQ0PZunUrnTp1wsrKil27dvHmm29SvHhxrKys8PX1ZeTIkcTHxz/1POnlIJ0+fZqXX34ZGxsbihUrxhdffIFer0+zb1bes02bNuWff/7h+vXrhu6i1PyrjHKQ/vvvPxo3boydnR3Ozs60bduWs2fPGpXJzvsiqzQaDY0aNUJRFK5cuZLm8aJFi/LSSy+l+ZssWrQIf39/qlSpYrQ9LCyM6OhoGjZsmO75PDw8clTP3MjKa/vgwQPeffddSpYsiZWVlaFF88iRI4YyFy9epGPHjnh5eWFtbU2xYsXo2rUrUVFRz/opvdCkBUkUGCVLlqR+/fosWbKE1157DYB///2XqKgounbtyg8//JBmn0GDBjFv3jz69evHiBEjuHr1KjNmzODo0aPs2bPH0L0wb9487O3tGTVqFPb29vz33398+umnREdH8+233xod8/79+7Rs2ZIOHTrQuXNnVq5cyQcffIC/v7+hXrkxceJELC0tGT16NImJiVhaWnLmzBnWrFnDm2++SalSpbh79y6//PILTZo04cyZM/j4+GR6TJ1OR2BgIAEBAUyZMoUtW7YwdepUypQpw5AhQ55ap86dO1OqVCkmTZrEkSNH+O233/Dw8ODrr782lOnbty/Lly+nV69e1KtXjx07dmS5hSU6OprffvuNbt26MXDgQB48eMDvv/9OYGAgBw4coHr16kblFy9ezIMHDxg0aBAajYZvvvmGDh06cOXKFcPfdNOmTXTs2BE/Pz8mTZpEeHg4/fr1o1ixYk+tj52dHW3btmXlypVERETg4uJieGzZsmXodDp69OgBwIoVK4iLi2PIkCG4urpy4MABfvzxR27dusWKFSuy9PxThYSE0KxZM1JSUvjwww+xs7Nj9uzZ2NjYpCmblffsRx99RFRUFLdu3eL7778HwN7ePsPzb9myhddee43SpUszfvx44uPj+fHHH2nYsCFHjhxJk9yelfdFdqQmoRcpUiTdx7t3784777xDTEwM9vb2pKSksGLFCkaNGpWme83DwwMbGxv+/vtvhg8fbvQ3zEhcXFy6LdHOzs657nrO6ms7ePBgVq5cybBhw/Dz8yM8PJzdu3dz9uxZatasSVJSEoGBgSQmJjJ8+HC8vLy4ffs269atIzIyEicnp1zVU2SDIoSJzZ07VwGUgwcPKjNmzFAcHByUuLg4RVEU5c0331SaNWumKIqilChRQmndurVhv127dimAsmjRIqPjbdiwIc321OM9btCgQYqtra2SkJBg2NakSRMFUBYsWGDYlpiYqHh5eSkdO3bM8nO6d++eAiifffaZYdu2bdsUQCldunSa+iQkJCg6nc5o29WrVxUrKyvl888/N9oGKHPnzjVs69OnjwIYlVMURalRo4ZSq1Yto21P1umzzz5TAKV///5G5dq3b6+4uroafj98+LACKO+++65Rub59+6Y5ZnpSUlKUxMREo233799XPD09jc6d+vxcXV2ViIgIw/a//vpLAZS///7bsK169eqKt7e3EhkZadi2adMmBVBKlCiRaX0URVH++ecfBVB++eUXo+316tVTihYtavh7pPfemTRpkqLRaJTr168btqW+lo8rUaKE0qdPH8Pv7777rgIo+/fvN2wLDQ1VnJycFEC5evWqYXtW37OtW7dO9/mm916pXr264uHhoYSHhxu2HT9+XNFqtUrv3r3TPJenvS8y0qdPH8XOzk65d++ecu/ePeXSpUvKlClTFI1Go1SpUkXR6/VG5QFl6NChSkREhGJpaaksXLhQURT1b6TRaJRr164Z6nTv3j3Dfp9++qkCKHZ2dsprr72mfPnll8rhw4czfC0yugUFBWX6fNI795Oy+to6OTkpQ4cOzfA4R48eVQBlxYoVmdZJ5D/pYhMFSufOnYmPj2fdunU8ePCAdevWZdi9tmLFCpycnGjRogVhYWGGW61atbC3tzfqvnn8Cv3BgweEhYXRuHFj4uLiOHfunNFx7e3tjfIULC0tqVu3brrdAjnRp0+fNC0GVlZWhjwknU5HeHg49vb2VKhQwajpPTODBw82+r1x48ZZrnN6+4aHhxMdHQ1gGBH09ttvG5UbPnx4lo5vZmZmSLzV6/VERESQkpJC7dq1031+Xbp0MWplaNy4MYDh+QQHB3Ps2DH69OljdEXdokUL/Pz8slSnV199FXd3d6MunatXr7Jv3z66detm+Hs8/reKjY0lLCyMBg0aoCgKR48ezdK5Uq1fv5569epRt25dwzZ3d3dDa9XjsvOezYrU16xv375GrS1Vq1alRYsWrF+/Ps0+T3tfZCY2NhZ3d3fc3d0pW7Yso0ePpmHDhvz1119ppkNIVaRIEVq2bMmSJUsAtSWxQYMGlChRIt3yEyZMYPHixdSoUYONGzfy0UcfUatWLWrWrJmmawvgrbfeYvPmzWluWX3PZCQ7r62zszP79+/nzp076R4r9f28cePGHHdnirwhAZIoUNzd3WnevDmLFy9m9erV6HQ6OnXqlG7ZixcvEhUVhYeHh+GDOPUWExNjlKR5+vRp2rdvj5OTE46Ojri7uxuCoCf79YsVK5bmA7xIkSJp8ppyqlSpUmm26fV6vv/+e8qVK4eVlRVubm64u7tz4sSJLOUdWFtb4+7unuM6Fy9ePM2+gGH/69evo9Vq09T98ZF7TzN//nyqVq2KtbU1rq6uuLu7888//6T7/LJSH4By5cql2bdChQpZqo+5uTldunRh165d3L59G8AQLD0esNy4ccPwxZea39WkSRMg7Xvnaa5fv57lOmfnPZvVc2d0rkqVKhEWFkZsbKzR9qf9HTJjbW1tCEDmzp1LpUqVCA0NTbc78XHdu3dn8+bN3LhxgzVr1mR4gZSqW7du7Nq1i/v377Np0ya6d+/O0aNHeeONN9J0y5UrV47mzZunuTk6Oj71+WQmO6/tN998w6lTp/D19aVu3bqMHz/e6EKmVKlSjBo1it9++w03NzcCAwOZOXOm5B+ZgOQgiQKne/fuDBw4kJCQEF577TWcnZ3TLafX6/Hw8Eh3yDBgCBgiIyNp0qQJjo6OfP7555QpUwZra2uOHDnCBx98kCZB1szMLN3jKYqS8yf1mPS+IL766is++eQT+vfvz8SJE3FxcUGr1fLuu++mm8D7pIzqnFX5/Zz/+OMP+vbtS7t27RgzZgweHh6YmZkxadIkLl++/Mzrk6pnz57MmDGDJUuWMHr0aJYsWYKfn58hJ0qn09GiRQsiIiL44IMPqFixInZ2dty+fZu+fftm6W+TE9l9z+aX3PwdzMzMaN68ueH3wMBAKlasyKBBg1i7dm2G+7Vp0wYrKyv69OlDYmIinTt3zlJdHR0dadGiBS1atMDCwoL58+ezf/9+QzBbUHTu3JnGjRvz559/smnTJr799lu+/vprVq9ebchxnDp1Kn379uWvv/5i06ZNjBgxgkmTJrFv374s5diJvCEBkihw2rdvz6BBg9i3bx/Lli3LsFyZMmXYsmULDRs2zPSqdPv27YSHh7N69Wpeeuklw/arV6/mab1zY+XKlTRr1ozff//daHtkZCRubm4mqtUjJUqUQK/Xc/XqVaMWkEuXLmVp/5UrV1K6dGlWr15t1Dr32Wef5bg+oLYiPun8+fNZPk5AQABlypRh8eLFtGjRgtOnT/Pll18aHj958iQXLlxg/vz59O7d27D98ZGW2a13VuqcnfdsRt1V6Z07vXMBnDt3Djc3N+zs7LJ0rJzw9vZm5MiRTJgwgX379lGvXr10y9nY2NCuXTv++OMPXnvttRy9/2vXrs38+fMJDg7ObbWzJLuvrbe3N2+//TZvv/02oaGh1KxZky+//NJoEIi/vz/+/v58/PHH7N27l4YNGzJr1iy++OKL/H9CApAuNlEA2dvb8/PPPzN+/HjeeOONDMt17twZnU7HxIkT0zyWkpJCZGQk8Ogq+PGr3qSkJH766ae8rXgumJmZpbkqX7FihaHrx9QCAwMB0rxmP/74Y5b2T+9vsH//foKCgnJUH29vb6pXr878+fONuh42b97MmTNnsnWsHj16cPToUT777DM0Go1Rl0569VYUhenTp+eo3q1atWLfvn0cOHDAsO3evXtpWkGz8561s7PLUvfL469Z6v8GwKlTp9i0aVO6EyjmteHDh2Nra/vUmatHjx7NZ599xieffJJhmbi4uAzfP//++y+Q9e7W3Mrqa6vT6dL8rTw8PPDx8SExMRFQR3ympKQYlfH390er1RrKiGdDWpBEgdSnT5+nlmnSpAmDBg1i0qRJHDt2jFdffRULCwsuXrzIihUrmD59Op06daJBgwYUKVKEPn36MGLECDQaDQsXLszz7prceP311/n888/p168fDRo04OTJkyxatIjSpUubumoA1KpVi44dOzJt2jTCw8MNw/wvXLgAPL0V4/XXX2f16tW0b9+e1q1bc/XqVWbNmoWfnx8xMTE5qtOkSZNo3bo1jRo1on///kRERPDjjz9SuXLlbB2zZ8+efP755/z11180bNjQaKh7xYoVKVOmDKNHj+b27ds4OjqyatWqHOejvf/++yxcuJCWLVvyzjvvGIb5lyhRghMnThjKZec9W6tWLZYtW8aoUaOoU6cO9vb2GV5YfPvtt7z22mvUr1+fAQMGGIaiOzk5PZM1+lxdXenXrx8//fQTZ8+epVKlSumWq1atGtWqVcv0WHFxcTRo0IB69erRsmVLfH19iYyMZM2aNezatYt27dqlmSH7yJEj/PHHH2mOVaZMGerXr//U+n/33XdpZl7XarWMGzcuS6/tgwcPKFasGJ06daJatWrY29uzZcsWDh48yNSpUwF1LqVhw4bx5ptvUr58eVJSUli4cCFmZmZ07NjxqXUUecgkY+eEeMzjw/wz8+Qw/1SzZ89WatWqpdjY2CgODg6Kv7+/8v777yt37twxlNmzZ49Sr149xcbGRvHx8VHef/99ZePGjQqgbNu2zVCuSZMmSuXKldOco0+fPlkaOp4qs2H+6Q3fTUhIUN577z3F29tbsbGxURo2bKgEBQUpTZo0UZo0aWIol9Ewfzs7uzTHTG/Y+ZN1ymj4curf5PFh57GxscrQoUMVFxcXxd7eXmnXrp1y/vx5BVAmT56c6euh1+uVr776SilRooRiZWWl1KhRQ1m3bl2a1zX1+X377bdpjvFk3RVFUVatWqVUqlRJsbKyUvz8/JTVq1dn+2+lKIpSp04dBVB++umnNI+dOXNGad68uWJvb6+4ubkpAwcOVI4fP57m75CVYf6KoignTpxQmjRpolhbWytFixZVJk6cqPz+++9pXu+svmdjYmKU7t27K87OzkZTHKT3XlEURdmyZYvSsGFDxcbGRnF0dFTeeOMN5cyZM0ZlsvO+SE9G70lFUZTLly8rZmZmRq8LD4f5Z+bJOiUnJyu//vqr0q5dO8P7ytbWVqlRo4by7bffGk0r8bRh/k/+jTI6d3o3MzMzQ7mnvbaJiYnKmDFjlGrVqikODg6KnZ2dUq1aNaP33ZUrV5T+/fsrZcqUUaytrRUXFxelWbNmypYtWzKto8h7GkUpQJfRQohC5dixY9SoUYM//vgj3aHqQghRWEkOkhAiS9JbWmPatGlotVqjRGIhhHgeSA6SECJLvvnmGw4fPkyzZs0wNzfn33//5d9//+Wtt97C19fX1NUTQog8JV1sQogs2bx5MxMmTODMmTPExMRQvHhxevXqxUcffZTrdayEEKKgkQBJCCGEEOIJkoMkhBBCCPEECZCEEEIIIZ4giQM5pNfruXPnDg4ODlme6l8IIYQQpqUoCg8ePMDHxwetNuN2IgmQcujOnTsyckcIIYQopG7evJnp4r8SIOWQg4MDoL7Ajo6OJq6NEEIIIbIiOjoaX19fw/d4RiRAyqHUbjVHR0cJkIQQQohC5mnpMZKkLYQQQgjxBAmQhBBCCCGeIAGSEEIIIcQTJAdJCCGeQq/Xk5SUZOpqCCGywMLCAjMzs1wfRwIkIYTIRFJSElevXkWv15u6KkKILHJ2dsbLyytX8xRKgCSEEBlQFIXg4GDMzMzw9fXNdFI5IYTpKYpCXFwcoaGhAHh7e+f4WBIgCSFEBlJSUoiLi8PHxwdbW1tTV0cIkQU2NjYAhIaG4uHhkePuNrkcEkKIDOh0OgAsLS1NXBMhRHakXtAkJyfn+BgSIAkhxFPIeotCFC558T8rAZIQQgghxBMkQBJCCPHc6Nu3L+3atTN1NcRzQAIkIYR4zvTt2xeNRsPgwYPTPDZ06FA0Gg19+/Z99hV7TNOmTdFoNBnemjZtmqPjTp8+nXnz5uWqbuPHjzfUI3UE41tvvUVERIRRuZIlS6LRaFi6dGmaY1SuXBmNRmNUl+PHj9OmTRs8PDywtramZMmSdOnSxTDi6tq1axm+Hvv27cuwvhqNhjVr1uTqOYu0JEAqYGITU7gaFkt0Qs4Ty4QQwtfXl6VLlxIfH2/YlpCQwOLFiylevLgJa6ZavXo1wcHBBAcHc+DAAQC2bNli2LZ69Wqj8llNtnVycsLZ2TnX9atcuTLBwcHcuHGDuXPnsmHDBoYMGZKmnK+vL3PnzjXatm/fPkJCQrCzszNsu3fvHq+88gouLi5s3LiRs2fPMnfuXHx8fIiNjTXa//HXIfVWq1atXD8nkT0SIBUwfeYcoNmU7ey5GGbqqgghCrGaNWvi6+trFGisXr2a4sWLU6NGDaOyer2eSZMmUapUKWxsbKhWrRorV640PK7T6RgwYIDh8QoVKjB9+nSjY6R2bU2ZMgVvb29cXV0ZOnRohoGNi4sLXl5eeHl54e7uDoCrq6thm6urKz///DNt2rTBzs6OL7/8Mlv1SNW0aVNGjBjB+++/bzjn+PHjn/r6mZub4+XlRdGiRWnevDlvvvkmmzdvTlOuR48e7Nixg5s3bxq2zZkzhx49emBu/mgmnT179hAVFcVvv/1GjRo1KFWqFM2aNeP777+nVKlSRsd8/HVIvVlYWDy1zunR6/V8/vnnFCtWDCsrK6pXr86GDRsMjyclJTFs2DC8vb2xtramRIkSTJo0CVDnFBo/fjzFixfHysoKHx8fRowYkaN6FEYSIBUwHo5WANyNTjBxTYQQT1IUhbikFJPcFEXJdn379+9v1LoxZ84c+vXrl6bcpEmTWLBgAbNmzeL06dOMHDmSnj17smPHDkD9ki1WrBgrVqzgzJkzfPrpp4wbN47ly5cbHWfbtm1cvnyZbdu2MX/+fObNm5er7q7x48fTvn17Tp48Sf/+/bNcjyfNnz8fOzs79u/fzzfffMPnn3+ebrCTkWvXrrFx48Z0p3vw9PQkMDCQ+fPnAxAXF8eyZcvo37+/UTkvLy9SUlL4888/c/S3zKnp06czdepUpkyZwokTJwgMDKRNmzZcvHgRgB9++IG1a9eyfPlyzp8/z6JFiyhZsiQAq1at4vvvv+eXX37h4sWLrFmzBn9//2dWd1OTiSILGA8HawDuPkg0cU2EEE+KT9bh9+lGk5z7zOeB2Fpm7yO7Z8+ejB07luvXrwNqK8bSpUvZvn27oUxiYiJfffUVW7ZsoX79+gCULl2a3bt388svv9CkSRMsLCyYMGGCYZ9SpUoRFBTE8uXL6dy5s2F7kSJFmDFjBmZmZlSsWJHWrVuzdetWBg4cmKPn3L179zQBXVbq8aSqVavy2WefAVCuXDlmzJjB1q1badGiRYb7nDx5Ent7e3Q6HQkJ6gXrd999l27Z/v3789577/HRRx+xcuVKypQpQ/Xq1Y3K1KtXj3HjxtG9e3cGDx5M3bp1efnll+nduzeenp5GZRs0aJBm1vaYmJgM65qZKVOm8MEHH9C1a1cAvv76a7Zt28a0adOYOXMmN27coFy5cjRq1AiNRkOJEiUM+964cQMvLy+aN2+OhYUFxYsXp27dujmqR2EkLUgFjKejGiCFRkuAJITIHXd3d1q3bs28efOYO3curVu3xs3NzajMpUuXiIuLo0WLFtjb2xtuCxYs4PLly4ZyM2fOpFatWri7u2Nvb8/s2bO5ceOG0bEqV65sNGuxt7e3IQE5J2rXrp1mW1bq8aSqVasa/Z6VelWoUIFjx45x8OBBPvjgAwIDAxk+fHi6ZVu3bk1MTAw7d+5kzpw5aVqPUn355ZeEhIQwa9YsKleuzKxZs6hYsSInT540Krds2TKOHTtmdMuJ6Oho7ty5Q8OGDY22N2zYkLNnzwJql+SxY8eoUKECI0aMYNOmTYZyb775JvHx8ZQuXZqBAwfy559/kpKSkqO6FEbSglTAeDioXWyhD6SLTYiCxsbCjDOfB5rs3DnRv39/hg0bBqjBxZNSWyb++ecfihYtavSYlZX6ebR06VJGjx7N1KlTqV+/Pg4ODnz77bfs37/fqPyTeTIajSZXi/w+nuScnXo8KSf1srS0pGzZsgBMnjyZ1q1bM2HCBCZOnJimrLm5Ob169eKzzz5j//79/Pnnnxke19XVlTfffJM333yTr776iho1ajBlyhRDFx2oid+p585vNWvW5OrVq/z7779s2bKFzp0707x5c1auXImvry/nz59ny5YtbN68mbfffptvv/2WHTt25DgnqjCRAKmASW1BkhwkIQoejUaT7W4uU2vZsiVJSUloNBoCA9MGd35+flhZWXHjxg2aNGmS7jH27NlDgwYNePvttw3bHm9delZMWY+PP/6Yl19+mSFDhuDj45Pm8f79+zNlyhS6dOlCkSJFsnRMS0tLypQpk2YUW15xdHTEx8eHPXv2GP1t9+zZY9RV5ujoSJcuXejSpQudOnWiZcuWRERE4OLigo2NDW+88QZvvPEGQ4cONbR41axZM1/qXJAUrv/0F4CnY2oLknSxCSFyz8zMzNCdkt6inQ4ODowePZqRI0ei1+tp1KgRUVFR7NmzB0dHR/r06UO5cuVYsGABGzdupFSpUixcuJCDBw+mGX2V30xZj/r161O1alW++uorZsyYkebxSpUqERYWluGixuvWrWPp0qV07dqV8uXLoygKf//9N+vXr08zTUB4eDghISFG25ydnbG2ts6wflevXk3TFVeuXDnGjBnDZ599ZsiLmjt3LseOHWPRokWAmlfl7e1NjRo10Gq1rFixAi8vL5ydnZk3bx46nY6AgABsbW35448/sLGxMcpTep5JgFTApCZpR8Ylk5CswzqHzepCCJHK0dEx08cnTpyIu7s7kyZN4sqVKzg7O1OzZk3GjRsHwKBBgzh69ChdunRBo9HQrVs33n77bf79999nUX0DU9dj5MiR9O3blw8++ABfX980j7u6uma4r5+fH7a2trz33nvcvHkTKysrypUrx2+//UavXr2MyjZv3jzN/kuWLDEkWqdn1KhRabbt2rWLESNGEBUVxXvvvUdoaCh+fn6sXbuWcuXKAWqA/M0333Dx4kXMzMyoU6cO69evR6vV4uzszOTJkxk1ahQ6nQ5/f3/+/vvvTJ/n80SjPMvxhs+R6OhonJyciIqKeuqHT3YoikLFTzaQmKJn1/vN8HVJ/2pECJH/EhISuHr1KqVKlcr06l0IUbBk9r+b1e9vGcVWwGg0mkcj2SRRWwghhDAJCZAKIE/DZJGShySEEEKYggRIBZBhskgZySaEEEKYhARIBZCHtCAJIYQQJiUBUgEkOUhCCCGEaUmAVAAZZtOWFiQhhBDCJCRAKoBkNm0hhBDCtCRAKoBkNm0hhBDCtCRAKoDcH45ii4pXZ9MWQgghxLMlAVIB5GhtjrWF+qeRPCQhhMjY+PHjqV69uuH3vn370q5du0z3adq0Ke+++26uz51XxxEFkwRIBdDjs2nflZFsQohs6tu3LxqNhsGDB6d5bOjQoWg0Gvr27fvsK/aYqVOnUqRIERIS0n7GxcXF4ejoyA8//JDt406fPp158+blQQ0f2b59OxqNhsjISKPtq1evZuLEiXl6riddu3YNjUZjuLm4uNCkSRN27dplVG78+PFoNBpatmyZ5hjffvstGo2Gpk2bGrbFxcUxduxYypQpg7W1Ne7u7jRp0oS//vrLUKZp06ZG5069pfe+SpWVALWwkACpgPJ82M0mLUhCiJzw9fVl6dKlxMfHG7YlJCSwePFiihcvbsKaqXr16kVsbCyrV69O89jKlStJSkqiZ8+e2T6uk5MTzs7OeVDDp3NxccHBweGZnGvLli0EBwezc+dOfHx8eP3117l7965RGW9vb7Zt28atW7eMts+ZMyfN33zw4MGsXr2aH3/8kXPnzrFhwwY6depEeHi4UbmBAwcSHBxsdPvmm2/y50kWMBIgFVDuhskipQVJCJF9NWvWxNfX1ygAWb16NcWLF6dGjRpGZfV6PZMmTaJUqVLY2NhQrVo1Vq5caXhcp9MxYMAAw+MVKlRg+vTpRsdIbTmYMmUK3t7euLq6MnToUJKTk9Otn4eHB2+88QZz5sxJ89icOXNo164dLi4ufPDBB5QvXx5bW1tKly7NJ598kuExH69HqtjYWHr37o29vT3e3t5MnTo1zT4LFy6kdu3aODg44OXlRffu3QkNDQXUFpxmzZoBUKRIEaPWtye72O7fv0/v3r0pUqQItra2vPbaa1y8eNHw+Lx583B2dmbjxo1UqlQJe3t7WrZsSXBwcIbPJ5WrqyteXl5UqVKFcePGER0dzf79+9O8pq+++irz5883bNu7dy9hYWG0bt3aqOzatWsZN24crVq1omTJktSqVYvhw4fTv39/o3K2trZ4eXkZ3XKzQPuOHTuoW7cuVlZWeHt78+GHH5KSkmJ4fOXKlfj7+2NjY4OrqyvNmzcnNjYWUFvy6tati52dHc7OzjRs2JDr16/nuC5PIwFSAZXagiRdbEIUIIoCSbGmuSlKtqvbv39/5s6da/h9zpw59OvXL025SZMmsWDBAmbNmsXp06cZOXIkPXv2ZMeOHYAaQBUrVowVK1Zw5swZPv30U8aNG8fy5cuNjrNt2zYuX77Mtm3bmD9/PvPmzcu0u2vAgAH8999/Rl9yV65cYefOnQwYMAAABwcH5s2bx5kzZ5g+fTq//vor33//fZZfgzFjxrBjxw7++usvNm3axPbt2zly5IhRmeTkZCZOnMjx48dZs2YN165dMwRBvr6+rFq1CoDz588THBycJjhM1bdvXw4dOsTatWsJCgpCURRatWplFNDFxcUxZcoUFi5cyM6dO7lx4wajR4/O8vOJj49nwYIFAFhaWqZ5vH///kav+Zw5c+jRo0easl5eXqxfv54HDx5k+dy5dfv2bVq1akWdOnU4fvw4P//8M7///jtffPEFAMHBwXTr1o3+/ftz9uxZtm/fTocOHVAUhZSUFNq1a0eTJk04ceIEQUFBvPXWW2g0mnyrr3m+HVnkSupQ/3vSxSZEwZEcB1/5mObc4+6ApV22dunZsydjx441BCB79uxh6dKlbN++3VAmMTGRr776ii1btlC/fn0ASpcuze7du/nll19o0qQJFhYWTJgwwbBPqVKlCAoKYvny5XTu3NmwvUiRIsyYMQMzMzMqVqxI69at2bp1KwMHDky3foGBgfj4+DB37lzGjx8PqK0svr6+vPLKKwB8/PHHhvIlS5Zk9OjRLF26lPfff/+pzz8mJobff/+dP/74w3C8+fPnU6xYMaNyj7ealC5dmh9++IE6deoQExODvb09Li4ugNpCk1H33cWLF1m7di179uyhQYMGACxatAhfX1/WrFnDm2++CajB2KxZsyhTpgwAw4YN4/PPP3/qc2nQoAFarZa4uDgURaFWrVqG5/S4119/ncGDB7Nz505q1arF8uXL2b17d5qWutmzZ9OjRw9cXV2pVq0ajRo1olOnTjRs2NCo3E8//cRvv/1mtO2XX36hR48eT63zk3766Sd8fX2ZMWMGGo2GihUrcufOHT744AM+/fRTgoODSUlJoUOHDpQoUQIAf39/ACIiIoiKiuL11183vHaVKlXKdh2yQwKkAsqwHpu0IAkhcsjd3Z3WrVszb948FEWhdevWuLm5GZW5dOkScXFxtGjRwmh7UlKSUVfczJkzmTNnDjdu3CA+Pp6kpCSj0WMAlStXxszMzPC7t7c3J0+ezLB+ZmZm9OnTh3nz5vHZZ5+hKArz58+nX79+aLVqB8eyZcv44YcfuHz5MjExMaSkpGS5i+fy5cskJSUREBBg2Obi4kKFChWMyh0+fJjx48dz/Phx7t+/j16vB+DGjRv4+fll6Vxnz57F3Nzc6Fyurq5UqFCBs2fPGrbZ2toavuBBfY1Su/Mys2zZMipWrMipU6d4//33mTdvHhYWFmnKWVhY0LNnT+bOncuVK1coX748VatWTVPupZde4sqVK+zbt4+9e/eydetWpk+fzoQJE/jkk08M5Xr06MFHH31ktK+np+dT65ues2fPUr9+faNWn4YNGxITE8OtW7eoVq0ar7zyCv7+/gQGBvLqq6/SqVMnihQpgouLC3379iUwMJAWLVrQvHlzOnfujLe3d47qkhUSIBVQhi42aUESouCwsFVbckx17hzo378/w4YNA9Qg50kxMTEA/PPPPxQtWtToMSsr9UJt6dKljB49mqlTp1K/fn0cHBz49ttv0+TAPPmFrdFoDMFGZvWbNGkS//33H3q9nps3bxq6AYOCgujRowcTJkwgMDAQJycnli5dmm4eUU7FxsYSGBhIYGAgixYtwt3dnRs3bhAYGEhSUlKenSdVeq+RkoXuU19fX8qVK0e5cuVISUmhffv2nDp1yvA3elz//v0JCAjg1KlTaXKKnqxL48aNady4MR988AFffPEFn3/+OR988IGhS87JyYmyZctm81nmjJmZGZs3b2bv3r1s2rSJH3/8kY8++oj9+/dTqlQp5s6dy4gRI9iwYQPLli3j448/ZvPmzdSrVy9f6iM5SAWUR+qCtZKkLUTBodGo3VymuOUw16Jly5YkJSWRnJxMYGBgmsf9/PywsrLixo0blC1b1ujm6+sLYOg2evvtt6lRowZly5bl8uXLuXopU5UpU4YmTZowZ84c5s6dS/PmzQ3dK3v37qVEiRJ89NFH1K5dm3LlymUrKbdMmTJYWFgYBXL379/nwoULht/PnTtHeHg4kydPpnHjxlSsWDFNi05qsKDTZTxxb6VKlUhJSTE6V3h4OOfPn89yK1RWderUCXNzc3766ad0H69cuTKVK1fm1KlTdO/ePcvH9fPzIyUlJd2pF/JCpUqVDLlZqfbs2YODg4Oh21Oj0dCwYUMmTJjA0aNHsbS05M8//zSUr1GjBmPHjmXv3r1UqVKFxYsX50tdQVqQCqzULrbohBTik3TYWJo9ZQ8hhEjLzMzM0MXzePdXKgcHB0aPHs3IkSPR6/U0atSIqKgo9uzZg6OjI3369KFcuXIsWLCAjRs3UqpUKRYuXMjBgwcpVapUntRxwIABhjylxxOMy5Urx40bN1i6dCl16tThn3/+MfqyfBp7e3sGDBjAmDFjcHV1xcPDg48++sjQfQdQvHhxLC0t+fHHHxk8eDCnTp1KM7dRiRIl0Gg0rFu3jlatWmFjY4O9vb1RmXLlytG2bVsGDhzIL7/8goODAx9++CFFixalbdu2OXhVMqbRaBgxYgTjx49n0KBB2NqmbV3877//SE5OzjBnqmnTpnTr1o3atWvj6urKmTNnGDduHM2aNTPqwoyLiyMkJMRoXysrK4oUKZJh/aKiojh27JjRNldXV95++22mTZvG8OHDGTZsGOfPn+ezzz5j1KhRaLVa9u/fz9atW3n11Vfx8PBg//793Lt3j0qVKnH16lVmz55NmzZt8PHx4fz581y8eJHevXtn/YXLJpO3IM2cOZOSJUtibW1NQEAABw4cyLT8ihUrqFixItbW1vj7+7N+/fo0Zc6ePUubNm1wcnLCzs6OOnXqcOPGDcPjCQkJDB06FFdXV+zt7enYsWOa+SRMzcHKHBsL9cMsVPKQhBC54OjomGnezsSJE/nkk0+YNGkSlSpVomXLlvzzzz+GAGjQoEF06NCBLl26EBAQQHh4OG+//Xae1a9jx45YWVlha2trNES/TZs2jBw5kmHDhlG9enX27t1rlB+TFd9++y2NGzfmjTfeoHnz5jRq1IhatWoZHnd3d2fevHmsWLECPz8/Jk+ezJQpU4yOUbRoUSZMmMCHH36Ip6enocvySXPnzqVWrVq8/vrr1K9fH0VRWL9+fbq5QrnVp08fkpOTmTFjRrqPpw6Fz0hgYCDz58/n1VdfpVKlSgwfPpzAwMA0IxN//fVXvL29jW7dunXLtG7bt2+nRo0aRrcJEyZQtGhR1q9fz4EDB6hWrRqDBw9mwIABhkR8R0dHdu7cSatWrShfvjwff/wxU6dO5bXXXsPW1pZz587RsWNHypcvz1tvvcXQoUMZNGhQ9l64bNAoWen8zCfLli2jd+/ezJo1i4CAAKZNm8aKFSs4f/48Hh4eacrv3buXl156iUmTJvH666+zePFivv76a44cOUKVKlUANSmvbt26DBgwgG7duuHo6Mjp06epV6+e4ZhDhgzhn3/+Yd68eTg5OTFs2DC0Wi179uzJct2jo6NxcnIiKioqV3NCZKbpt9u4Fh7H8kH1qVvKJV/OIYTIWEJCAlevXqVUqVJYW1ubujpCiCzK7H83q9/fJg2QAgICqFOnjiEC1uv1+Pr6Mnz4cD788MM05bt06UJsbCzr1q0zbKtXrx7Vq1dn1qxZAHTt2hULCwsWLlyY7jmjoqJwd3dn8eLFdOrUCVD7oFP7RrOa7PUsAqTOvwRx4GoEM7rX4PWqJhpaLMQLTAIkIQqnvAiQTNbFlpSUxOHDh2nevPmjymi1NG/enKCgoHT3CQoKMioPajNhanm9Xs8///xD+fLlCQwMxMPDg4CAANasWWMof/jwYZKTk42OU7FiRYoXL57heUGdKyQ6Otrolt88HFJn05aRbEIIIcSzZLIAKSwsDJ1Ol2Y+BU9PzzQJYalCQkIyLR8aGkpMTAyTJ0+mZcuWbNq0ifbt29OhQwfDjLAhISFYWlqm6ZvN7LygzjTr5ORkuKWO7shPnjKSTQghhDAJkydp56XU+Tbatm3LyJEjqV69Oh9++CGvv/66oQsup8aOHUtUVJThdvPmzbyocqZSZ9MOfSAtSEIIIcSzZLJh/m5ubpiZmaUZPXb37l28vLzS3cfLyyvT8m5ubpibm6eZc6JSpUrs3r3bcIykpCQiIyONWpEyOy+owxrTm5ArP3kYJouUFiQhTMmEqZpCiBzIi/9Zk7UgWVpaUqtWLbZu3WrYptfr2bp1q2E9oCfVr1/fqDzA5s2bDeUtLS2pU6cO58+fNypz4cIFw8RjtWrVwsLCwug458+f58aNGxme11QMy41IgCSESaTOG5QfMyoLIfJPXFwckHbm8uww6USRo0aNok+fPtSuXZu6desybdo0YmNjDdPM9+7dm6JFizJp0iQA3nnnHZo0acLUqVNp3bo1S5cu5dChQ8yePdtwzDFjxtClSxdeeuklmjVrxoYNG/j7778NizM6OTkxYMAARo0ahYuLC46OjgwfPpz69evn23TlOfUoB0m62IQwBXNzc2xtbbl37x4WFhZGEwwKIQoeRVGIi4sjNDQUZ2fndCdHzSqTBkhdunTh3r17fPrpp4SEhFC9enU2bNhgSMS+ceOG0QdSgwYNWLx4MR9//DHjxo2jXLlyrFmzxjAHEkD79u2ZNWsWkyZNYsSIEVSoUIFVq1bRqFEjQ5nvv/8erVZLx44dSUxMJDAwMMMp200pdRTbg8QU4pJSsLWUic+FeJY0Gg3e3t5cvXo1W0tcCCFMy9nZOdO0maww6TxIhdmzmAdJURQqf7aRuCQd20c3paSbXb6cRwiROb1eL91sQhQSFhYWmbYcZfX7W5okCjCNRoOnozVXw2K5G50gAZIQJqLVamWiSCFeMNKhXsCldrPJUH8hhBDi2ZEAqYDzcJSh/kIIIcSzJgFSAecpLUhCCCHEMycBUgEny40IIYQQz54ESAXco8kipQVJCCGEeFYkQCrgDMuNPJAWJCGEEOJZkQCpgDMsWCstSEIIIcQzIwFSAZc6ii0mMYXYxBQT10YIIYR4MUiAVMDZW5ljZ6nOCCoj2YQQQohnQwKkQsBT5kISQgghnikJkAqB1JFs0oIkhBBCPBsSIBUCqSPZZC4kIYQQ4tmQAKkQ8DTMhSQBkhBCCPEsSIBUCDzKQZIuNiGEEOJZkACpEHA3rMcmLUhCCCHEsyABUiHwaD02aUESQgghngUJkAoBGeYvhBBCPFsSIBUCHg+72GKTdMTIbNpCCCFEvpMAqRCwszLH3sockKH+QgghxLMgAVIh4WEY6i95SEIIIUR+kwCpkPBMnSxSRrIJIYQQ+U4CpELCsNyItCAJIYQQ+U4CpEJCRrIJIYQQz44ESIVE6ki2u7JgrRBCCJHvJEAqJDwcZcFaIYQQ4lmRAKmQ8DQsNyItSEIIIUR+kwCpkJAcJCGEEOLZkQCpkEgdxRYns2kLIYQQ+U4CpELC1tIch4ezaUsrkhBCCJG/JEAqRB7Npi0BkhBCCJGfJEAqRDwNI9kkUVsIIYTITxIgFSIehpFs0oIkhBBC5CcJkAqRRyPZpAVJCCGEyE8SIBUiHjLUXwghhHgmJEAqRDxkskghhBDimZAAqRDxlOVGhBBCiGdCAqRCxPPhMP+Q6AQURTFxbYQQQojnlwRIhYi3kw02FmYkJOs5F/LA1NURQgghnlsSIBUiluZa6pdxBWDb+VAT10YIIYR4fkmAVMg0q+AOwPbz90xcEyGEEOL5JQFSIdO0ggcAh6/fJyo+2cS1EUIIIZ5PEiAVMr4utpRxt0OnV9hzKczU1RFCCCGeSxIgFULNHrYibTsneUhCCCFEfpAAqRBK7WbbfuEeer0M9xdCCCHymgRIhVCdUkWwtTTj3oNEzgRHm7o6QgghxHNHAqRCyMrcjAZl3ADYLsP9hRBCiDwnAVIh1ayiDPcXQggh8osESIVUah7SkRv3iYxLMnFthBBCiOeLBEiFVFFnG8p72qNXYOdFGe4vhBBC5CUJkAqx1OH+kockhBBC5K0CESDNnDmTkiVLYm1tTUBAAAcOHMi0/IoVK6hYsSLW1tb4+/uzfv16o8f79u2LRqMxurVs2dKoTMmSJdOUmTx5cp4/t/zU5OGyIzvOy3B/IYQQIi+ZPEBatmwZo0aN4rPPPuPIkSNUq1aNwMBAQkPTbxXZu3cv3bp1Y8CAARw9epR27drRrl07Tp06ZVSuZcuWBAcHG25LlixJc6zPP//cqMzw4cPz5Tnml9olXLC3Mic8NomTt6NMXR0hhBDiuWHyAOm7775j4MCB9OvXDz8/P2bNmoWtrS1z5sxJt/z06dNp2bIlY8aMoVKlSkycOJGaNWsyY8YMo3JWVlZ4eXkZbkWKFElzLAcHB6MydnZ2+fIc84uluZaGZV0BGc0mhBBC5CWTBkhJSUkcPnyY5s2bG7ZptVqaN29OUFBQuvsEBQUZlQcIDAxMU3779u14eHhQoUIFhgwZQnh4eJpjTZ48GVdXV2rUqMG3335LSkpKHjyrZ8uw7IjkIQkhhBB5xtyUJw8LC0On0+Hp6Wm03dPTk3PnzqW7T0hISLrlQ0JCDL+3bNmSDh06UKpUKS5fvsy4ceN47bXXCAoKwszMDIARI0ZQs2ZNXFxc2Lt3L2PHjiU4OJjvvvsu3fMmJiaSmJho+D06umDMYJ2ah3T8ViQRsUm42FmauEZCCCFE4WfSACm/dO3a1XDf39+fqlWrUqZMGbZv384rr7wCwKhRowxlqlatiqWlJYMGDWLSpElYWVmlOeakSZOYMGFC/lc+m7ydbKjo5cC5kAfsvHCPdjWKmrpKQgghRKFn0i42Nzc3zMzMuHv3rtH2u3fv4uXlle4+Xl5e2SoPULp0adzc3Lh06VKGZQICAkhJSeHatWvpPj527FiioqIMt5s3b2Z4rGetWUXpZhNCCCHykkkDJEtLS2rVqsXWrVsN2/R6PVu3bqV+/frp7lO/fn2j8gCbN2/OsDzArVu3CA8Px9vbO8Myx44dQ6vV4uHhke7jVlZWODo6Gt0Kiqbl1W62nRfuoZPh/kIIIUSumbyLbdSoUfTp04fatWtTt25dpk2bRmxsLP369QOgd+/eFC1alEmTJgHwzjvv0KRJE6ZOnUrr1q1ZunQphw4dYvbs2QDExMQwYcIEOnbsiJeXF5cvX+b999+nbNmyBAYGAmqi9/79+2nWrBkODg4EBQUxcuRIevbsme5ot4KuZokiOFibcz8umeO3IqlZvPA9ByGEEKIgMXmA1KVLF+7du8enn35KSEgI1atXZ8OGDYZE7Bs3bqDVPmroatCgAYsXL+bjjz9m3LhxlCtXjjVr1lClShUAzMzMOHHiBPPnzycyMhIfHx9effVVJk6caMgtsrKyYunSpYwfP57ExERKlSrFyJEjjfKSChMLMy2Ny7mx/mQI28/fkwBJCCGEyCWNoijSJ5MD0dHRODk5ERUVVSC625Yfusn7K09QtZgTa4c1MnV1hBBCiAIpq9/fJp8oUuSN1DykE7eiuPcg8SmlhRBCCJEZCZCeEx6O1lT2USPhnRdkVm0hhBAiNyRAeo6kzqr994k7Jq6JEEIIUbhJgPQc6VSrGFqNui7bKVm8VgghhMgxCZCeIyXd7Hijmg8AM7dlPCmmEEIIITInAdJzZmizsgD8eyqEC3cfmLg2QgghROEkAdJzprynAy0rq8uu/CStSEIIIUSOSID0HBr2stqKtPb4Ha6FxZq4NkIIIUThIwHSc6hKUSeaVXBHr8CsHZdNXR0hhBCi0JEA6Tk17OVyAKw6covbkfEmro0QQghRuEiA9JyqVaIIDcq4kqxTmC2tSEIIIUS2SID0HBv2cETbkoM3CX2QYOLaCCGEEIWHBEjPsfplXKlZ3JmkFD2/7bpq6uoIIYQQhYYESM8xjUbD8Ie5SH/su05EbJKJaySEEEIUDhIgPeeaVnCnso8jcUk65u6RViQhhBAiKyRAes6prUhqLtK8PdeIik82cY2EEEKIgk8CpBfAq35elPOw50FiCguDrpm6OkIIIUSBJwHSC0Cr1Rhm1/5991ViE1NMXCMhhBCiYJMA6QXR2t+bEq623I9LZsOpEFNXRwghhCjQJEB6QZibaWlTzQeALWfvmrg2QgghRMEmAdILpIWfJwA7LtwjIVln4toIIYQQBZcESC+QKj5OeDpaEZekY9+VcFNXRwghhCiwJEB6gWi1Gl6ppLYiSTebEEIIkTEJkF4wqd1sW86EoiiKiWsjhBBCFEwSIL1g6pd2xdbSjJDoBE7djjZ1dYQQQogCSQKkF4y1hRkvlXMHYLN0swkhhBDpkgDpBZTazbb5jARIQgghRHokQHoBNavogVYDZ4OjuXU/ztTVEUIIIQocCZBeQC52ltQu4QLA1rOhJq6NEEIIUfBIgPSCMoxmkzwkIYQQIg0JkF5QzR8GSPuuhBOdkGzi2gghhBAFiwRIL6hSbnaUcbcjWaew4/w9U1dHCCGEKFAkQHqBtfDzAqSbTQghhHiSBEgvsBZ+HgBsOxdKsk5v4toIIYQQBYcESC+w6r5FcLWzJDohhYNXI0xdHSGEEKLAkADpBWam1fBKJbUVSWbVFkIIIR6RAOkF17zSo+H+snitEEIIoZIA6QXXqJwbVuZabkbEc/7uA1NXRwghhCgQJEB6wdlamtO4nBsAW2RtNiGEEAKQAEnwqJttsyw7IoQQQgASIAng5YeJ2sdvRnI3OsHEtRFCCCFMTwIkgYeDNdV9nQFZvFYIIYQACZDEQ6mL124+E2LimgghhBCmJwGSACCwshogbTt/j//OSbK2EEKIF5sESAKAsh4O9KlfAoB3lx7jZkSciWskhBBCmI4ESMLgo9Z+VPd1JjohhcF/HCYhWWfqKgkhhBAmIQGSMLA01/JTj5q42Fly+k4049eeNnWVhBBCCJOQAEkY8XG24YeuNdBoYOnBmyw/eNPUVRJCCCGeOQmQRBqNyrnxXovyAHzy1ylO3Y4ycY2EEEKIZ0sCJJGut5uW5ZWKHiSm6Bmy6DBRcckZlr0UGsPkf88xcd0ZklL0z7CWQgghRP4wN3UFRMGk1Wr4rnN1Xp+xi5sR8Yxafoxfe9dGq9UAEJuYwj8ng1l+8CaHrt837FelqCPtaxQzVbWFEEKIPCEBksiQk60FP/eoRYef97L1XCg/bb9Eg7JuLD94k7+P3yE2SR3lptVAcRdbroXHsfzgLQmQhBBCFHoFoott5syZlCxZEmtrawICAjhw4ECm5VesWEHFihWxtrbG39+f9evXGz3et29fNBqN0a1ly5ZGZSIiIujRoweOjo44OzszYMAAYmJi8vy5FXZVijrxRdsqAEzZdIEOP+1l6cGbxCbpKOlqy5jACgSNfYVFA+uh0UDQlXBuhMscSkIIIQo3kwdIy5YtY9SoUXz22WccOXKEatWqERgYSGho+muC7d27l27dujFgwACOHj1Ku3btaNeuHadOnTIq17JlS4KDgw23JUuWGD3eo0cPTp8+zebNm1m3bh07d+7krbfeyrfnWZh1ruNL1zq+AFhbaOlQsyjL3qrHttFNGdqsLJ6O1hR1tqFRWTcAVh6WkW9CCCEKN42iKIopKxAQEECdOnWYMWMGAHq9Hl9fX4YPH86HH36YpnyXLl2IjY1l3bp1hm316tWjevXqzJo1C1BbkCIjI1mzZk265zx79ix+fn4cPHiQ2rVrA7BhwwZatWrFrVu38PHxeWq9o6OjcXJyIioqCkdHx+w+7UJHp1c4dC2CSj6OOFpbpFvm7+N3GL7kKD5O1uz64GXMHuYrCSGEEAVFVr+/TdqClJSUxOHDh2nevLlhm1arpXnz5gQFBaW7T1BQkFF5gMDAwDTlt2/fjoeHBxUqVGDIkCGEh4cbHcPZ2dkQHAE0b94crVbL/v370z1vYmIi0dHRRrcXiZlWQ0Bp1wyDI1AXvHWyseBOVAJ7LoU9w9oJIQqUmFB4IAtfi8ItRwHSzZs3uXXrluH3AwcO8O677zJ79uxsHScsLAydToenp6fRdk9PT0JC0v/nCgkJeWr5li1bsmDBArZu3crXX3/Njh07eO2119DpdIZjeHh4GB3D3NwcFxeXDM87adIknJycDDdfX99sPdcXgbWFGe2qq61vyw9JN5sQLyRdMsxuBrMaQaLkdYrCK0cBUvfu3dm2bRugBhstWrTgwIEDfPTRR3z++ed5WsGc6Nq1K23atMHf35927dqxbt06Dh48yPbt23N8zLFjxxIVFWW43bwpAUB63qytBo6bTt8lMi7JxLURQjxzoWch+hbE3oM7R0xdGyFyLEcB0qlTp6hbty4Ay5cvp0qVKuzdu5dFixYxb968LB/Hzc0NMzMz7t69a7T97t27eHl5pbuPl5dXtsoDlC5dGjc3Ny5dumQ4xpNJ4CkpKURERGR4HCsrKxwdHY1uIq0qRZ3w83YkSafnr2N3TF0dIcSzFnz80f1bB01XDyFyKUcBUnJyMlZWVgBs2bKFNm3aAFCxYkWCg4OzfBxLS0tq1arF1q1bDdv0ej1bt26lfv366e5Tv359o/IAmzdvzrA8wK1btwgPD8fb29twjMjISA4fPmwo899//6HX6wkICMhy/UX6OtdW50GSbjYhXkDBxx7dv3XIZNUQIrdyFCBVrlyZWbNmsWvXLjZv3myYY+jOnTu4urpm61ijRo3i119/Zf78+Zw9e5YhQ4YQGxtLv379AOjduzdjx441lH/nnXfYsGEDU6dO5dy5c4wfP55Dhw4xbNgwAGJiYhgzZgz79u3j2rVrbN26lbZt21K2bFkCAwMBqFSpEi1btmTgwIEcOHCAPXv2MGzYMLp27ZqlEWwic22rF8XSTMvpO9GyjpsQL5o7xx7dv3UQTDtQWogcy1GA9PXXX/PLL7/QtGlTunXrRrVq1QBYu3atoestq7p06cKUKVP49NNPqV69OseOHWPDhg2GROwbN24YtUo1aNCAxYsXM3v2bKpVq8bKlStZs2YNVaqokxmamZlx4sQJ2rRpQ/ny5RkwYAC1atVi165dhlYvgEWLFlGxYkVeeeUVWrVqRaNGjbKdZC7SV8TOkhaV1b/fysO3nlJaCPHc0KXA3cfmpIu9B5HXTVcfIXIhx/Mg6XQ6oqOjKVKkiGHbtWvXsLW1TTNC7Hn0os2DlF07Ltyjz5wDONlYsH/cK1hbmJm6SkKI/BZyCmY1BCtHcC0Dd45Cx9/Bv5OpayaEQb7OgxQfH09iYqIhOLp+/TrTpk3j/PnzL0RwJJ6uUVk3vJ2siYpPZsvZu0/fQQhR+KUmaHtXA9+H+ZySqC0KqRwFSG3btmXBggUAREZGEhAQwNSpU2nXrh0///xznlZQFE5mWg2daqUma0s3mxAvhNQEbe9qUKyOev9m5mtrClFQ5ShAOnLkCI0bNwZg5cqVeHp6cv36dRYsWMAPP/yQpxUUhVdqgLTr4j3uRMabuDZCiHyXmqDtXR2KPVypIOQEJMv/vyh8chQgxcXF4eDgAMCmTZvo0KEDWq2WevXqcf26JOQJVQlXO+qVdkFRYFUGydqh0QnM3HaJ/vMOcuJW5LOtoBAi7+hSIOSket+nOjiXADt30KdA8AmTVk2InMhRgFS2bFnWrFnDzZs32bhxI6+++ioAoaGhkrAsjHR+OLP2isO30OvV8QA6vcK2c6G8teAQ9Sf/x7cbz/PfuVB6zznAxbsPTFldIUROhV2AlHiwtAeXMqDRPOpmkzwkUQjlKED69NNPGT16NCVLlqRu3bqGSRo3bdpEjRo18rSC4gl3T0PYRVPXIsteq+KNvZU5NyLi+Ov4baZtuUDjr/+j37yDbDpzF51eoWZxZ/y8HYmMS6bX7we4dT/O1NUWQmRXaoK2V1XQPvxqkQBJFGLmOdmpU6dONGrUiODgYMMcSACvvPIK7du3z7PKiSdc3AyLu4C1E7x3HswtTV2jp7KxNOONat4sOXCTkcseLUHgbGtB+xpF6Va3OOU9Hbgfm8SbvwRxKTSG3r8fYMXg+rjaW2VyZCFEgZKaoO1T/dE2Q4AkM2qLwidHLUigrmdWo0YN7ty5w61ban5J3bp1qVixYp5VTjzmzjFY3gcUHcRHqM3ZhUT3uiXQatT79Uq7ML1rdfaNfYXP3qhMeU81l62InSULB9SlqLMNV8Ji6Tv3IDGJKSastRAiWx5P0E7lUwM0WnXx2mhZm1EULjkKkPR6PZ9//jlOTk6UKFGCEiVK4OzszMSJE9Hr9XldRxF5AxZ3huTYR9tCCk/So38xJ9a/05gdY5qy9K36tK1eNN2JI72dbFgwoC4udpacvB3FWwsOkZCsM0GNhRDZotc9+kx6vAXJyh48Kqv3pRVJFDI5CpA++ugjZsyYweTJkzl69ChHjx7lq6++4scff+STTz7J6zq+2OLvwx+dIOau+kFTvYe6PXW0SCFR0cuREq52Ty1Xxt2eef3qYGdpxt7L4by79Bg6vazlJESBFn4JkuPAwg5cyxo/ljrc/5bMhyQKlxwFSPPnz+e3335jyJAhVK1alapVq/L222/z66+/Mm/evDyu4gssJRGW9YKw8+DgAz1WQImG6mPP8bDZqsWc+bV3bSzNtGw4HcLHa06SwxVxhBDPQmr3mpc/aJ9oHZY8JFFI5ShAioiISDfXqGLFikREROS6UgJ1Bey/hsK1XWDpAD2Wg1NR8K6qPh5y8rleJbtBWTd+6FYdrQaWHLjJtxvPm7pKQoiMpJegncr34QLmd46CLvlZ1UiIXMtRgFStWjVmzJiRZvuMGTOoWrVqrislgP8mwskVoDWHzvPVKzMAtwqgtYDEqOd+leyWVbz5sr36vH/afpnjNyNNWyEhRPrSS9BO5VIGrJ0hJQHunnqGlRIid3IUIH3zzTfMmTMHPz8/BgwYwIABA/Dz82PevHlMmTIlr+v44jk0F3ZNVe+/MR3KvvLoMXNL8Kik3i9IeUj/vAcz6kJc3rYgdqtbnPY1igLw++6reXpsIUQe0OvTT9BOpdU+lock3Wyi8MhRgNSkSRMuXLhA+/btiYyMJDIykg4dOnD69GkWLlyY13V8sVzYpAYbAE0+hBo905bxeqybrSBIjIHD89RcqfPr8/zwAxqVAmD9yWCCo2RNJyEKlIjLkBQD5jbgWi79MjJhpCiEcjwPko+PD19++SWrVq1i1apVfPHFF9y/f5/ff/89L+v3YklJgn9GqXMdVesOTT9Mv1xqHlJBSdS+sU9dbwng8rY8P3yVok7UK+1Cil5h/t7nu1tRiELn8QRtswzmHja0IEmAJAqPHAdIIh+YW0KvNVCzt9q1ptGkXy41H6mgtCBd3fHo/pXtapN7HhvQqDQAi/dfJ1YmkBSi4MgsQTtV0Vrqz4grEBuW3zUSIk9IgFTQuJWFNj9mvoyIZxX1Z/StPM/5yZGrOx/djwuD0DN5fopXKnpQ0tWW6IQUVh25lefHF0LkUGYJ2qlsioBbefW+5CGJQkICpMLI2hGKqHk5Jp9ROy7CeJFKUFuR8phWq6FfQ/U5z91zDb1MHimE6en1j/7/M2tBAij2cLi/dLOJQiJbi9V26NAh08cjIyNzUxeRHd5V4f5VNQ+pdFPT1eP6HkBRrw6rdVUDtivbocGwPD9Vp1rFmLrpPFfDYvnvXCjN/Tzz/BxCiGy4fxWSHoC5tToFSWaK1YZjf0iAJAqNbLUgOTk5ZXorUaIEvXv3zq+6iscVlDyk1O61Ui89CtSu71FnAc9jdlbmdAsoDsiQf5GOsIsyEeGzdueo+tOzSsYJ2qlSR7LdPqKu3SZEAZetFqS5c+fmVz1EdnlVU3+auovt8QDJww/s3CH2nnqVWLJRnp+uT/2S/LbrKkFXwjl9J4rKPk55fg5RCJ35C5b3hhKNoNefmefwibyTlQTtVB6V1LXakh7AvfPg6ZefNRMi1yQHqbBKbUEKuwDJJpob6MFduHcO0EDJxuqou9RWpHzIQwLwcbahlb83IK1I4iFdMmz+TL1/fTesG/lcL8NToGQlQTuV1gyK1lTvSzebKAQkQCqsHLzU1hpFD3fzftRYllzbpf708gdbF/V+PgdI8GjiyL+P3yE0OiHfziMKiaN/qLkwVk6g0ap5LkFpl0ISeUxRHs3F5l0ta/sYJow8kD91EiIPSYBUWGk0j+UhmaibLXX+o1IvPdqWGiDdPgwJUfly2uq+ztQuUYRkncLCfTJx5AstOR52fKPebzYOAr9S72/6BM5vMF29XgT3r6prQppZPVr+6GlSF6590Yb6X9kBP9SA9WNMXRORDRIgFWaGJUdMFSCl5h81ebTNqZi63ICih6u78u3Uqa1If+y7TkKyJHy+sA7+Dg/ugGMxqN0PAgZDrb6AAqsGmK519UWQ2r3mWRnMLLK2T9GHM2rfOwfxkflRq4Ln0Fz4o4M6SeaB2RB6ztQ1ElkkAVJhZsqRbPevw/1roDGDEvWNH3sG3WyvVvaiWBEb7scls/rI7Xw7jyjAEqIfLerc9EMwt1JbVltNUXPikmJgSReIuWfaej6vspOgncreHYqUVO/fOZLHFSpg9Dr490NY9666FJP1wwEl+dH9Gx8peXf5QAKkwiy1Benu6Wc/bDY1/6hoLbByMH7sGQRIZloNfRuUBGDOnqsoBfHDQZciH1r5ad9PEB8BrmWhWrdH280soPMCcCkNkTdgWc98mXbiuXb/OizsABvGZhxgGhK0s5h/lCo1D+nmc5yonRANS7rC/p/V35t9DN2Xq/dPLIOY0Lw717El8G1ZmP+G6QbsPKckQCrMXMuAhS0kx0H45Wd77seH9z+pZCM1WTb8IkTl37IgXer4Ym9lzqXQGHZcKGCtBLHhMKM2zG6SL2vTvfBiw2HvwyvxZh+lnYPH1gW6LVMTt2/ug7/fyThYVRR1RKZ8uajiIuCPjnB5qxqE/lAdtk2CxAePyijKoxm0szKC7XG+AerPvT/A8WV5UeOC5f51mBMIFzeBuQ28OQ+ajIHi9dTgUJcEB37Nm3OdWAFrhoA+Wb1oXfU/mWMqD0mAVJhpzdT+f3i2eUiKknmAZOMMPg+H817ZkfbxPOJgbUGXOr4AfLf5QsHKRfp3zMOZzo/Dzf2mro2x6OBHE/wVVnu+V+fT8aoKfu3SL+NeHjrPU7uBjy+B7ZPh0lY4+Bts/AiW9oCfGsBXPjC1PMwMkIVUkxNgSTf14saxKPjUULsqd0yG6dVh3yy1NS7yOiREgpmlOv9ZdlTvDsUbqMf98y1Y/Zba4vI8uLEPfn1ZXY/S3gv6rYfK7R89Xv/hCgMHf4OkuNyd6/Sf8OcgQIHyr6nJ8ufWwfrR0nKdRyRAKuxMkagdfgkeBKv/kKmjUp5k6Gbblq9VGdCoFI7W5py4FcX7K08UjK62s+vg1KpHv5/5y3R1eZJerzbFz26qJo8WRtF3Hl2Bv/IpaDP5GCvzMrScrN7fMVlNlv3nPTUP5Nw6CD2ttsCC+qW/sv+LewWu16sBy819astbj5UwcJvaAuJSRl2IesMHastoau6Xh1/2J+W0tIO+66DpOLWl+cQy+OUldeRrYXZ8qfq/FRemfi4P/O/RvE+pKr0BziXUruHjS3J+rrPr1NYiRQfVe0DXxdDxN0ADh+bAzim5eipZoihwZCGs6PfcJp5LgFTYpSZqBz/DACl1eL9vXbCwSb/M43lI+Ri0+DjbMKtnLcy1GtYev8P0rRfz7VxZEhehTlQIjxbnPPNXwelmu7xVbR0A+GeU+kFb2Oz4BlISoHh9KNv86eXrDoQGw8HSXl0vrHxLCBgCr32jBgHDDsOgXeosz1d3wH9f5P9zSJUUCxFXC8YV/6aP1feq1gK6/qHOdK3RqC0gQ/fD69+rrSKRN+DIAnWf7CRoP05rBk0/gL7rwclXbW39/VXY/X3B+V/JKr1OnVbiz0Fq91nF16H/BnAqmras1gzqva3eD5qZs+d6YSOs6KsmflftAm1+VC8S/NpAq2/VMtu+ePQ3yg9JcfDnYFg7DE6vVi+4jv5RMN7HeUgCpMLOO7UF6eSze3OmN7z/Sb511fyo2Htqc3M+alDWjS/aVQFg2paL/HXMhKPaNoyF2FD1i7jnSrB0UIeh3y4g874c/E39mTrJ6KoBcD3ItHXKjvDLcHShev+VT9Uv8KfRaODVL2DcbRh2ALovg9cmQ8AgKNcC3Mqq/0dtf1TL7/4Ozv2Td3VOSVKvsM+ugz3TYe0ImNsaplZUu/d+qA5bPsu78+VE0EzYN1O93+7ntF3nZhZQuz+MOKq+7lYPR2SVbpa785aoD4N3qd2k+hTYMh4WtlO7gRVF7XqLvAkhp+DabvXvcmzxowRxU0uIVrsk9/6g/t74Pei8UG0ly0iNnuqItojLcOHf7J3v0hZ10IE+GSp3gLY/qUFXqroD1ToA/P1u/swFFn4ZfmsOJ5aq3ddeVSElHv4aqgZNiTF5f04TydZabKIA8vBT36RxYWq3l6NPxmUfhKhrIJV6KWtfLOnRPza/UelMAiRzKyjRQP2HvrL9Ua5UPulatzhXwmKZvfMKY1aeoFgRG2qVcMnXc6ZxfsPDDw0ttPtJ/RCs8BqcXK5emWfUHfms3L+uXn0C9FkHWyfA+fXqUPh+GwrH2ljbJ6tfpGVbqO+vvFSlozqyav/P6gf9W9vVgRC5cXWX2m0X+5RRS3umq3k5FVrm7nw5cfpPNScLoPkEqPpmxmUtbdUv4Fr91FYfn5oZl80qmyJqN96RBbDhQ7UVb5q/2n2kZNDCYmaptj751sn9+XMq/LIaHIWdB3NraDsT/Ds9fT8re/X12zNNHWhQsXXWzndlu5o3p0tSu+o6zE5/geCXP1E/648tUlua+vydd6/TuX8eBkHRYOcBb85V37e7v4NtX6qff7cPqX/P1N6NQkxakAo7CxtwK6/ez2w+pJREmPc6LGjzKH8gJ+6eUvvPLe3VBM7MPIPh/sRFwJqhcGYtH7SsSAs/T5JS9Ly14DA3I3KZBJkd8ZHqfCcA9YdCsYcT4vm1VX+e+cv0zc+H5wKK+nfxqAgdf1dHFCVEqaOWIm+atn5Pc/c0nFyh3n/54/w5x6sTwbee+gWwrKfaBZZTh+eprSGxoWpLond1qNIJmnwAHX6F//0HH1xTu/sA1gyGqGfc+nk9CFY/TPStMxAavpO1/Wxd1Ck+cnqh9SSNBmr1gbd2qF+s+uRHwZHWQm3xdC2nTjTpVl4NEpb1yNvXKy5Czd3Z+e3TW+SvbFeTscPOg4MP9Ps3a8FRqoBBoDWHG3uzlnt1bTcs7qp2LZd/DTrOyXhyTo0G3piuXkSkxMPizhCWy9QDXYq63uHS7ur/hm89GLRTHbGs1cJLo6HvP+prEX4Jfn1FncQ1o9cwKU4dKHL27wI9MEKjFIis1sInOjoaJycnoqKicHR0NG1lVg1UWyle/hheymAq++1fw/avHv3eaY56xZxde2fApo+g3KvQY0XmZUNOwqxGam7HB9fyfoV1XQr80V7t8rN1g/fOEafT8OasIE7fiaachz2r3m6Ao3UWZ/nNjTVD1TXAXMvC4N2PcrOS49U5SpJi1C/EYrXyvy7pSUmE7/zUlsYuf6hXoKB+Kcx9TZ3Z2K089N/4aF29gkSXDIu7qDlUfu2g8/z8O9eDEJjVWA1s/DurV+rZCQR0KWo+T+ocOFU6qq0LGeXrpSSq+TfBx9S8qj7r0m8ZyEt6vTqwY0FbdTRahdbQZaFxd42p6PVq65SFrdoKa2Fj/PonPlBfr9AzatDZ71+1ZSundMlq1/P2yeprkcrJV20BLt9SnXjU3FL9wj8wW+1KV3RqwNZ1kbo2ZnatHqS2uFTuoLbEZOTkSrX7KiVBDXq6LlJb6J8mKVa9KL5zRJ1pvkxTteXNzEp9LmZW6u/mluoFr50b2Lqqn6V2bmrLntZMnbNpZf9Hc9/VextafJ5+gBYbrk47cPFhS7VfO2g4Qm1tCz2rfs6EnlFbs3kYepjbqN23DUfk7HXMgax+f0uAlEMFKkDa+6P6gVypjfoh96Twy/BTfdAlqh/AN4LUf44+f0PxgOyda1Fn9c3/6hdq4mtm9HqYUk79Uu67Hko2zN65nmbDuEd5EwBdl0DFVoREJdB25m7uRifSuJwbc/vWwdwsHxtLL26BRR0BjZqcWbye8eMr+6uj2hqMUFsoTOHEClj9P/UK792Txl/AUbfUL5zo2+o8Lb3X5u4LJ69F3VJfw5v71e7kt/epQ/jz07U96ogkRafOzF13YNb2S4hSR/Vc3qr+3uxj9er6aQFWxBWY9ZI6dcFLY/KmhUxR1DXPIi6ridWP36Juqa00UDD/5k9z/xrMbqa2ZlfpqLaGZrc1S1Hgwgb1szP8krrNs4oaGF3Zrra+pLJ0gLKvqEFBaitm1a5qS42Fdc6eQ+oFpMYM3jkGzsWNH9fr1YvanQ8Tr8u/pnZdZed8sWHwewv1/ZVtGjVI0qeorUYWdtB2BlTpkPluer36ubxlvLpvRmzdHuVigfqdVKuP2orpVCwH9c06CZDyWYEKkK5sV68Ei5RS/9EepyiwsL063L7My+qonWU91dwTW1f431ZwKZW18+iS4euSamvIoJ1Zm0E3NTh46X14+aNsPrFMHF+mDkkG9QP+1kF19EjXRQCcuh3Fm7OCiE/W0bNecSa2rYImr7oDHpcQDT/VU4OLgCFq8u+TzvwFy3urw3vfOZ533RLZ8XugOny72UfQ5P20j4eeUye3S4hUr5i7LMqfVoyIq2p3iZV91spf2KiODoq/D1aOam5XautXfkttLdVaqPPZPC2HLPyyOnty2AW19aP9rEddrFlxcqWaNI8Gev0JZXKRAK1LgVX9M59iQmOmXjB1nq+2GBQ213arn3v6FDXv5qXRWd835BRsHPdoRK6duxqU1uiltpokxamPnV+vvgdj7j62swZaTFAveHL7vzy/jXqeekOh5WMt/Emx6vv+7N/q7w1GQPPxOWvhiw1XP4OTHqgDBnQPbymJ6kVzSpLaKhcXrl7MxoYZt6SB2rrc5Q9wr5D18946pA5IiL6tLmbsXlHNmfWoCO6V1GVnFEWdm2znN4/mi9NaQI0e0Gjko2Vp8pgESPmsQAVIcRHwzcMg58Mbj9b8gUcfumZW8HaQmnSaFKt2qwQfV/v1/7dZvVJ4mpsH4ffmYO0M71/NfP6ZVEcWwNrh6pD3/23O0dNL485RmNNSbXJ+aYzaRP1zfbVP/73zhg/7jadDGPzHYRQFpr5ZjY618uGq5O931FyTIiVhyN70R68kxcG3ZdT5dt7akfOh0TmVeqWqNYeRpzNuxr6xX81RS0lQr+60ZmoeiF6ntqTo9epPc2u1RaXRqKxfzcZHqlfqRxeqgU71HuoxMkqC1iXD1s8fjQ7yrq52Q7iUzu6zzzlFUZNcz6wBB2/16tneU31tbF2Nu4yv7FCD4IRIdYLFbkuyvwQHqF8oR+arCbBD9oC9R/aPoder3RwnlqpdKMXrg7OvGqA7F1dbSJyLq88pv7vy8tuhOY+m1ei6+OkJzw9CYPsk9XNJ0auvT7231cRz6ww+x/V69TPn/Hq1uypgCJR/NW/qf3EzLOqktlCNOq1+dkfdUgPtkJNq/d6Yrk6u+SzpUtTWudgw9fvCu2rWuvVyKnXy4Z3fPurK05hBta7q3ya3gyWeIAFSPitQARLAd5Uh+pbaH586uic+EmbWVa9+nmw5iA6G315Ro/uSjaHn6qfnCO2cAv9NVK/gu/yRtXpF3oRpVdQ3+wdXjYO3nIi5p865EX1LbenoukQN1GY3VT/EWn4N9QYbiv+w9SLfbb6Aq50l/73XFCfbPMxHurRFTW4GNUGxZKOMyy7vrV7NNxoFzZ/xkO6/31UTtLOSu3P+X7WuuqSnH9eljDo3TmajGUEd3bfuXXWUpRGNmssWMEht3Uy9Go+8qbY83jqg/l53kNo1mZ8f0BlJfPAwGfdC2sesnMDOVQ2W7hxVWzJyk5MCajD92ytqnkbpptDzz6xdiKRSFHV+q0Nz1P+5LguzPkqqsPrnPTWHyNIeBmxKO2JWUeD6XrXM2bWPun382qktQfnUSpEliqK2QN87By0mqt3zS3uo+W927mpLbnbTIAq760Fqi9Ll/9TfO/6evQT4LMjq97eMYntepM6H9PiEkf99oQZHrmXTjk5x9FYXT7S0VyP2de8+fZRVVuY/epKzr/pFqujUvI7c0CXDij5qcORaTk2eTf3yqPbwCuvYIqNdBjcpQ1kPe8Jjk5i6+Xzuzp9Kr1fzvhZ3VX+vMzDz4AgeG8225tmOZkuIhhMPF8ms87+nl6/wGow8ow5xH7RLbRV7ez8MOwTDj6hdhJ3mqBMGRlxWW5xWD0p/JEpchDqAYEkXNThyKaPmovVcpQZGKGo+2x8d1ED+wK9qEPlLYzU4snJS55Rp9Y1pgiNQF2LutlTtvvXwU1t2NA/fc4lRam7HrYPql67/mw9H8uQi0dTSFjrNVbvormxXh09nlaLA5k/V4AiN+v/xvAdHoM6UXrKx2vW/pKvapQRqcHvwN/i5AcxrpU5oqE9RR2D1+1e9WDBlcATqRUH9oer93d/BvNZqcOTpr87E/aIFR6DOjdXrTzX9o1Zf46VanjFpQcqhAteCtO0r2PE1VO8J7WaqQ0d/fQVQ1ATMjK7yL25Wh4Eq+sz78ZMT4OsSavfL0APZ64teNwoO/Q5133o002tOrB+jjiCxdFA/PB5P1I2LgCnl1cTTwXvAq4rhob2Xw+j+6360Glg7rBFViuaiFevBXXU4durVTaU3oP3spye4Jsao3WwpCeoot6fNEXJssRowpCSogaEuSf2pT72fouaotPs5464BUI+xfrQ6ceXQ/XmX/5QQpQbgB34FFLWLtsXn6vtPq4XTa9Tzxt5TA4r6w6DZOOORXGGX4OCvcHSRmh/xOJ8aaqCQ1fy4Z0mvV7vSYsPU5xcXpnYblm6ad6/v0T/UkUsaMzXoKlH/6fvs+EadiwbgjR/UhNcXRVyE2tJ3/6o6L4+nn7r0R9LDSQstbNUAts7/Hl1MFhTJCWore+zDBbcrvg7tf8l6np7INuliy2cFLkA6u06dF8TLHwZuh1+bqcN4q3ZRryQzc/A3tZka1JYmO3dAo36xaTTq/Qd31Mns7L3gvXPZ+yI4sxaW91JHQbT6Rs0/ye4XSeoXBhhGq6WxrJfahF5/GAR+afTQiCVHWXv8DtV9nVk9pAFabQ6+yC5uUYOj2Hvq0NSWk9QrnKw+l6U91PW/njZK6dIW+KMThmGwmfGpAT1WqV09T3q8+f61byHgrazVMztuHVbzsO4+nIOreAM1B+zsWvV390rqEPfMpjdIiFbXpTowWx1NZMoutYJCUdQk3RPL1Lyn+kPV0VoZje4JmqkmHQMEToL6bz+7uhYUoWfhtxbGwbZrOTUoqtZVXUS7oDqyAP79EOoNUdMhstOtKrJNAqR8VuACpPvXYXpVdQTAK5/C5k/UfJ9hh7KW6PnkkPmM+HeGjr9mr24piWquTmryXcXX1cTDrI6cuXVITSrXJakLXDb9IP1y5zeo3Tl27jDqrNE8HXejE3hl6g5iElOY3MGfrnWLp3+MjOq/9XN1gVMAj8pqN5NHxawfAx4NtXctB8MOph9YRd5UF+6Mj1CD22rdHs5dYqkm1Kbej76jJhDHR6gjTHqtSbv207XdapO9hR28dzb3+V8Z0aXA/llq60Xqwq9aczXf6qXRWQ90UltmCuI8TKaQ+EBtBQ57rGu4eAPw7wh+7R8FxYfmPpqktNnH0CSDudBeBBc3qwG7Tw11EECpJqYZNZoTel3BmIfqBSABUj4rcAGSoqhdYAlRarO8olMTaGv3z9r+ep36JZc6g6yiBx7+VPTqNnMrtfXDrVz266fXqSOS/vtS7Sayc4c2MzJeWkFR1NyOA7PV7hp9shpYdV6Y8dWVLhm+q6S28HRbqubTPOa3XVf44p+zFLG14L/3mlLELgsTV4ZdUodLBx9Xf6/7lppMmZO5TxKi1W42XZI6l49HJePHU5LUQPD2IXUEVP9NmZ/n3nl1Cofo2+rIpN5/GY/2WNFXXUaiVj94Y1r265tdkTfUHJjYMAj8quB1ZRRGCdFwaqU6GvX6Yzl8WnN1HTQvf3WBVxS19bf5hMITEAhhIhIg5bMCFyCBOmtqaitNsTrqF2xBa6oNPgGr34J7Z9Xfa/ZRv0xT+9uT49Uvg4O/PgpKQL0S7LpITZrNzMaP1JaedEbapej0vP7jbs6FPKBb3eJM6vCUPKDzG9TRVMmxYOOidhWl17WXHYu7qgtUNvkQmo01fiw1x8raSZ1nKisJpJE3YEE7NWHazl0djehdVR3O/H1lNSk1KzlPouCLuq0mGp9cYfy/AWo3UqspEhwJkQUyiu1F5PXwil1jprYeFbTgCNQv77e2q3lCoM75MquRugji5k/VFqC1w9QvAHNrNen3re3QZ+3TgyNQu6RADW7iIoweMjfT8nlbNXl76cEbHLsZmfFxjixQ1x1KjlVHyAzZk/vgCKByO/XnkxP4nVypBkegJn1ndXSNc3F1eRAvf7XlbN7r6jDZIwsejtgJkODoeeFUVJ29ftBOteu86Vj1bxswRM0xk+BIiDwlLUg5VCBbkK4HqcOuXxqT/mzJBc3VnfDnEHXY/uOci6tXxDV65Swf5ZeX1AArg8TkUcuOsfrobfyLOrFmaEPMHk/YVhR1vqdtX6i/V++h5ktltDBkdsVHqmuz6ZMfjQYMPaeOwEmOzfk8SQlR6lplN4LUBHILGzU/qcOvULVz3tRdCCGeA9KC9CIqUR8+CikcwRFAqZfUlpmqXQGNOllgt6Uw4piaT5HTZN0M5kRKNbZVJRyszDl5O4olB248ekCvU0fzpQZHjUer3Wp5FRyBOpImdQmJM2vV4f/Lez9qqWqWw+VYrJ3U7rXUFbzjI9QJDLOz1IUQQggDCZCeN4VtFISNM3T4BT4OVScHq/Ba7p+D/5vqaL7gY3D3dJqH3R2seO9VdQ6lbzeeJzwmUZ2LZEUfdb4mNGo+xyuf5E+3xeOTRv49Qh2lZO+ljozLzdIPlrbqcgtVHs46W3fQiz1UXgghckECJFEwPG2Zk+ywc4Xyger9Y4vTLdKzXgn8vB2Jik/mq9VBJM5rqy4MaWaprpid1dXbc6JCK3UU0t1T6iKSWnN1Vt+crLv1JHNL6PibmqOSncU7hRBCGCkQAdLMmTMpWbIk1tbWBAQEcODAgUzLr1ixgooVK2JtbY2/vz/r16/PsOzgwYPRaDRMmzbNaHvJkiXRaDRGt8mT01mJXRRO1XuoP08sV+fpeYK5mZaJ7argRThvXRqK1e19xGLLwrLfs8eqEUkp+vyrm62L8XItLT5X12DKKxqNOhVDYWtNFEKIAsTkSzkvW7aMUaNGMWvWLAICApg2bRqBgYGcP38eD4+0V9R79+6lW7duTJo0iddff53FixfTrl07jhw5QpUqVYzK/vnnn+zbtw8fH590z/35558zcOCjlgIHhyyMkhKFQ7kW6qrrsaFweeujFiVQJyS8vptax5ex23415ilxhChF6Jv0AeeOF4Hj+7G1NKNBGTeaVnCnlb83LlmZMyk7avZS61W5g7qauBBCiALF5KPYAgICqFOnDjNmqLMU6/V6fH19GT58OB9++GGa8l26dCE2NpZ169YZttWrV4/q1asza9Ysw7bbt28TEBDAxo0bad26Ne+++y7vvvuu4fGSJUum2ZYdBXIUmzC2YSzs+0nN+em8QJ1Y8fhStVXp8ZFznv5EtZvPzlAbtp+/x44L9wiLSTQ8bG2hpXNtXwY2Lo2vy1PWXMuOiCvgXLJgTscghBDPqax+f5u0BSkpKYnDhw8zduyjCfO0Wi3NmzcnKCgo3X2CgoIYNWqU0bbAwEDWrFlj+F2v19OrVy/GjBlD5cqVMzz/5MmTmThxIsWLF6d79+6MHDkSc/P0X5LExEQSEx99aUZHR2flKQpTqt5dDZDO/wuzm8Kdo48es3JS5ySq1g2K18NJo+ENb3ijmg96vcKZ4Gi2nw9l/ckQzgRHsyDoOn/su07rqj4Meql07ha8TeVSOvfHEEIIkS9MGiCFhYWh0+nw9PQ02u7p6cm5c+fS3SckJCTd8iEhIYbfv/76a8zNzRkxYkSG5x4xYgQ1a9bExcWFvXv3MnbsWIKDg/nuu+/SLT9p0iQmTJiQ1acmCgIvf/UWclINjrTm6jD4al2g/GsZLuOh1WqoUtSJKkWdGNqsLEGXw5m18wo7L9zj7+N3+Pv4HRqVdWNQk9I0KuuGRiboE0KI547Jc5Dy2uHDh5k+fTpHjhzJ9Ivr8VaoqlWrYmlpyaBBg5g0aRJWVmmHRo8dO9Zon+joaHx9ffO28iLvvfaNutJ5qZfU1dCzukDuQxqNhgZl3WhQ1o3Td6KYvfMK604Es/tSGLsvhVGlqCPfd65OOU/JXxNCiOeJSZMf3NzcMDMz4+7du0bb7969i5eXV7r7eHl5ZVp+165dhIaGUrx4cczNzTE3N+f69eu89957lCxZMsO6BAQEkJKSwrVr19J93MrKCkdHR6ObKARKNFDXcAsYlO3g6EmVfZyY3rUG20c3pW+DkthYmHHqdjQdftrLrov38qjCQgghCgKTBkiWlpbUqlWLrVu3Grbp9Xq2bt1K/fr1092nfv36RuUBNm/ebCjfq1cvTpw4wbFjxww3Hx8fxowZw8aNGzOsy7Fjx9BqtemOnBPicb4utoxvU5ldHzSjTskiPEhMoe/cg8azcgshhCjUTN7FNmrUKPr06UPt2rWpW7cu06ZNIzY2ln79+gHQu3dvihYtyqRJkwB45513aNKkCVOnTqV169YsXbqUQ4cOMXu2utCnq6srrq6uRuewsLDAy8uLChUqAGqi9/79+2nWrBkODg4EBQUxcuRIevbsSZEiRZ7hsxeFmZu9FX/8L4APV53kz6O3Gbv6JFfDYvmwZUW0WslLEkKIwszkAVKXLl24d+8en376KSEhIVSvXp0NGzYYErFv3LiB9rFh0A0aNGDx4sV8/PHHjBs3jnLlyrFmzZo0cyBlxsrKiqVLlzJ+/HgSExMpVaoUI0eOTDM6ToinsTI347vO1Sjpasf3Wy4we+cVroXFMq1rdWwtTf7vJYQQIodMPg9SYSXzIIkn/XXsNmNWnCBJp8e/qBO/9amNp2P6I+WEEEKYRla/v2WGOiHySNvqRVk8MAAXO0tO3o6i3cw9nLkj82UJIURhJAGSEHmodkkX/ny7AaXd7QiOSuDNWXvZfObu03cUQghRoEiAJEQeK+Fqx59DGtKgjCuxSTreWniIn7dfRnqzhRCi8JAASYh84GRrwfz+dekRUBxFga83nOO9FcdJSNaZumpCCCGyQAIkIfKJhZmWL9v783nbyphpNaw+cpvuv+7j3oPEp+8shBDCpCRAEiKf9a5fknn96uBobc6RG5G0nbGb03eiTF0tIYQQmZAASYhnoHE5d9YMbUhpNzvuRCXQ6ecgNpwKefqOQgghTEICJCGekdLu9vz5dkMalXUjPlnH4D8O88PWi+j1krwthBAFjQRIQjxDTrYWzOtXhz71SwDw3eYLDFxwiKj4ZBPXTAghxOMkQBLiGTM30zKhbRW+6VQVS3MtW8+F0mbGbs4Gy6SSQghRUEiAJISJdK7ty+ohDSjqbMP18Dja/7SHNUdvm7paQgghkABJCJOqUtSJdcMb8VJ5dxKS9by77Bjj154mKUVv6qoJIcQLTQIkIUysiJ0lc/vWYcTLZQGYt/ca3X/dx93oBBPXTAghXlwSIAlRAJhpNYx6tQK/9a6Ng7U5h67f5/UfdxMcFW/qqgkhxAtJAiQhCpDmfp6sHdaIMu523HuQyIz/Lpm6SkII8UKSAEmIAqaUmx1ftvcHYMWhW4RESVebEEI8axIgCVEA1SvtSt2SLiTp9MzeecXU1RFCiBeOBEhCFFDDHiZtLz5wnbAYWeBWCCGeJQmQhCigGpdzo1oxJxKS9fy266qpqyOEEC8UCZCEKKA0Gg3DXi4HwMKga0TGJZm4RkII8eKQAEmIAqx5JQ8qeTsSm6Rj7p5rpq6OEEK8MCRAEqIA02g0DGum5iLN3XOVBwlPX9T24LUI/jf/IHsvheV39YQQ4rklAZIQBVzLKl6UcbcjOiGFBUHXMy0bdDmc3r8fYMvZUPrPP8ihaxHPqJZCCPF8kQBJiALOTKsxjGj7ffdV4pJS0i0XdDmcfvMOEJ+sw8HanIRkPf3nHeRscPSzrK4QQjwXJEASohB4o6oPxV1siYhNYvH+G2ke33s5jH7zDpCQrKdpBXd2v/8ytUsUITohhd5zDnAjPM4EtRZCiMJLAiQhCgFzMy1vNy0DwOydV0hI1hke23spjP7zDpKQrKdZBXdm9ayFk60Fv/epQ0UvB+49SKTn7/sJfSAzcgshRFZJgCREIdGhZjF8nKwJfZDIikM3AdhzKYz+8x8FRz/3rIW1hRkATrYWLOhfl+IuttyIiKP37weIin96krcQQggJkIQoNCzNtQx+2Io0a8cVtp8PZcBjwdGsXo+Co1Qejtb8MSAAdwcrzoU8YMC8g8Qn6dI7vBBCiMdoFEVRTF2Jwig6OhonJyeioqJwdHQ0dXXECyIhWUfjb7Zx78GjpUderujBzz1rYmVuluF+Z4Oj6fxLEA8SUmhWwZ3ZvWtjYaZeH+n0Cnci47kREce18FiCIxNoWNaN+mVc8/35CCHEs5bV728JkHJIAiRhKr/uvMKX688CWQuOUh28FkGv3/eTkKynbikXbCzMuBERx637cSTrjD8GNBr4pLUf/RuVypfnIIQQpiIBUj6TAEmYSlxSCgMXHMLTwZpJHf2zFByl2nYulIELDpGiN/63tzTTUszFhhIutmg0Gv47FwpA/4al+Lh1JbRaTZ4+ByGEMBUJkPKZBEiisNp54R77roTj62JLCRdbSrjZ4eVojdnDIEhRFGbtuMLXG84B0Mrfi+86V0+T3ySEEIWRBEj5TAIk8bz769htRq84TrJOoXaJIvzauzZF7CxNXS0hhMiVrH5/yyg2IUS62lYvyoL+AThYm3Po+n06ztorE04KIV4YEiAJITJUv4wrq4Y0wMfJmiv3Yunw8x6O34w0dbWEECLfSYAkhMhUeU8H/hzaED9vR8Jikug6e58sgiuEeO5JgCSEeCpPR2uWD65P43JuxCfreG/FcaPlToQQ4nkjAZIQIkvsrcyZ2aMmXo7WXA+PY/rWi6aukhBC5BsJkIQQWeZobcHEdlUAddHc03eiTFwjIYTIHxIgCSGypYWfJ639vdHpFT5cdZIUnd7UVRJCiDwnAZIQIts+a+OHo7U5J29HMXfPNVNXRwgh8pwESEKIbPNwsOaj1pUAmLr5vMyPJIR47kiAJITIkc61falf2pWEZD0frTmJTMovhHieSIAkhMgRjUbDVx38sTTXsutiGKuP3DZ1lYQQIs9IgCSEyLFSbna827wcABP/OUNYTKKJaySEEHnD3NQVEEIUbgMbl+bv48GcDY5m4rozTO9aI1/OE5uYwpngaBKSdcQn6YhP1hnuJ6ToSU7R07qqN6Xd7fPl/EKIF4sESEKIXLEw0/J1R3/azdzDX8fu0K56UZpV9MjTcwRHxdNu5h7uRmfeQrXi8C3Wv9MYeyv5aBNC5I50sQkhcq1qMWf6NywFwMdrThGbmJJnx07W6Rm2+Ch3oxNxtrWgkrcjNYs706CMK69U9KB1VW861SqGt5M1NyLi+Pzv03l2biHEi0sus4QQeWLUq+XZcDqEW/fj+W7zBT553S9PjvvNhnMcvn4fB2tz1g5tRHFX23TLHbgaQZfZQSw/dItXKnkSWNkrT84vhHgxSQuSECJP2Fqa88XDZUjm7rnKyVu5X4Zk0+kQft11FYBvO1XLMDgCqFvKhUEvlQFg7OqThD5IyPX5hRAvLgmQhBB5pmkFD9pU80GvwIerT+RqGZIb4XG8t+I4AP9rVIqWVZ7eIjSqRXn8vB2JiE3i/ZUnZG4mIUSOSYAkhMhTn7yuLkNy+k408/Zey9ExElN0DF18hAcJKdQs7swHr1XM0n6W5lqmda2OpbmW7efv8cf+Gzk6vxBCFIgAaebMmZQsWRJra2sCAgI4cOBApuVXrFhBxYoVsba2xt/fn/Xr12dYdvDgwWg0GqZNm2a0PSIigh49euDo6IizszMDBgwgJiYmL56OEC80dwcrxrV6uAzJpgvcup/9ZUi+WHeWk7ejKGJrwYzuNbEwy/pHVXlPBz5sqQZUX/5zhsv35P9aCJF9Jg+Qli1bxqhRo/jss884cuQI1apVIzAwkNDQ0HTL7927l27dujFgwACOHj1Ku3btaNeuHadOnUpT9s8//2Tfvn34+PikeaxHjx6cPn2azZs3s27dOnbu3Mlbb72V589PiBdR59q+1C3pQnyyjk//Op2trq61x++wcN91AL7rUh0fZ5tsn79vg5I0KutGQrKekcuOkZyLrj4hxIvJ5AHSd999x8CBA+nXrx9+fn7MmjULW1tb5syZk2756dOn07JlS8aMGUOlSpWYOHEiNWvWZMaMGUblbt++zfDhw1m0aBEWFhZGj509e5YNGzbw22+/ERAQQKNGjfjxxx9ZunQpd+7cybfnKsSLQqvV8FWHKliaafnvXCjrT4Zkab/L92IYu+oEAEOblaFZhZzNp6TVapjyZjWcbCw4cSuKH7ZezNFxhBAvLpMGSElJSRw+fJjmzZsbtmm1Wpo3b05QUFC6+wQFBRmVBwgMDDQqr9fr6dWrF2PGjKFy5crpHsPZ2ZnatWsbtjVv3hytVsv+/fvTPW9iYiLR0dFGNyFExsp6ODCkqTqqbPzfp4mKT860fHySjqGLjhCbpCOglAsjm5fP1fm9nKz5sr06qm7mtkscvh6Rq+MJIV4sJp0HKSwsDJ1Oh6enp9F2T09Pzp07l+4+ISEh6ZYPCXl0hfr1119jbm7OiBEjMjyGh4fxlam5uTkuLi5Gx3ncpEmTmDBhwlOfkxDikbebleHvE3e4ci+Wrzec46v2/mnKxCfpWH30Fr/vvsqVe7G42VvxY7camGcj7ygjr1f1YevZUP48epvhi4/SsKwbiSl6ElN0JCSrPxNT9CQm63Gxs6SarxPVijlT3dcZD0frXJ9fCFF4PXcTRR4+fJjp06dz5MgRNBpNnh137NixjBo1yvB7dHQ0vr6+eXZ8IZ5HVuZmfNXen66z97F4/w061ChK7ZIuANyNTmBB0DUW7b9BZJzauuRgbc6M7jXyNDiZ0LYyB65GcDsynhWHb2VadvelMMN9bydrqvs6U83XmRq+ztQt5ZKnnylCiILNpAGSm5sbZmZm3L1712j73bt38fJKf84TLy+vTMvv2rWL0NBQihcvbnhcp9Px3nvvMW3aNK5du4aXl1eaJPCUlBQiIiIyPK+VlRVWVlbZfo5CvOjqlXalS21flh26ydjVJ/mmU1UWBF1n3Yk7JOvU5G1fFxv6NihF59rFcLC2eMoRs8fR2oIFA+ryz4lgzM00WJmbYWWuxdpC/WllrsXSXMudyASO34zk2M1ILoQ+IDgqgeCoEP49pbYq92tYks/eSNtlL4R4PmkUE8+kFhAQQN26dfnxxx8BNX+oePHiDBs2jA8//DBN+S5duhAXF8fff/9t2NagQQOqVq3KrFmzCA8PJzg42GifwMBAevXqRb9+/ahQoQJnz57Fz8+PQ4cOUatWLQA2bdpEy5YtuXXrVrqj3p4UHR2Nk5MTUVFRODo65uYlEOK5FxmXRPPvdhAWk2S0vU7JIgxoVIoWfl6YaQtO60xMYgqnbkdx/GYkR29EsuF0CFoNbHj3Jcp7Opi6ekKIXMjq97fJu9hGjRpFnz59qF27NnXr1mXatGnExsbSr18/AHr37k3RokWZNGkSAO+88w5NmjRh6tSptG7dmqVLl3Lo0CFmz54NgKurK66urkbnsLCwwMvLiwoVKgBQqVIlWrZsycCBA5k1axbJyckMGzaMrl27Zik4EkJkj7OtJZ++UZkRS45irtXQuqo3AxqVomoxZ1NXLV32VubUK+1KvdLqZ8nghYfZcDqEyf+eY07fOiaunRDiWTB5gNSlSxfu3bvHp59+SkhICNWrV2fDhg2GROwbN26g1T5K1mzQoAGLFy/m448/Zty4cZQrV441a9ZQpUqVbJ130aJFDBs2jFdeeQWtVkvHjh354Ycf8vS5CSEeaVPNh1Kudng4WuFZyBKg329ZgS1n7/LfuVD2Xg6jQRk3U1dJCJHPTN7FVlhJF5sQL5ZP/zrFgqDrVCnqyNqhjdAWoC5BIUTWZfX72+QTRQohRGHwzivlsLcy59TtaNYelwllhXjeSYAkhBBZ4GpvZZj48tuN50lI1pm4RkKI/CQBkhBCZFH/hqXwcrTmdmQ88/deM3V1hBD5SAIkIYTIIhtLM957VV0CZca2S9yPTXrKHkKIwkoCJCGEyIYONYtR0cuBBwkp/PjfJVNXJ9/I+B3xopMASQghssFMq2Fcq0oALNx3jevhsSauUd7bfyWcul9tpc+cA8/l8xMiKyRAEkKIbHqpvDuNy7mRrFP4ZuN5U1cnT528FcWA+Ye49yCRHRfuEThtJ7N2XCZFp38m59969i7vLT/OzYi4Z3I+ITIi8yDlkMyDJMSL7cydaFr/uAtFgT/fbkCN4kUASEjWcTY4mpO3ozhxK4pzIdFU93Xmo1Z+2FiaZfn4er3Czzsuc/HuA95vWREfZ5v8eioGl0Jj6PxLEBGxSdQt6YK5mYa9l8MB8PN25OuOVfEv5pRv5997OYzevx8gRa/gZm/FvH51qFI0/84nXkxZ/f6WACmHJEASQry3/Dirjtyiso8jVYs5ceJWFOdDHpCiT/uxWt3XmV9718bd4emLXsclpTBy2TE2nlYX5i5ia8F3navTrKJHnj+HVLfux/HmrCCCoxKoWsyJRf8LwN7KnJWHb/HFP2eJik9Gq1FH8o16tTy2lnm7EMOl0Bg6/LSH6IQUrC20JCTrsbM04+eetXipvHuenku82CRAymcSIAkhgqPiafrtdhJTjLufXOwsqVrMiapFnfB2tuHrDeeIjEumWBEb5vWrQ1mPjBe8vRudwP/mH+Lk7SgszbQUd7XlUmgMAIOblOG9V8tjYZa32RH3HiTy5qy9XAuPo4y7HSsGN8DFztLweFhMIp//fcYwQWZRZxu+bF+FphXyJmALj0mk/U97uRERR83izvzSqzYjlhwl6Eo45loN33SqSoeaxfLkXEJIgJTPJEASQgD8efQW644HU97LgWrFnPAv5oyPkzUazaOlSK7ci6H/vINcC4/DwdqcX3rWokHZtOu5nbodxf/mHyIkOgEXO0t+6VWLqsWc+Oqfs8wPug5A7RJF+LF7Dbyd8qbLLSo+ma6z93E2OJqizjasHFI/w2NvOxfKx2tOcTsyHoCvO/rTpU7xXJ0/IVlHj9/2c/j6fXxdbFjzdkNc7a1ITNExesUJ/n4YlL3fsgJDmpQxel2FyAkJkPKZBEhCiOyIiE3irQWHOHT9PuZaDZM6+PNmbV/D45vP3OWdpUeJS9JRxt2OuX3rUtzV1vD4PyeC+WDVCWISU9Quty7VaZbLFpy4pBR6/X6Aw9fv42ZvxYrB9SnlZpfpPrGJKXzxz1mWHLiBtYWWtcMaUd4z4xaxzCiKwjtLj7H2+B0crM358+0GRq1rer3CpH/P8uuuqwD0rl+Cz96ojJmsgydyQQKkfCYBkhAiuxKSdYxZ+ahVZPjLZRnVojy/7brKV/+eRVGgUVk3ZvaoiZONRZr9r4XFMmzJEU7djgZgSNMyDGlaBp1OIVmvJ1mnkJyiJ1mnJ0mnR68HG0sttpbm2FmZY2dphvnD7rmkFD3/W3CInRfu4WhtztK36uPnk7XPMr1eoc/cA+y6GEZ5T3vWDmuEtUXWE9BTfbfpPD/8dwlzrYYF/eum26oG8Pvuq3zxzxkUBVpW9mJa1+o5Op8QIAFSvpMASQiRE3q9wtTN55m57TIA5Tzsufgwx6h7QHEmtKmcaY5RYorOqMstu6zMtdhZmaPVQFhMEjYWZvzxv7rUKuGSrePce5DIa9N3ERaTSI+A4nzZ3j9b+686fIv3VhwH4JuOVelcxzfT8utO3GHUsuMk6fTULenC731r42CdNogU4mmy+v0t8yAJIcQzpNVqGBNYkW86VsVcq+FiaAwaDXzcuhJftqvy1ARsK3MzJrStwszuNSli+yhA0GjU4MfeypwithZ4OFjh5WiNk40F5o91SSWm6ImITSIsJglLMy2zetXKdnAE4O5gxfddqgGwaP8N/j0ZnOV9918J58PVJwC1FexpwRHA61V9mN+/Lg7W5hy4FkGv3w8QFZ+c7XoLkVXSgpRD0oIkhMitvZfC+H33VXrUK87LFT2zvX+KTk+KXsHCTPvUvJykFD2xiSnEJqUQm6gjNikF3yK2WZp2IDOT/z3HrB2XcbA2Z/2Ixvi62GZafv+VcAb9cZjIuGRa+3vzY7caaLORU3TyVhS95uwnMi6ZKkUd+WNAAM62lk/fUYiHpIstn0mAJIQQkKzT8+asII7djKRmcWeWDaqfbitYbGIK32w4Z+garO7rzNK36uUol+jMnWh6/r6fiNgkKno5sOh/Abja5y7QEy8O6WITQgiR7yzMtPzYrQYOVuYcuRHJ95svpCmz93IYLafvNARHXev4snBA3RwnWvv5OLL0rXq42VtxLuQBXWfvI/RBQq6ehxBPkgBJCCFErvi62DK5Y1UAft5xmd0XwwC11eiTNafo/ut+bkbEU9TZhgX96zK5Y9VcJ1iX93Rg2aB6eDpacTE0hq6/7CMkSoIkkXckQBJCCJFrrat6061ucRQFRi4/xj8nggmctpOF+9RWox4BxdnwbuM8XTakjLs9ywfVp6izDVfCYukyO8gwiaUQuSU5SDkkOUhCCGEsPklH25m7uXA3xrCtqLMN33SqSsMM5jjKCzcj4uj+2z5uRsRTrIgNSwbWe2qyuHhxSQ6SEEKIZ8rG0owfu9XE2kL9aulVrwQbR76Ur8ERqF18y95SZwG/dT+e4UuOoktnwWAhskNakHJIWpCEECJ9V+7FkKxTqOCVsyVIcupOZDyB3+/kQWIKn7zux4BGpZ7p+UXhIC1IQgghTKK0u/0zD44AfJxtGNuqEgBTNp7nZkTcM6+DeH5IgCSEEOK50bWOLwGlXIhP1jHuz5NIJ4nIKQmQhBBCPDe0Wg2TOvhjaa5l18UwVh25beoqiUJKAiQhhBDPldLu9rzbvBwAE9ed4d6DRBPXKH9duRdDfJLO1NV47kiAJIQQ4rkzsHFp/LwdiYpPZvzfp01al4RkHetO3OFS6IM8P/aGUyG8PHUHXX/dR1KKPs+P/yKTAEkIIcRzx8JMyzedqmKm1fDPiWA2n7n7zOsQk5jCLzsu0/ibbQxbfJS2M/aw/0p4nh3/fmwSH685CcDxm5F8veFcnh1bSIAkhBDiOVWlqBP/a6wO9f94zUmiE5KfyXkjYpP4btN5GkzayqR/z3HvQSKW5lpik3T0nXuQvZfC8uQ8n687Q1hMEu4O6kK9v+++ytazzz4QfF5JgCSEEOK5NbJ5eUq62nI3OpHJ/+ZvC0tIVAIT152h4eT/+OG/S0QnpFDa3Y5vO1Xl8MfNaVLenfhkHf3mHWTnhXu5OtfWs3f58+httBqY3asWfRuUBGD0iuMER+X/cisPEpJZefgWP2y9SFxSSr6fzxRkosgckokihRCicAi6HE63X/cBsPStetQr7ZrjYymKQlhMEjfvx3EzIo5b9+O5GRHHzftxHLgaQbJO/UqtUtSRoU3L8mplL8y0GgASU3QMXXSELWdDsTTTMqtXTV6u6JntOkTFJ/Pq9zu4G53IWy+VZlyrSiSm6Oj4815O3Y6mbikXFv8vAHOzvG0DSUjWse1cKGuP32HruVBDztObtYrx7ZvV8vRc+Smr398SIOWQBEhCCFF4jF19giUHblLC1ZZPX/ejQRk3bCzNnrqfXq9w5MZ9NpwKYdfFMK5HxJKQnHEydN1SLgxtVpaXyrmh0WjSPJ6Uomf4kiNsPH0XCzMNM7vX5NXKXtl6Lh+uOsHSgzcp5WbHv+80xtpCfR5Xw2J5/YddxCbpGPFKOUa1KJ+t46YnRadnz+Vw/jp2m02n7xKT+Ki1qLSbHVfDY1EUmNm9Jq2reuf6fM+CBEj5TAIkIYQoPKLik2nx3Q5CHw75tzLXUr+MKy9X9KBZBQ+jxW2TUvTsuxLOhtMhbDp9l7AY42kCNBrwdrSmmIstvkVs8XWxwbeILRW9Hajs4/TUuiTr9Ixcdox1J4Ix12r4oVsNWvlnLbjYdfEevX4/AMDyQfWpW8rF6PG/jt3mnaXH0Ghg0f8CaFAm5+vgnQ2Opt/cg4REJxi2FXW24fVq3rSp5oOftyNTNp1n5rbLOFqb8++7L1HU2SbH53tWJEDKZxIgCSFE4XI1LJbfd19h27l73I40ztMp52FPs4oehD1IZMvZu0QnPGopcbA2p3klT17186SStyM+zjZYmueu+ypFp2fMyhP8efQ2ZloN33WuRtvqRTPdJzYxhVe/38ntyHj61C/BhLZV0i33/srjLD90Cw8HK/59pzGu9lbZrl+yTk+bGXs4GxyNi50lrf29aVPdh1rFi6DVaozKdZoVxPGbkdQt5cKSgfUMXYoFlQRI+UwCJCGEKJwUReHC3Rj+OxfKtnOhHL5xH53e+KvQzd6KVyt70rKyF/VKu+Y6IEqPTq/w4aoTrDh8C60G3qzlS/9GpTJcx+6zv04xP+g6RZ1t2DTyJeyszNMtF5eUQpsZe7gUGkPTCu7M6VPHKKjJih+3XmTq5gsUsbVg86gmuGUSZF0Li6X1w669MYEVGNqsbLbO9axJgJTPJEASQojnQ1RcMjsu3mPPxTAcrM0JrOJFzeJFnklLiF6v8Mlfp1i0/4ZhW+NybvRvVIom5dwNgc2BqxF0/iUIgD8GBNCoXOZdZ+dComk7Yw+JKXrGtarIWy+VyXKdLt59QOsfdpOk0zOtS3Xa1ci8ZQtgxaGbjFl5AnOthpVDGlDd1znL53vWJEDKZxIgCSGEyAuKonDo+n3m7L7KxtMhpDZmlXG3o1/DUrTy96bDT3u4Fh5H1zq+TO5YNUvHXbT/Oh/9eQrz/7d390FNnfkewL8JIeFFhACFhPoCXlF8A1/AmNJur8KI1t2trlXs4Da1u9eqiCjbO6u7VujuWLjrareuDK6WqjO1Yuksri9VS6nSqgiCoLgq1ZZWrxKRtmKIAkKe+4drbhOCCxQ4RL+fmTNDnuc5Ob/zm0z4zXOecyKX4b1f6zp0916rRWB21glUXL2FKWEByDZEOlxs7ugclu4qx4GzNRjs54GPlj3T7gyX1Fgg9TAWSERE1N2ufncHO058jd2nrsL0rzvGFHIZWiwCmv5u+DjlJ+jv5tqh9/ph0eKhdMH2BRPbLOq2l32sGn/cfx5eKgU+TvkJtN4dX3Rdf+cepr/9Ga7XN/bpW/87+v+bD4okIiLqIwb6emD1T0fixKopWPPTkRjk64GWf00prZ01usPFEQDIZDKsnxOBZ0L9cae5FS9vK8Gpr79rd/yVb+/gz4erAACrnhvRqeIIALw9XPFW/FjIZUBu2f/iwNmaTu3f13AGqYs4g0RERD2t1SJQ+EUtZJBhclhAl96j8V4rfr2jFMcu18FT6YIdr0xEZLDtTJIQAgnvFOPEl99CP8QP7/+XrkOX1hz58+EqbDpyuc/e+s8ZJCIiIifnIpdhSlhgl4sjAHBzdcHWlyLx1H/4wdzcCsO7JSj7xnYmKefUVZz48lu4ucqRMXtMl4sjAEiODcXYgT643diCxJ2n2zxHylmwQCIiInrEuStdkG2Ign7IgyLpFE5f+R7A/d+Qe/PABQDAa1OHY7Cf5486lquLHG/PGwsvlQIVV2/hubc/R/FX3/7oc+htLJCIiIgeA+5KF2S/HIlJQ3zR0NQCQ3YJyq98j9V7KmFqasHYgT5YEB3SLcca7OeJvy95CkMD+qHW1IQXt55E5pHLsFicZ1UP1yB1EdcgERGRM7rT3IIF206huPo7KBVyNLdY4Ooiw4Flz2BYoOOHVP6YY63OO4e/l18DAPzn8CewYe5Y+Hoqu/U4ncE1SERERNSGh1KBbQuiMDHEF80t9394N2lKaLcXRw+OtX5uBP5n9hioFHIcrbqJGRs/b7MGyhGLRUg648QZpC7iDBIRETkzc1MLVu85B4sQWPdCRI/8nMoPXai5jcSdp/FVnRkKuQz/HTcckcFqGOubUFN/F8b6RhhvN8JY34ia+kbUmhrxwat6jBuk7tY4+KDIHsYCiYiIqHMamlqw6u+V2HfmeofGb54/HtNGa7s1ho7+/+6bzwEnIiKiR04/lQIb542FLsQXmUcuw0Uug9bbDRpvd2j6q6DxdofW2w2B/d2g9XZDgFf7P5Lb0ziD1EWcQSIiInI+XKRNRERE1EV9okDKzMxEcHAw3NzcoNPpUFJS8tDxubm5CAsLg5ubG8aMGYOPPvrIpj8tLQ1hYWHw9PSEWq1GbGwsiouLbcYEBwdDJpPZbBkZGd1+bkREROR8JC+Qdu/ejZSUFKSmpuL06dOIiIhAXFwcamtrHY4/ceIEXnzxRfzqV79CeXk5Zs6ciZkzZ+LcuXPWMcOGDcOmTZtQWVmJY8eOITg4GFOnTsXNmzdt3usPf/gDampqrFtSUlKPnisRERE5B8nXIOl0OkRFRWHTpk0AAIvFgoEDByIpKQkrV65sMz4+Ph5msxn79++3tk2aNAljx47F5s2bHR7jwfXGTz75BDExMQDuzyAtX74cy5cv71LcXINERETkfJxiDVJzczPKysoQGxtrbZPL5YiNjUVRUZHDfYqKimzGA0BcXFy745ubm7FlyxZ4e3sjIiLCpi8jIwN+fn4YN24c1q1bh5aWlnZjbWpqwu3bt202IiIiejRJept/XV0dWltbERgYaNMeGBiIixcvOtzHaDQ6HG80Gm3a9u/fj3nz5uHOnTvQarXIz8+Hv7+/tX/ZsmUYP348fH19ceLECaxatQo1NTXYsGGDw+Omp6fjjTfe6MppEhERkZN5ZJ+DNHnyZFRUVKCurg5bt27F3LlzUVxcjICAAABASkqKdWx4eDiUSiVeffVVpKenQ6Vq+9yFVatW2exz+/ZtDBw4sOdPhIiIiHqdpJfY/P394eLighs3bti037hxAxqNxuE+Go2mQ+M9PT0xdOhQTJo0CdnZ2VAoFMjOzm43Fp1Oh5aWFnz99dcO+1UqFfr372+zERER0aNJ0gJJqVRiwoQJKCgosLZZLBYUFBRAr9c73Eev19uMB4D8/Px2x//wfZuamtrtr6iogFwut84wERER0eNL8ktsKSkpMBgMiIyMxMSJE/GXv/wFZrMZCxYsAAC89NJLePLJJ5Geng4ASE5OxrPPPov169djxowZyMnJQWlpKbZs2QIAMJvNWLt2LX7+859Dq9Wirq4OmZmZuHbtGubMmQPg/kLv4uJiTJ48GV5eXigqKsKKFSswf/58qNXd+6N4RERE5HwkL5Di4+Nx8+ZNrFmzBkajEWPHjsWhQ4esC7GvXLkCufz/J7qeeuopvP/++1i9ejV+97vfITQ0FHv27MHo0aMBAC4uLrh48SJ27NiBuro6+Pn5ISoqCp9//jlGjRoF4P7lspycHKSlpaGpqQkhISFYsWKFzRojIiIienxJ/hwkZ8XnIBERETkfp3gOEhEREVFfxAKJiIiIyI7ka5Cc1YMrk3yiNhERkfN48H/7360wYoHURSaTCQD4sEgiIiInZDKZ4O3t3W4/F2l3kcViwfXr1+Hl5QWZTNapfR88hfvq1atc4N0BzFfnMWedw3x1DvPVecxZ5/RkvoQQMJlMCAoKsrlL3h5nkLpILpdjwIABP+o9+ETuzmG+Oo856xzmq3OYr85jzjqnp/L1sJmjB7hIm4iIiMgOCyQiIiIiOyyQJKBSqZCamgqVSiV1KE6B+eo85qxzmK/OYb46jznrnL6QLy7SJiIiIrLDGSQiIiIiOyyQiIiIiOywQCIiIiKywwKJiIiIyA4LpF6WmZmJ4OBguLm5QafToaSkROqQ+ozPPvsMP/vZzxAUFASZTIY9e/bY9AshsGbNGmi1Wri7uyM2NhaXLl2SJtg+ID09HVFRUfDy8kJAQABmzpyJqqoqmzGNjY1ITEyEn58f+vXrh9mzZ+PGjRsSRSytrKwshIeHWx88p9frcfDgQWs/c/VwGRkZkMlkWL58ubWNObOVlpYGmUxms4WFhVn7ma+2rl27hvnz58PPzw/u7u4YM2YMSktLrf1Sfu+zQOpFu3fvRkpKClJTU3H69GlEREQgLi4OtbW1UofWJ5jNZkRERCAzM9Nh/5/+9Cds3LgRmzdvRnFxMTw9PREXF4fGxsZejrRvKCwsRGJiIk6ePIn8/Hzcu3cPU6dOhdlsto5ZsWIF9u3bh9zcXBQWFuL69ev4xS9+IWHU0hkwYAAyMjJQVlaG0tJSTJkyBc8//zz++c9/AmCuHubUqVP429/+hvDwcJt25qytUaNGoaamxrodO3bM2sd82fr+++8RHR0NV1dXHDx4EOfPn8f69euhVqutYyT93hfUayZOnCgSExOtr1tbW0VQUJBIT0+XMKq+CYDIy8uzvrZYLEKj0Yh169ZZ227duiVUKpXYtWuXBBH2PbW1tQKAKCwsFELcz4+rq6vIzc21jrlw4YIAIIqKiqQKs09Rq9XinXfeYa4ewmQyidDQUJGfny+effZZkZycLITg58uR1NRUERER4bCP+Wrrt7/9rXj66afb7Zf6e58zSL2kubkZZWVliI2NtbbJ5XLExsaiqKhIwsicQ3V1NYxGo03+vL29odPpmL9/qa+vBwD4+voCAMrKynDv3j2bnIWFhWHQoEGPfc5aW1uRk5MDs9kMvV7PXD1EYmIiZsyYYZMbgJ+v9ly6dAlBQUEYMmQIEhIScOXKFQDMlyN79+5FZGQk5syZg4CAAIwbNw5bt2619kv9vc8CqZfU1dWhtbUVgYGBNu2BgYEwGo0SReU8HuSI+XPMYrFg+fLliI6OxujRowHcz5lSqYSPj4/N2Mc5Z5WVlejXrx9UKhUWLVqEvLw8jBw5krlqR05ODk6fPo309PQ2fcxZWzqdDtu3b8ehQ4eQlZWF6upqPPPMMzCZTMyXA1999RWysrIQGhqKw4cPY/HixVi2bBl27NgBQPrvfUWPH4GIelxiYiLOnTtns96B2ho+fDgqKipQX1+PDz/8EAaDAYWFhVKH1SddvXoVycnJyM/Ph5ubm9ThOIXp06db/w4PD4dOp8PgwYPxwQcfwN3dXcLI+iaLxYLIyEi8+eabAIBx48bh3Llz2Lx5MwwGg8TRcQap1/j7+8PFxaXNHQs3btyARqORKCrn8SBHzF9bS5cuxf79+3HkyBEMGDDA2q7RaNDc3Ixbt27ZjH+cc6ZUKjF06FBMmDAB6enpiIiIwNtvv81cOVBWVoba2lqMHz8eCoUCCoUChYWF2LhxIxQKBQIDA5mzf8PHxwfDhg3D5cuX+RlzQKvVYuTIkTZtI0aMsF6WlPp7nwVSL1EqlZgwYQIKCgqsbRaLBQUFBdDr9RJG5hxCQkKg0Whs8nf79m0UFxc/tvkTQmDp0qXIy8vDp59+ipCQEJv+CRMmwNXV1SZnVVVVuHLlymObM3sWiwVNTU3MlQMxMTGorKxERUWFdYuMjERCQoL1b+bs4RoaGvDll19Cq9XyM+ZAdHR0m0eTfPHFFxg8eDCAPvC93+PLwMkqJydHqFQqsX37dnH+/HmxcOFC4ePjI4xGo9Sh9Qkmk0mUl5eL8vJyAUBs2LBBlJeXi2+++UYIIURGRobw8fER//jHP8TZs2fF888/L0JCQsTdu3cljlwaixcvFt7e3uLo0aOipqbGut25c8c6ZtGiRWLQoEHi008/FaWlpUKv1wu9Xi9h1NJZuXKlKCwsFNXV1eLs2bNi5cqVQiaTiY8//lgIwVx1xA/vYhOCObP3m9/8Rhw9elRUV1eL48ePi9jYWOHv7y9qa2uFEMyXvZKSEqFQKMTatWvFpUuXxM6dO4WHh4d47733rGOk/N5ngdTL/vrXv4pBgwYJpVIpJk6cKE6ePCl1SH3GkSNHBIA2m8FgEELcv+Xz9ddfF4GBgUKlUomYmBhRVVUlbdAScpQrAGLbtm3WMXfv3hVLliwRarVaeHh4iFmzZomamhrpgpbQK6+8IgYPHiyUSqV44oknRExMjLU4EoK56gj7Aok5sxUfHy+0Wq1QKpXiySefFPHx8eLy5cvWfuarrX379onRo0cLlUolwsLCxJYtW2z6pfzelwkhRM/PUxERERE5D65BIiIiIrLDAomIiIjIDgskIiIiIjsskIiIiIjssEAiIiIissMCiYiIiMgOCyQiIiIiOyyQiIi6iUwmw549e6QOg4i6AQskInokvPzyy5DJZG22adOmSR0aETkhhdQBEBF1l2nTpmHbtm02bSqVSqJoiMiZcQaJiB4ZKpUKGo3GZlOr1QDuX/7KysrC9OnT4e7ujiFDhuDDDz+02b+yshJTpkyBu7s7/Pz8sHDhQjQ0NNiMeffddzFq1CioVCpotVosXbrUpr+urg6zZs2Ch4cHQkNDsXfv3p49aSLqESyQiOix8frrr2P27Nk4c+YMEhISMG/ePFy4cAEAYDabERcXB7VajVOnTiE3NxeffPKJTQGUlZWFxMRELFy4EJWVldi7dy+GDh1qc4w33ngDc+fOxdmzZ/Hcc88hISEB3333Xa+eJxF1g175SVwioh5mMBiEi4uL8PT0tNnWrl0rhBACgFi0aJHNPjqdTixevFgIIcSWLVuEWq0WDQ0N1v4DBw4IuVwujEajEEKIoKAg8fvf/77dGACI1atXW183NDQIAOLgwYPddp5E1Du4BomIHhmTJ09GVlaWTZuvr6/1b71eb9On1+tRUVEBALhw4QIiIiLg6elp7Y+OjobFYkFVVRVkMhmuX7+OmJiYh8YQHh5u/dvT0xP9+/dHbW1tV0+JiCTCAomIHhmenp5tLnl1F3d39w6Nc3V1tXktk8lgsVh6IiQi6kFcg0REj42TJ0+2eT1ixAgAwIgRI3DmzBmYzWZr//HjxyGXyzF8+HB4eXkhODgYBQUFvRozEUmDM0hE9MhoamqC0Wi0aVMoFPD39wcA5ObmIjIyEk8//TR27tyJkpISZGdnAwASEhKQmpoKg8GAtLQ03Lx5E0lJSfjlL3+JwMBAAEBaWhoWLVqEgIAATJ8+HSaTCcePH0dSUlLvnigR9TgWSET0yDh06BC0Wq1N2/Dhw3Hx4kUA9+8wy8nJwZIlS6DVarFr1y6MHDkSAODh4YHDhw8jOTkZUVFR8PDwwOzZs7FhwwbrexkMBjQ2NuKtt97Ca6+9Bn9/f7zwwgu9d4JE1GtkQgghdRBERD1NJpMhLy8PM2fOlDoUInICXINEREREZIcFEhEREZEdrkEioscCVxMQUWdwBomIiIjIDgskIiIiIjsskIiIiIjssEAiIiIissMCiYiIiMgOCyQiIiIiOyyQiIiIiOywQCIiIiKywwKJiIiIyM7/AZdd4DsvPjy9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 60\n",
    "\n",
    "# Extract optimal hyperparameters \n",
    "num_layers = best_model_info['num_layers']\n",
    "dropout = best_model_info['dropout']\n",
    "learning_rate = best_model_info['learning_rate']\n",
    "\n",
    "# You can also test a model with your own parameters by replacing the values of num_layers, dropout and learning_rate and see what happens with the loss\n",
    "\n",
    "# Define the model with the same hyperparameters\n",
    "new_model = SimpleCNN(num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "new_model = new_model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(new_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model again\n",
    "train_rmse_losses = []\n",
    "val_rmse_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    _, train_rmse_loss = train(new_model, train_loader, criterion, optimizer, device)\n",
    "    _, val_rmse_loss = validate(new_model, val_loader, criterion, device)\n",
    "    train_rmse_losses.append(train_rmse_loss)\n",
    "    val_rmse_losses.append(val_rmse_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train RMSE Loss: {train_rmse_loss:.4f}, Val RMSE Loss: {val_rmse_loss:.4f}')\n",
    "\n",
    "# Plot the mean loss curves\n",
    "plt.plot(range(1,num_epochs+1), train_rmse_losses, label='Mean Train RMSE Loss')\n",
    "plt.plot(range(1,num_epochs+1), val_rmse_losses, label='Mean Validation RMSE Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Mean Training and Validation RMSE Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "True Targets: tensor([0.2364, 0.0735, 0.0451, 0.1635, 0.0507, 0.0450, 0.1089, 0.0855, 0.1124,\n",
      "        0.0788], device='cuda:0')\n",
      "Predicted Values: tensor([0.3897, 0.0928, 0.0072, 0.1868, 0.0090, 0.0224, 0.0615, 0.0553, 0.0975,\n",
      "        0.0777], device='cuda:0')\n",
      "Sample 2:\n",
      "True Targets: tensor([0.1689, 0.0923, 0.0416, 0.1576, 0.0591, 0.0945, 0.1179, 0.0641, 0.0779,\n",
      "        0.1260], device='cuda:0')\n",
      "Predicted Values: tensor([0.2127, 0.1009, 0.0456, 0.1540, 0.0307, 0.0705, 0.0864, 0.0790, 0.1110,\n",
      "        0.1093], device='cuda:0')\n",
      "Sample 3:\n",
      "True Targets: tensor([0.1738, 0.0910, 0.0435, 0.1046, 0.0547, 0.0245, 0.0626, 0.1064, 0.1804,\n",
      "        0.1583], device='cuda:0')\n",
      "Predicted Values: tensor([0.3055, 0.1005, 0.0171, 0.1751, 0.0091, 0.0393, 0.0745, 0.0670, 0.1122,\n",
      "        0.0997], device='cuda:0')\n",
      "Sample 4:\n",
      "True Targets: tensor([0.1926, 0.0651, 0.0372, 0.0977, 0.0391, 0.0579, 0.0334, 0.1575, 0.1364,\n",
      "        0.1830], device='cuda:0')\n",
      "Predicted Values: tensor([0.3331, 0.0870, 0.0117, 0.1507, 0.0153, 0.0266, 0.0751, 0.0656, 0.1267,\n",
      "        0.1081], device='cuda:0')\n",
      "Sample 5:\n",
      "True Targets: tensor([0.4243, 0.1051, 0.0303, 0.1021, 0.0435, 0.0272, 0.0706, 0.0578, 0.0783,\n",
      "        0.0607], device='cuda:0')\n",
      "Predicted Values: tensor([0.3385, 0.0997, 0.0123, 0.1827, 0.0080, 0.0327, 0.0675, 0.0615, 0.1065,\n",
      "        0.0906], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the first 5 predictions\n",
    "num_predictions = 5\n",
    "with torch.no_grad():\n",
    "    new_model.eval()\n",
    "    for i, (inputs, targets) in enumerate(val_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = new_model(inputs)\n",
    "        predicted_values = outputs[:num_predictions]\n",
    "        true_values = targets[:num_predictions]\n",
    "        break  # Stop after the first batch\n",
    "\n",
    "# Display the true targets and predictions\n",
    "for i in range(num_predictions):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"True Targets:\", true_values[i])\n",
    "    print(\"Predicted Values:\", predicted_values[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
